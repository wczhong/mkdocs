<!DOCTYPE html>
<html lang="en" data-bs-theme="light">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        <link rel="canonical" href="https://kafka.apachecn.org/6/">
        <link rel="shortcut icon" href="../img/favicon.ico">
        <title>6. 运维 - 【布客】kafka 中文翻译</title>
        <link href="../css/bootstrap.min.css" rel="stylesheet">
        <link href="../css/fontawesome.min.css" rel="stylesheet">
        <link href="../css/brands.min.css" rel="stylesheet">
        <link href="../css/solid.min.css" rel="stylesheet">
        <link href="../css/v4-font-face.min.css" rel="stylesheet">
        <link href="../css/base.css" rel="stylesheet">
        <link id="hljs-light" rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" >
        <link id="hljs-dark" rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github-dark.min.css" disabled>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
        <script>hljs.highlightAll();</script>
        <script async src="https://www.googletagmanager.com/gtag/js?id=G-8DP4GX97XY"></script>
        <script>
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());

          gtag('config', "G-8DP4GX97XY");
        </script> 
    </head>

    <body>
        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">
                <a class="navbar-brand" href="..">【布客】kafka 中文翻译</a>
                <!-- Expander button -->
                <button type="button" class="navbar-toggler" data-bs-toggle="collapse" data-bs-target="#navbar-collapse" aria-controls="navbar-collapse" aria-expanded="false" aria-label="Toggle navigation">
                    <span class="navbar-toggler-icon"></span>
                </button>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="nav-item">
                                <a href=".." class="nav-link">kafke 中文文档</a>
                            </li>
                            <li class="nav-item">
                                <a href="https://kafka1x.apachecn.org" class="nav-link">kafke 1.0.x 版本</a>
                            </li>
                            <li class="nav-item dropdown">
                                <a href="#" class="nav-link dropdown-toggle active" aria-current="page" role="button" data-bs-toggle="dropdown"  aria-expanded="false">kafke 3.5.x 版本</a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../1/" class="dropdown-item">1. 入门</a>
</li>
                                    
<li>
    <a href="../2/" class="dropdown-item">2. APIs</a>
</li>
                                    
<li>
    <a href="../3/" class="dropdown-item">3. 配置</a>
</li>
                                    
<li>
    <a href="../4/" class="dropdown-item">4. 设计</a>
</li>
                                    
<li>
    <a href="../5/" class="dropdown-item">5. 实施</a>
</li>
                                    
<li>
    <a href="./" class="dropdown-item active" aria-current="page">6. 运维</a>
</li>
                                    
<li>
    <a href="../7/" class="dropdown-item">7. 安全</a>
</li>
                                    
<li>
    <a href="../8/" class="dropdown-item">8. Kafka 连接</a>
</li>
                                    
<li>
    <a href="../9/" class="dropdown-item">9. Kafka Streams</a>
</li>
                                </ul>
                            </li>
                            <li class="nav-item">
                                <a href="../downloads/" class="nav-link">下载 Kafka</a>
                            </li>
                            <li class="nav-item">
                                <a href="../contrib/" class="nav-link">贡献指南</a>
                            </li>
                            <li class="nav-item">
                                <a href="https://www.apachecn.org/about" class="nav-link">关于我们</a>
                            </li>
                            <li class="nav-item">
                                <a href="https://www.apachecn.org/join" class="nav-link">加入我们</a>
                            </li>
                            <li class="nav-item">
                                <a href="https://docs.apachecn.org" class="nav-link">中文资源合集</a>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ms-md-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-bs-toggle="modal" data-bs-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../5/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../7/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                            <li class="nav-item">
                                <a href="https://github.com/apachecn/kafka-doc-zh/edit/master/docs/6.md" class="nav-link">Edit on apachecn/kafka-doc-zh
                                    </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            <div class="row">
                    <div class="col-md-3"><div class="navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-bs-toggle="collapse" data-bs-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>

    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-body-tertiary">
        <ul class="nav flex-column">
            
            <li class="nav-item" data-bs-level="1"><a href="#6-operations" class="nav-link">6. Operations 操作</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-bs-level="2"><a href="#61-kafka" class="nav-link">6.1 Kafka基本操作</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#62" class="nav-link">6.2 数据中心</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#63" class="nav-link">6.3 异地复制（跨集群数据镜像）</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#64" class="nav-link">6.4 多租户</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#65-kafka" class="nav-link">6.5 Kafka配置</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#66-java" class="nav-link">6.6 Java版本</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#67" class="nav-link">6.7 硬件和操作系统</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#68" class="nav-link">6.8 监控</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#69" class="nav-link">6.9 动物园管理员</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#610-kraft" class="nav-link">6.10 KRaft</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#_60" class="nav-link">术语</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#_61" class="nav-link">准备迁移</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#kraft_2" class="nav-link">配置 KRaft 控制器仲裁</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#_62" class="nav-link">在代理上启用迁移</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#kraft_3" class="nav-link">将代理迁移到 KRaft</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#_63" class="nav-link">完成迁移</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#611" class="nav-link">6.11分层存储</a>
              <ul class="nav flex-column">
              </ul>
            </li>
              </ul>
            </li>
        </ul>
    </div>
</div></div>
                    <div class="col-md-9" role="main">

<h1 id="6-operations"><a href="https://kafka.apache.org/documentation/#operations">6. Operations 操作</a></h1>
<p>以下是根据 LinkedIn 的使用情况和经验实际将 Kafka 作为生产系统运行的一些信息。请将您知道的任何其他提示发送给我们。</p>
<h2 id="61-kafka"><a href="https://kafka.apache.org/documentation/#basic_ops">6.1 Kafka基本操作</a></h2>
<p>本节将回顾您将在 Kafka 集群上执行的最常见操作。本节中回顾的所有工具都可以在<code>bin/</code>Kafka 发行版的目录下找到，如果不带参数运行，每个工具都会打印所有可能的命令行选项的详细信息。</p>
<h3 id="_1"><a href="https://kafka.apache.org/documentation/#basic_ops_add_topic">添加和删​​除主题</a></h3>
<p>您可以选择手动添加主题，也可以在数据首次发布到不存在的主题时自动创建主题。如果主题是自动创建的，那么您可能需要调整用于自动创建主题的 默认<a href="https://kafka.apache.org/documentation/#topicconfigs">主题配置。</a></p>
<p>使用主题工具添加和修改主题：</p>
<pre><code class="language-bash">&gt; bin/kafka-topics.sh --bootstrap-server broker_host:port --create --topic my_topic_name \
      --partitions 20 --replication-factor 3 --config x=y
</code></pre>
<p>复制因子控制有多少服务器将复制每条写入的消息。如果复制因子为 3，则在您失去对数据的访问权限之前最多 2 个服务器可能会发生故障。我们建议您使用 2 或 3 的复制因子，以便您可以透明地弹跳机器而不中断数据消耗。</p>
<p>分区计数控制主题将被分片为多少个日志。分区计数有多种影响。首先，每个分区必须完全适合一台服务器。因此，如果您有 20 个分区，则完整数据集（以及读写负载）将由不超过 20 个服务器（不包括副本）处理。最后，分区计数会影响消费者的最大并行度。<a href="https://kafka.apache.org/documentation/#intro_consumers">概念部分</a>对此进行了更详细的讨论。</p>
<p>每个分片分区日志都放置在 Kafka 日志目录下自己的文件夹中。此类文件夹的名称由主题名称、附加破折号 (-) 和分区 ID 组成。由于典型的文件夹名称长度不能超过 255 个字符，因此主题名称的长度将受到限制。我们假设分区数量永远不会超过 100,000。因此，主题名称不能超过 249 个字符。这会在文件夹名称中留下足够的空间用于短划线和可能为 5 位数长的分区 ID。</p>
<p>在命令行上添加的配置会覆盖服务器的默认设置，例如数据应保留的时间长度。完整的每个主题配置集记录<a href="https://kafka.apache.org/documentation/#topicconfigs">在此处</a>。</p>
<h3 id="_2"><a href="https://kafka.apache.org/documentation/#basic_ops_modify_topic">修改主题</a></h3>
<p>您可以使用同一主题工具更改主题的配置或分区。</p>
<p>要添加分区，您可以执行以下操作</p>
<pre><code class="language-bash">&gt; bin/kafka-topics.sh --bootstrap-server broker_host:port --alter --topic my_topic_name \
      --partitions 40
</code></pre>
<p>请注意，分区的一种用例是对数据进行语义分区，并且添加分区不会更改现有数据的分区，因此如果消费者依赖该分区，这可能会干扰他们。也就是说，如果数据已分区<code>hash(key) % number_of_partitions</code>，则该分区可能会通过添加分区而被打乱，但 Kafka 不会尝试以任何方式自动重新分配数据。</p>
<p>添加配置：</p>
<pre><code class="language-bash">&gt; bin/kafka-configs.sh --bootstrap-server broker_host:port --entity-type topics --entity-name my_topic_name --alter --add-config x=y
</code></pre>
<p>要删除配置：</p>
<pre><code class="language-bash">&gt; bin/kafka-configs.sh --bootstrap-server broker_host:port --entity-type topics --entity-name my_topic_name --alter --delete-config x
</code></pre>
<p>最后删除一个主题：</p>
<pre><code class="language-bash">&gt; bin/kafka-topics.sh --bootstrap-server broker_host:port --delete --topic my_topic_name
</code></pre>
<p>Kafka目前不支持减少主题的分区数量。</p>
<p><a href="https://kafka.apache.org/documentation/#basic_ops_increase_replication_factor">可以在此处</a> 找到有关更改主题的复制因子的说明。</p>
<h3 id="_3"><a href="https://kafka.apache.org/documentation/#basic_ops_restarting">优雅关机</a></h3>
<p>Kafka 集群将自动检测任何代理关闭或故障，并为该机器上的分区选举新的领导者。无论服务器发生故障还是为了维护或配置更改而故意关闭服务器，都会发生这种情况。对于后一种情况，Kafka 支持一种更优雅的机制来停止服务器，而不仅仅是杀死它。当服务器正常停止时，它将利用两个优化：</p>
<ol>
<li>它将所有日志同步到磁盘，以避免重新启动时需要执行任何日志恢复（即验证日志尾部所有消息的校验和）。日志恢复需要时间，因此这可以加快有意重新启动的速度。</li>
<li>它将在关闭之前将服务器作为领导者的任何分区迁移到其他副本。这将使领导权转移更快，并将每个分区不可用的时间最小化到几毫秒。</li>
</ol>
<p>每当服务器停止（除了硬终止）时，同步日志都会自动发生，但受控领导迁移需要使用特殊设置：</p>
<pre><code class="language-text">controlled.shutdown.enable=true
</code></pre>
<p>请注意，只有在代理上托管的<em>所有</em>分区都有副本（即复制因子大于 1<em>并且</em>这些副本中至少有一个处于活动状态）时，受控关闭才会成功。这通常是您想要的，因为关闭最后一个副本将使该主题分区不可用。</p>
<h3 id="_4"><a href="https://kafka.apache.org/documentation/#basic_ops_leader_balancing">平衡领导力</a></h3>
<p>每当代理停止或崩溃时，该代理的分区的领导权就会转移到其他副本。当代理重新启动时，它只会成为其所有分区的追随者，这意味着它不会用于客户端读取和写入。</p>
<p>为了避免这种不平衡，Kafka 有一个首选副本的概念。如果分区的副本列表为 1、5、9，则节点 1 优先作为节点 5 或 9 的领导者，因为它在副本列表中较早。默认情况下，Kafka 集群将尝试恢复首选副本的领导地位。此行为配置为：</p>
<pre><code class="language-text">auto.leader.rebalance.enable=true
</code></pre>
<p>您也可以将其设置为 false，但随后您需要通过运行以下命令手动恢复已恢复副本的领导权：</p>
<pre><code class="language-bash">&gt; bin/kafka-leader-election.sh --bootstrap-server broker_host:port --election-type preferred --all-topic-partitions
</code></pre>
<h3 id="_5"><a href="https://kafka.apache.org/documentation/#basic_ops_racks">跨机架平衡副本</a></h3>
<p>机架感知功能将同一分区的副本分布在不同的机架上。这扩展了 Kafka 为代理故障提供的保证以涵盖机架故障，从而限制了机架上的所有代理同时发生故障时数据丢失的风险。该功能还可以应用于其他代理分组，例如 EC2 中的可用区域。</p>
<p>您可以通过向代理配置添加属性来指定代理属于特定机架：</p>
<pre><code class="language-text">broker.rack=my-rack-id
</code></pre>
<p><a href="https://kafka.apache.org/documentation/#basic_ops_add_topic">当创建</a>、<a href="https://kafka.apache.org/documentation/#basic_ops_modify_topic">修改主题</a>或重新分配<a href="https://kafka.apache.org/documentation/#basic_ops_cluster_expansion">副本</a> 时，将遵守机架约束，确保副本跨越尽可能多的机架（分区将跨越 min(#racks,replication-factor) 个不同的机架）。</p>
<p>用于将副本分配给代理的算法可确保每个代理的领导者数量保持不变，无论代理如何跨机架分布。这确保了平衡的吞吐量。</p>
<p>但是，如果为机架分配了不同数量的代理，则副本的分配将不均匀。具有较少代理的机架将获得更多副本，这意味着它们将使用更多存储并将更多资源用于复制。因此，每个机架配置相同数量的代理是明智的。</p>
<h3 id="_6"><a href="https://kafka.apache.org/documentation/#basic_ops_mirror_maker">集群之间的数据镜像和异地复制</a></h3>
<p>Kafka 管理员可以定义跨越各个 Kafka 集群、数据中心或地理区域边界的数据流。<a href="https://kafka.apache.org/documentation/#georeplication">请参阅异地复制</a>部分以获取更多信息。</p>
<h3 id="_7"><a href="https://kafka.apache.org/documentation/#basic_ops_consumer_lag">检查消费者位置</a></h3>
<p>有时了解消费者的立场很有用。我们有一个工具可以显示消费者组中所有消费者的位置以及它们距离日志末尾有多远。<em>要在使用名为my-topic 的</em>主题的名为<em>my-group 的</em>消费者组上运行此工具，如下所示：</p>
<pre><code class="language-bash">&gt; bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group my-group

TOPIC                          PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG        CONSUMER-ID                                       HOST                           CLIENT-ID
my-topic                       0          2               4               2          consumer-1-029af89c-873c-4751-a720-cefd41a669d6   /127.0.0.1                     consumer-1
my-topic                       1          2               3               1          consumer-1-029af89c-873c-4751-a720-cefd41a669d6   /127.0.0.1                     consumer-1
my-topic                       2          2               3               1          consumer-2-42c1abd4-e3b2-425d-a8bb-e1ea49b29bb2   /127.0.0.1                     consumer-2
</code></pre>
<h3 id="_8"><a href="https://kafka.apache.org/documentation/#basic_ops_consumer_group">管理消费者群体</a></h3>
<p>使用 ConsumerGroupCommand 工具，我们可以列出、描述或删除消费者组。可以手动删除消费者组，也可以在该组的最后提交的偏移量到期时自动删除该消费者组。仅当组没有任何活动成员时，手动删除才有效。例如，要列出所有主题的所有消费者组：</p>
<pre><code class="language-bash">&gt; bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --list

test-consumer-group
</code></pre>
<p>要查看偏移量，如前所述，我们像这样“描述”消费者组：</p>
<pre><code class="language-bash">&gt; bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group my-group

TOPIC           PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID                                    HOST            CLIENT-ID
topic3          0          241019          395308          154289          consumer2-e76ea8c3-5d30-4299-9005-47eb41f3d3c4 /127.0.0.1      consumer2
topic2          1          520678          803288          282610          consumer2-e76ea8c3-5d30-4299-9005-47eb41f3d3c4 /127.0.0.1      consumer2
topic3          1          241018          398817          157799          consumer2-e76ea8c3-5d30-4299-9005-47eb41f3d3c4 /127.0.0.1      consumer2
topic1          0          854144          855809          1665            consumer1-3fc8d6f1-581a-4472-bdf3-3515b4aee8c1 /127.0.0.1      consumer1
topic2          0          460537          803290          342753          consumer1-3fc8d6f1-581a-4472-bdf3-3515b4aee8c1 /127.0.0.1      consumer1
topic3          2          243655          398812          155157          consumer4-117fe4d3-c6c1-4178-8ee9-eb4a3954bee0 /127.0.0.1      consumer4
</code></pre>
<p>有许多附加的“描述”选项可用于提供有关消费者组的更详细信息：</p>
<ul>
<li>--members：此选项提供消费者组中所有活跃成员的列表。</li>
</ul>
<pre><code class="language-bash">&gt; bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group my-group --members

CONSUMER-ID                                    HOST            CLIENT-ID       #PARTITIONS
consumer1-3fc8d6f1-581a-4472-bdf3-3515b4aee8c1 /127.0.0.1      consumer1       2
consumer4-117fe4d3-c6c1-4178-8ee9-eb4a3954bee0 /127.0.0.1      consumer4       1
consumer2-e76ea8c3-5d30-4299-9005-47eb41f3d3c4 /127.0.0.1      consumer2       3
consumer3-ecea43e4-1f01-479f-8349-f9130b75d8ee /127.0.0.1      consumer3       0
</code></pre>
<ul>
<li>--members --verbose：除了上面的“--members”选项报告的信息之外，此选项还提供分配给每个成员的分区。</li>
</ul>
<pre><code class="language-bash">&gt; bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group my-group --members --verbose

CONSUMER-ID                                    HOST            CLIENT-ID       #PARTITIONS     ASSIGNMENT
consumer1-3fc8d6f1-581a-4472-bdf3-3515b4aee8c1 /127.0.0.1      consumer1       2               topic1(0), topic2(0)
consumer4-117fe4d3-c6c1-4178-8ee9-eb4a3954bee0 /127.0.0.1      consumer4       1               topic3(2)
consumer2-e76ea8c3-5d30-4299-9005-47eb41f3d3c4 /127.0.0.1      consumer2       3               topic2(1), topic3(0,1)
consumer3-ecea43e4-1f01-479f-8349-f9130b75d8ee /127.0.0.1      consumer3       0               -
</code></pre>
<ul>
<li>--offsets：这是默认的描述选项，提供与“--describe”选项相同的输出。</li>
<li>--state：此选项提供有用的组级别信息。</li>
</ul>
<pre><code class="language-bash">      &gt; bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group my-group --state

      COORDINATOR (ID)          ASSIGNMENT-STRATEGY       STATE                #MEMBERS
      localhost:9092 (0)        range                     Stable               4
</code></pre>
<p>要手动删除一个或多个消费者组，可以使用“--delete”选项：</p>
<pre><code class="language-bash">  &gt; bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --delete --group my-group --group my-other-group

  Deletion of requested consumer groups ('my-group', 'my-other-group') was successful.
</code></pre>
<p>要重置消费者组的偏移量，可以使用“--reset-offsets”选项。此选项同时支持一个消费者组。它需要定义以下范围：--all-topics 或--topic。除非您使用“--from-file”方案，否则必须选择一个范围。另外，首先确保使用者实例处于非活动状态。有关更多详细信息， 请参阅 <a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-122%3A+Add+Reset+Consumer+Group+Offsets+tooling">KIP-122 。</a></p>
<p>它有 3 个执行选项：</p>
<ul>
<li>（默认）显示要重置的偏移量。</li>
<li>--execute : 执行 --reset-offsets 进程。</li>
<li>--export ：将结果导出为 CSV 格式。</li>
</ul>
<p>--reset-offsets 还有以下场景可供选择（至少必须选择一种场景）：</p>
<ul>
<li>--to-datetime <String: datetime> ：将偏移量重置为日期时间的偏移量。格式：'YYYY-MM-DDTHH:mm:SS.sss'</li>
<li>--to-earliest ：将偏移量重置为最早的偏移量。</li>
<li>--to-latest ：将偏移量重置为最新偏移量。</li>
<li>--shift-by <Long: number-of-offsets> ：重置偏移量，将当前偏移量移动“n”，其中“n”可以是正数或负数。</li>
<li>--from-file ：将偏移量重置为 CSV 文件中定义的值。</li>
<li>--to-current ：将偏移量重置为当前偏移量。</li>
<li>--by-duration <String:uration> ：将偏移量重置为从当前时间戳开始按持续时间偏移。格式：'PnDTnHnMnS'</li>
<li>--to-offset ：将偏移量重置为特定偏移量。</li>
</ul>
<p>请注意，超出范围的偏移将调整为可用的偏移端。例如，如果偏移结束为10，偏移移位请求为15，则实际上会选择10处的偏移。</p>
<p>例如，将消费者组的偏移量重置为最新的偏移量：</p>
<pre><code class="language-bash">&gt; bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --reset-offsets --group consumergroup1 --topic topic1 --to-latest

TOPIC                          PARTITION  NEW-OFFSET
topic1                         0          0
</code></pre>
<p>如果您使用旧的高级使用者并将组元数据存储在 ZooKeeper 中（即<code>offsets.storage=zookeeper</code>），请传递 <code>--zookeeper</code>而不是<code>--bootstrap-server</code>：</p>
<pre><code class="language-bash">&gt; bin/kafka-consumer-groups.sh --zookeeper localhost:2181 --list
</code></pre>
<h3 id="_9"><a href="https://kafka.apache.org/documentation/#basic_ops_cluster_expansion">扩展您的集群</a></h3>
<p>将服务器添加到 Kafka 集群非常简单，只需为它们分配一个唯一的代理 ID 并在新服务器上启动 Kafka 即可。但是，这些新服务器不会自动分配任何数据分区，因此除非将分区移动到它们，否则在创建新主题之前它们不会执行任何工作。因此，通常当您向集群添加机器时，您会希望将一些现有数据迁移到这些机器。</p>
<p>迁移数据的过程是手动启动的，但完全自动化。在幕后发生的事情是，Kafka 会将新服务器添加为它正在迁移的分区的跟随者，并允许它完全复制该分区中的现有数据。当新服务器完全复制该分区的内容并加入同步副本时，现有副本之一将删除其分区的数据。</p>
<p>分区重新分配工具可用于跨代理移动分区。理想的分区分布将确保所有代理的数据负载和分区大小均匀。分区重新分配工具无法自动研究 Kafka 集群中的数据分布并移动分区以获得均匀的负载分布。因此，管理员必须弄清楚应该移动哪些主题或分区。</p>
<p>分区重新分配工具可以在 3 种互斥的模式下运行：</p>
<ul>
<li>--generate：在此模式下，给定主题列表和代理列表，该工具会生成候选重新​​分配，以将指定主题的所有分区移动到新代理。此选项仅提供了一种在给定主题和目标代理列表的情况下生成分区重新分配计划的便捷方法。</li>
<li>--execute：在此模式下，该工具根据用户提供的重新分配计划启动分区的重新分配。（使用 --reassignment-json-file 选项）。这可以是由管理员手工制作的自定义重新分配计划，也可以使用 --generate 选项提供</li>
<li>--verify：在此模式下，该工具验证上次 --execute 期间列出的所有分区的重新分配状态。状态可以是成功完成、失败或正在进行</li>
</ul>
<h4 id="_10"><a href="https://kafka.apache.org/documentation/#basic_ops_automigrate">自动将数据迁移到新机器</a></h4>
<p>分区重新分配工具可用于将某些主题从当前代理集中移至新添加的代理。这在扩展现有集群时通常很有用，因为将整个主题移动到一组新的代理比一次移动一个分区更容易。当用于执行此操作时，用户应提供应移动到新代理集的主题列表以及新代理的目标列表。然后，该工具将给定主题列表的所有分区均匀分布在新的代理集上。在此移动过程中，主题的复制因子保持不变。实际上，主题输入列表的所有分区的副本都从旧的代理集移动到新添加的代理。</p>
<p>例如，以下示例将主题 foo1,foo2 的所有分区移动到新的代理集 5,6。在此移动结束时，主题 foo1 和 foo2 的所有分区将<em>仅存</em>在于代理 5,6 上。</p>
<p>由于该工具接受 json 文件形式的主题输入列表，因此您首先需要确定要移动的主题并创建 json 文件，如下所示：</p>
<pre><code class="language-bash">&gt; cat topics-to-move.json
{&quot;topics&quot;: [{&quot;topic&quot;: &quot;foo1&quot;},
            {&quot;topic&quot;: &quot;foo2&quot;}],
&quot;version&quot;:1
}
</code></pre>
<p>json 文件准备好后，使用分区重新分配工具生成候选分配：</p>
<pre><code class="language-bash">&gt; bin/kafka-reassign-partitions.sh --bootstrap-server localhost:9092 --topics-to-move-json-file topics-to-move.json --broker-list &quot;5,6&quot; --generate
Current partition replica assignment

{&quot;version&quot;:1,
&quot;partitions&quot;:[{&quot;topic&quot;:&quot;foo1&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[2,1]},
              {&quot;topic&quot;:&quot;foo1&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[1,3]},
              {&quot;topic&quot;:&quot;foo1&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[3,4]},
              {&quot;topic&quot;:&quot;foo2&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[4,2]},
              {&quot;topic&quot;:&quot;foo2&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[2,1]},
              {&quot;topic&quot;:&quot;foo2&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[1,3]}]
}

Proposed partition reassignment configuration

{&quot;version&quot;:1,
&quot;partitions&quot;:[{&quot;topic&quot;:&quot;foo1&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[6,5]},
              {&quot;topic&quot;:&quot;foo1&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[5,6]},
              {&quot;topic&quot;:&quot;foo1&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[6,5]},
              {&quot;topic&quot;:&quot;foo2&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[5,6]},
              {&quot;topic&quot;:&quot;foo2&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[6,5]},
              {&quot;topic&quot;:&quot;foo2&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[5,6]}]
}
</code></pre>
<p>该工具生成一个候选分配，将所有分区从主题 foo1,foo2 移动到代理 5,6。但请注意，此时分区移动尚未开始，它仅告诉您当前分配和建议的新分配。应保存当前分配，以防您想回滚到当前分配。新分配应保存在 json 文件中（例如 Expand-cluster-reassignment.json），以便使用 --execute 选项输入到工具中，如下所示：</p>
<pre><code class="language-bash">&gt; bin/kafka-reassign-partitions.sh --bootstrap-server localhost:9092 --reassignment-json-file expand-cluster-reassignment.json --execute
Current partition replica assignment

{&quot;version&quot;:1,
&quot;partitions&quot;:[{&quot;topic&quot;:&quot;foo1&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[2,1]},
              {&quot;topic&quot;:&quot;foo1&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[1,3]},
              {&quot;topic&quot;:&quot;foo1&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[3,4]},
              {&quot;topic&quot;:&quot;foo2&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[4,2]},
              {&quot;topic&quot;:&quot;foo2&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[2,1]},
              {&quot;topic&quot;:&quot;foo2&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[1,3]}]
}

Save this to use as the --reassignment-json-file option during rollback
Successfully started partition reassignments for foo1-0,foo1-1,foo1-2,foo2-0,foo2-1,foo2-2

</code></pre>
<p>最后，--verify 选项可以与该工具一起使用来检查分区重新分配的状态。请注意，相同的 Expand-cluster-reassignment.json（与 --execute 选项一起使用）应与 --verify 选项一起使用：</p>
<pre><code class="language-bash">&gt; bin/kafka-reassign-partitions.sh --bootstrap-server localhost:9092 --reassignment-json-file expand-cluster-reassignment.json --verify
Status of partition reassignment:
Reassignment of partition [foo1,0] is completed
Reassignment of partition [foo1,1] is still in progress
Reassignment of partition [foo1,2] is still in progress
Reassignment of partition [foo2,0] is completed
Reassignment of partition [foo2,1] is completed
Reassignment of partition [foo2,2] is completed
</code></pre>
<h4 id="_11"><a href="https://kafka.apache.org/documentation/#basic_ops_partitionassignment">自定义分区分配和迁移</a></h4>
<p>分区重新分配工具还可用于有选择地将分区的副本移动到一组特定的代理。当以这种方式使用时，假设用户知道重新分配计划并且不需要该工具来生成候选重新​​分配，从而有效地跳过 --generate 步骤并直接进入 --execute 步骤</p>
<p>例如，以下示例将主题 foo1 的分区 0 移动到代理 5,6，将主题 foo2 的分区 1 移动到代理 2,3：</p>
<p>第一步是在 json 文件中手工制定自定义重新分配计划：</p>
<pre><code class="language-bash">&gt; cat custom-reassignment.json
{&quot;version&quot;:1,&quot;partitions&quot;:[{&quot;topic&quot;:&quot;foo1&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[5,6]},{&quot;topic&quot;:&quot;foo2&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[2,3]}]}
</code></pre>
<p>然后，使用带有 --execute 选项的 json 文件来启动重新分配过程：</p>
<pre><code class="language-bash">&gt; bin/kafka-reassign-partitions.sh --bootstrap-server localhost:9092 --reassignment-json-file custom-reassignment.json --execute
Current partition replica assignment

{&quot;version&quot;:1,
&quot;partitions&quot;:[{&quot;topic&quot;:&quot;foo1&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[1,2]},
              {&quot;topic&quot;:&quot;foo2&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[3,4]}]
}

Save this to use as the --reassignment-json-file option during rollback
Successfully started partition reassignments for foo1-0,foo2-1

</code></pre>
<p>--verify 选项可以与该工具一起使用来检查分区重新分配的状态。请注意，相同的 custom-reassignment.json （与 --execute 选项一起使用）应与 --verify 选项一起使用：</p>
<pre><code class="language-bash">&gt; bin/kafka-reassign-partitions.sh --bootstrap-server localhost:9092 --reassignment-json-file custom-reassignment.json --verify
Status of partition reassignment:
Reassignment of partition [foo1,0] is completed
Reassignment of partition [foo2,1] is completed
</code></pre>
<h3 id="broker"><a href="https://kafka.apache.org/documentation/#basic_ops_decommissioning_brokers">退役broker</a></h3>
<p>分区重新分配工具尚不具备为退役代理自动生成重新分配计划的能力。因此，管理员必须制定一个重新分配计划，将要停用的代理上托管的所有分区的副本移动到其余代理。这可能相对繁琐，因为重新分配需要确保所有副本不会从已停用的代理仅移动到另一个代理。为了使这个过程变得轻松，我们计划在未来为退役broker添加工具支持。</p>
<h3 id="_12"><a href="https://kafka.apache.org/documentation/#basic_ops_increase_replication_factor">增加复制因子</a></h3>
<p>增加现有分区的复制因子很容易。只需在自定义重新分配 json 文件中指定额外的副本，并将其与 --execute 选项一起使用即可增加指定分区的复制因子。</p>
<p>例如，以下示例将主题 foo 的分区 0 的复制因子从 1 增加到 3。在增加复制因子之前，该分区的唯一副本存在于代理 5 上。作为增加复制因子的一部分，我们将在代理 5 上添加更多副本broker 6 和 7。</p>
<p>第一步是在 json 文件中手工制定自定义重新分配计划：</p>
<pre><code class="language-bash">&gt; cat increase-replication-factor.json
{&quot;version&quot;:1,
&quot;partitions&quot;:[{&quot;topic&quot;:&quot;foo&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[5,6,7]}]}
</code></pre>
<p>然后，使用带有 --execute 选项的 json 文件来启动重新分配过程：</p>
<pre><code class="language-bash">&gt; bin/kafka-reassign-partitions.sh --bootstrap-server localhost:9092 --reassignment-json-file increase-replication-factor.json --execute
Current partition replica assignment

{&quot;version&quot;:1,
&quot;partitions&quot;:[{&quot;topic&quot;:&quot;foo&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[5]}]}

Save this to use as the --reassignment-json-file option during rollback
Successfully started partition reassignment for foo-0
</code></pre>
<p>--verify 选项可以与该工具一起使用来检查分区重新分配的状态。请注意，相同的increase-replication-factor.json（与--execute选项一起使用）应与--verify选项一起使用：</p>
<pre><code class="language-bash">&gt; bin/kafka-reassign-partitions.sh --bootstrap-server localhost:9092 --reassignment-json-file increase-replication-factor.json --verify
Status of partition reassignment:
Reassignment of partition [foo,0] is completed
</code></pre>
<p>您还可以使用 kafka-topics 工具验证复制因子的增加：</p>
<pre><code class="language-bash">&gt; bin/kafka-topics.sh --bootstrap-server localhost:9092 --topic foo --describe
Topic:foo   PartitionCount:1    ReplicationFactor:3 Configs:
  Topic: foo    Partition: 0    Leader: 5   Replicas: 5,6,7 Isr: 5,6,7
</code></pre>
<h3 id="_13"><a href="https://kafka.apache.org/documentation/#rep-throttle">限制数据迁移期间的带宽使用</a></h3>
<p>Kafka 允许您对复制流量进行限制，设置用于在机器之间移动副本的带宽上限。这在重新平衡集群、引导新代理或添加或删除代理时非常有用，因为它限制了这些数据密集型操作对用户产生的影响。</p>
<p>有两个接口可用于接合油门。最简单、最安全的方法是在调用  <code>kafka-reassign-partitions.sh</code> 时应用限制，但 <code>kafka-configs.sh</code> 也可用于直接查看和更改限制值。</p>
<p>例如，如果您要使用以下命令执行重新平衡，它将以不超过 50MB/s 的速度移动分区。</p>
<pre><code class="language-bash">$ bin/kafka-reassign-partitions.sh --bootstrap-server localhost:9092 --execute --reassignment-json-file big-cluster.json --throttle 50000000
</code></pre>
<p>当您执行此脚本时，您将看到油门接合：</p>
<pre><code class="language-bash">The inter-broker throttle limit was set to 50000000 B/s
Successfully started partition reassignment for foo1-0
</code></pre>
<p>如果您希望在重新平衡期间改变油门，例如增加吞吐量，以便更快地完成，您可以通过使用传递相同的重新分配 json 文件的 --additional 选项重新运行执行命令来完成此操作：</p>
<pre><code class="language-bash">$ bin/kafka-reassign-partitions.sh --bootstrap-server localhost:9092  --additional --execute --reassignment-json-file bigger-cluster.json --throttle 700000000
  The inter-broker throttle limit was set to 700000000 B/s
</code></pre>
<p>重新平衡完成后，管理员可以使用 --verify 选项检查重新平衡的状态。如果重新平衡已完成，则将通过 --verify 命令删除限制。重要的是，管理员通过运行带有 --verify 选项的命令，在重新平衡完成后及时取消限制。如果不这样做可能会导致常规复制流量受到限制。</p>
<p>当执行 --verify 选项并且重新分配完成时，脚本将确认限制已被删除：</p>
<pre><code class="language-bash">&gt; bin/kafka-reassign-partitions.sh --bootstrap-server localhost:9092  --verify --reassignment-json-file bigger-cluster.json
Status of partition reassignment:
Reassignment of partition [my-topic,1] is completed
Reassignment of partition [my-topic,0] is completed

Clearing broker-level throttles on brokers 1,2,3
Clearing topic-level throttles on topic my-topic
</code></pre>
<p>管理员还可以使用 kafka-configs.sh 验证分配的配置。有两对节流配置用于管理节流过程。第一对指油门值本身。这是在代理级别使用动态属性进行配置的：</p>
<pre><code class="language-text">leader.replication.throttled.rate
follower.replication.throttled.rate
</code></pre>
<p>然后是限制副本的枚举集的配置对：</p>
<pre><code class="language-text">leader.replication.throttled.replicas
follower.replication.throttled.replicas
</code></pre>
<p>哪些是按主题配置的。</p>
<p>所有四个配置值均由 <code>kafka-reassign-partitions.sh</code> 自动分配（如下所述）。</p>
<p>查看油门限制配置：</p>
<pre><code class="language-bash">&gt; bin/kafka-configs.sh --describe --bootstrap-server localhost:9092 --entity-type brokers
Configs for brokers '2' are leader.replication.throttled.rate=700000000,follower.replication.throttled.rate=700000000
Configs for brokers '1' are leader.replication.throttled.rate=700000000,follower.replication.throttled.rate=700000000
</code></pre>
<p>这显示了应用于复制协议的领导者和跟随者端的限制。默认情况下，双方都分配有相同的限制吞吐量值。</p>
<p>要查看受限制的副本列表：</p>
<pre><code class="language-bash">&gt; bin/kafka-configs.sh --describe --bootstrap-server localhost:9092 --entity-type topics
Configs for topic 'my-topic' are leader.replication.throttled.replicas=1:102,0:101,
    follower.replication.throttled.replicas=1:101,0:102
</code></pre>
<p>在这里，我们看到领导者节流应用于代理 102 上的分区 1 和代理 101 上的分区 0。同样，追随者节流应用于代理 101 上的分区 1 和代理 102 上的分区 0。</p>
<p>默认情况下，kafka-reassign-partitions.sh 会将领导者限制应用于重新平衡之前存在的所有副本，其中任何一个都可能是领导者。它将对所有移动目的地应用跟随油门。因此，如果代理 101,102 上有一个具有副本的分区，被重新分配给 102,103，则该分区的领导者限制将应用于 101,102，追随者限制将仅应用于 103。</p>
<p>如果需要，您还可以使用 kafka-configs.sh 上的 --alter 开关手动更改节流配置。</p>
<h4 id="_14">安全使用限制复制</h4>
<p>使用限制复制时应小心。尤其：</p>
<p><em>(1) Throttle Removal:</em></p>
<p>重新分配完成后应及时移除限制（通过运行 <code>kafka-reassign-partitions.sh --verify</code>）。</p>
<p><em>(2) Ensuring Progress:</em></p>
<p>如果与传入写入速率相比，限制设置得太低，复制可能无法取得进展。出现这种情况时：</p>
<pre><code class="language-text">max(BytesInPerSec) &gt; throttle
</code></pre>
<p>其中 BytesInPerSec 是监控生产者写入每个代理的吞吐量的指标。</p>
<p>管理员可以在重新平衡期间使用以下指标监控复制是否取得进展：</p>
<pre><code class="language-bash">kafka.server:type=FetcherLagMetrics,name=ConsumerLag,clientId=([-.\w]+),topic=([-.\w]+),partition=([0-9]+)
</code></pre>
<p>复制过程中滞后应该不断减少。如果指标没有减少，管理员应如上所述增加限制吞吐量。</p>
<h3 id="_15"><a href="https://kafka.apache.org/documentation/#quotas">设置配额</a></h3>
<p>配额覆盖和默认值可以在（用户、客户端 ID）、用户或客户端 ID 级别进行配置，如此处<a href="https://kafka.apache.org/documentation/#design_quotas">所述</a>。默认情况下，客户端获得无限配额。可以为每个（用户、客户端 ID）、用户或客户端 ID 组设置自定义配额。</p>
<p>配置自定义配额（user=user1，client-id=clientA）：</p>
<pre><code class="language-bash">&gt; bin/kafka-configs.sh  --bootstrap-server localhost:9092 --alter --add-config 'producer_byte_rate=1024,consumer_byte_rate=2048,request_percentage=200' --entity-type users --entity-name user1 --entity-type clients --entity-name clientA
Updated config for entity: user-principal 'user1', client-id 'clientA'.
</code></pre>
<p>为 user=user1 配置自定义配额：</p>
<pre><code class="language-bash">&gt; bin/kafka-configs.sh  --bootstrap-server localhost:9092 --alter --add-config 'producer_byte_rate=1024,consumer_byte_rate=2048,request_percentage=200' --entity-type users --entity-name user1
Updated config for entity: user-principal 'user1'.
</code></pre>
<p>为 client-id=clientA 配置自定义配额：</p>
<pre><code class="language-bash">&gt; bin/kafka-configs.sh  --bootstrap-server localhost:9092 --alter --add-config 'producer_byte_rate=1024,consumer_byte_rate=2048,request_percentage=200' --entity-type clients --entity-name clientA
Updated config for entity: client-id 'clientA'.
</code></pre>
<p>通过指定 --entity-default 选项而不是<em>--entity-name ，</em> 可以为每个（用户、客户端 ID）、用户或客户端 ID 组设置<em>默认</em>配额。</p>
<p>为 user=userA 配置默认 client-id 配额：</p>
<pre><code class="language-bash">&gt; bin/kafka-configs.sh  --bootstrap-server localhost:9092 --alter --add-config 'producer_byte_rate=1024,consumer_byte_rate=2048,request_percentage=200' --entity-type users --entity-name user1 --entity-type clients --entity-default
Updated config for entity: user-principal 'user1', default client-id.
</code></pre>
<p>为用户配置默认配额：</p>
<pre><code class="language-bash">&gt; bin/kafka-configs.sh  --bootstrap-server localhost:9092 --alter --add-config 'producer_byte_rate=1024,consumer_byte_rate=2048,request_percentage=200' --entity-type users --entity-default
Updated config for entity: default user-principal.
</code></pre>
<p>配置 client-id 的默认配额：</p>
<pre><code class="language-bash">&gt; bin/kafka-configs.sh  --bootstrap-server localhost:9092 --alter --add-config 'producer_byte_rate=1024,consumer_byte_rate=2048,request_percentage=200' --entity-type clients --entity-default
Updated config for entity: default client-id.
</code></pre>
<p>以下是描述给定（用户、客户端 ID）的配额的方法：</p>
<pre><code class="language-bash">&gt; bin/kafka-configs.sh  --bootstrap-server localhost:9092 --describe --entity-type users --entity-name user1 --entity-type clients --entity-name clientA
Configs for user-principal 'user1', client-id 'clientA' are producer_byte_rate=1024,consumer_byte_rate=2048,request_percentage=200
</code></pre>
<p>描述给定用户的配额：</p>
<pre><code class="language-bash">&gt; bin/kafka-configs.sh  --bootstrap-server localhost:9092 --describe --entity-type users --entity-name user1
Configs for user-principal 'user1' are producer_byte_rate=1024,consumer_byte_rate=2048,request_percentage=200
</code></pre>
<p>描述给定客户端 ID 的配额：</p>
<pre><code class="language-bash">&gt; bin/kafka-configs.sh  --bootstrap-server localhost:9092 --describe --entity-type clients --entity-name clientA
Configs for client-id 'clientA' are producer_byte_rate=1024,consumer_byte_rate=2048,request_percentage=200
</code></pre>
<p>如果未指定实体名称，则描述指定类型的所有实体。例如，描述所有用户：</p>
<pre><code class="language-bash">&gt; bin/kafka-configs.sh  --bootstrap-server localhost:9092 --describe --entity-type users
Configs for user-principal 'user1' are producer_byte_rate=1024,consumer_byte_rate=2048,request_percentage=200
Configs for default user-principal are producer_byte_rate=1024,consumer_byte_rate=2048,request_percentage=200
</code></pre>
<p>类似地对于（用户，客户端）：</p>
<pre><code class="language-bash">&gt; bin/kafka-configs.sh  --bootstrap-server localhost:9092 --describe --entity-type users --entity-type clients
Configs for user-principal 'user1', default client-id are producer_byte_rate=1024,consumer_byte_rate=2048,request_percentage=200
Configs for user-principal 'user1', client-id 'clientA' are producer_byte_rate=1024,consumer_byte_rate=2048,request_percentage=200
</code></pre>
<h2 id="62"><a href="https://kafka.apache.org/documentation/#datacenters">6.2 数据中心</a></h2>
<p>某些部署需要管理跨越多个数据中心的数据管道。我们推荐的方法是在每个数据中心部署一个本地 Kafka 集群，每个数据中心中的应用程序实例仅与其本地集群交互并在集群之间镜像数据（有关如何执行此操作的信息， 请参阅<a href="https://kafka.apache.org/documentation/#georeplication">异地复制</a>文档）。</p>
<p>这种部署模式允许数据中心充当独立实体，并允许我们集中管理和调整数据中心间的复制。即使数据中心间链路不可用，这也允许每个设施独立运行：当发生这种情况时，镜像会落后，直到链路恢复，此时它会赶上。</p>
<p>对于需要所有数据的全局视图的应用程序，您可以使用镜像来提供具有从所有<em>数据</em>中心的本地集群镜像的聚合数据的集群。这些聚合集群用于由需要完整数据集的应用程序进行读取。</p>
<p>这不是唯一可能的部署模式。可以通过 WAN 读取或写入远程 Kafka 集群，但显然这会增加获取集群所需的延迟。</p>
<p>Kafka 自然地在生产者和消费者中对数据进行批处理，因此即使在高延迟连接下也能实现高吞吐量。为了实现这一点，可能需要使用<code>socket.send.buffer.bytes</code>和<code>socket.receive.buffer.bytes</code>配置来增加生产者、消费者和代理的 TCP 套接字缓冲区大小。<a href="http://en.wikipedia.org/wiki/Bandwidth-delay_product">此处</a>记录了设置此值的适当方法。</p>
<p>通常<em>不</em>建议通过高延迟链路运行跨多个数据中心的<em>单个Kafka 集群。</em>这将导致 Kafka 写入和 ZooKeeper 写入产生非常高的复制延迟，并且如果位置之间的网络不可用，Kafka 和 ZooKeeper 都不会在所有位置保持可用。</p>
<h2 id="63"><a href="https://kafka.apache.org/documentation/#georeplication">6.3 异地复制（跨集群数据镜像）</a></h2>
<h3 id="_16"><a href="https://kafka.apache.org/documentation/#georeplication-overview">异地复制概述</a></h3>
<p>Kafka 管理员可以定义跨越各个 Kafka 集群、数据中心或地理区域边界的数据流。组织、技术或法律要求通常需要此类事件流设置。常见场景包括：</p>
<ul>
<li>异地复制</li>
<li>灾难恢复</li>
<li>将边缘集群馈送到中央聚合集群</li>
<li>集群的物理隔离（例如生产与测试）</li>
<li>云迁移或混合云部署</li>
<li>法律和合规要求</li>
</ul>
<p>管理员可以使用 Kafka 的 MirrorMaker（版本 2）设置此类集群间数据流，这是一种以流式传输方式在不同 Kafka 环境之间复制数据的工具。MirrorMaker 构建在 Kafka Connect 框架之上，支持以下功能：</p>
<ul>
<li>复制主题（数据加配置）</li>
<li>复制消费者组，包括在集群之间迁移应用程序的偏移量</li>
<li>复制 ACL</li>
<li>保留分区</li>
<li>自动检测新主题和分区</li>
<li>提供广泛的指标，例如跨多个数据中心/集群的端到端复制延迟</li>
<li>容错和水平可扩展的操作</li>
</ul>
<p><em>注意：使用 MirrorMaker 进行异地复制可跨 Kafka 集群复制数据。<a href="https://kafka.apache.org/documentation/#replication">这种集群间复制与 Kafka 的集群内复制</a>不同，后者在同一个 Kafka 集群内复制数据。</em></p>
<h3 id="_17"><a href="https://kafka.apache.org/documentation/#georeplication-flows">什么是复制流</a></h3>
<p>借助 MirrorMaker，Kafka 管理员可以将主题、主题配置、消费者组及其偏移量以及 ACL 从一个或多个源 Kafka 集群复制到一个或多个目标 Kafka 集群，即跨集群环境。简而言之，MirrorMaker 使用连接器从源集群进行消费并生产到目标集群。</p>
<p>这些从源集群到目标集群的定向流称为复制流。它们是使用 MirrorMaker 配置文件中的格式定义的，<code>{source_cluster}-&gt;{target_cluster}</code>如下所述。管理员可以根据这些流程创建复杂的复制拓扑。</p>
<p>以下是一些示例模式：</p>
<ul>
<li>主动/主动高可用性部署：<code>A-&gt;B, B-&gt;A</code></li>
<li>主动/被动或主动/备用高可用性部署：<code>A-&gt;B</code></li>
<li>聚合（例如，从多个集群到一个集群）：<code>A-&gt;K, B-&gt;K, C-&gt;K</code></li>
<li>扇出（例如，从一个集群到多个集群）：<code>K-&gt;A, K-&gt;B, K-&gt;C</code></li>
<li>转发：<code>A-&gt;B, B-&gt;C, C-&gt;D</code></li>
</ul>
<p>默认情况下，流会复制所有主题和消费者组。但是，每个复制流都可以独立配置。例如，您可以定义仅将特定主题或消费者组从源集群复制到目标集群。</p>
<p>以下是有关如何配置从<code>primary</code>集群到<code>secondary</code>集群的数据复制（主动/被动设置）的第一个示例：</p>
<pre><code class="language-text"># Basic settings
clusters = primary, secondary
primary.bootstrap.servers = broker3-primary:9092
secondary.bootstrap.servers = broker5-secondary:9092

# Define replication flows
primary-&gt;secondary.enabled = true
primary-&gt;secondary.topics = foobar-topic, quux-.*
</code></pre>
<h3 id="_18"><a href="https://kafka.apache.org/documentation/#georeplication-mirrormaker">配置异地复制</a></h3>
<p>以下部分介绍如何配置和运行专用 MirrorMaker 集群。如果您想在现有 Kafka Connect 集群或其他支持的部署设置中运行 MirrorMaker，请参阅<a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-382%3A+MirrorMaker+2.0">KIP-382：MirrorMaker 2.0</a>，并注意配置设置的名称可能因部署模式而异。</p>
<p>除了以下部分中介绍的内容之外，有关配置设置的更多示例和信息可在以下位置找到：</p>
<ul>
<li><a href="https://github.com/apache/kafka/blob/trunk/connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorMakerConfig.java">MirrorMakerConfig</a> , <a href="https://github.com/apache/kafka/blob/trunk/connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorConnectorConfig.java">MirrorConnectorConfig</a></li>
<li><a href="https://github.com/apache/kafka/blob/trunk/connect/mirror/src/main/java/org/apache/kafka/connect/mirror/DefaultTopicFilter.java">DefaultTopicFilter</a>用于主题，<a href="https://github.com/apache/kafka/blob/trunk/connect/mirror/src/main/java/org/apache/kafka/connect/mirror/DefaultGroupFilter.java">DefaultGroupFilter</a>用于消费者组</li>
<li><a href="https://github.com/apache/kafka/blob/trunk/config/connect-mirror-maker.properties">connect-mirror-maker.properties</a>中的配置设置示例，<a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-382%3A+MirrorMaker+2.0">KIP-382：MirrorMaker 2.0</a></li>
</ul>
<h4 id="_19"><a href="https://kafka.apache.org/documentation/#georeplication-config-syntax">配置文件语法</a></h4>
<p>MirrorMaker 配置文件通常命名为<code>connect-mirror-maker.properties</code>. 您可以在此文件中配置各种组件：</p>
<ul>
<li>MirrorMaker 设置：全局设置，包括集群定义（别名）以及每个复制流的自定义设置</li>
<li>Kafka Connect 和连接器设置</li>
<li>Kafka 生产者、消费者和管理客户端设置</li>
</ul>
<p>示例：定义 MirrorMaker 设置（稍后详细说明）。</p>
<pre><code class="language-text"># Global settings
clusters = us-west, us-east   # defines cluster aliases
us-west.bootstrap.servers = broker3-west:9092
us-east.bootstrap.servers = broker5-east:9092

topics = .*   # all topics to be replicated by default

# Specific replication flow settings (here: flow from us-west to us-east)
us-west-&gt;us-east.enabled = true
us-west-&gt;us.east.topics = foo.*, bar.*  # override the default above
</code></pre>
<p>MirrorMaker 基于 Kafka Connect 框架。<a href="https://kafka.apache.org/documentation/#connectconfigs">Kafka Connect 文档章节</a>中描述的任何 Kafka Connect、源连接器和接收器连接器设置都可以直接在 MirrorMaker 配置中使用，而无需更改配置设置的名称或为其添加前缀。</p>
<p>示例：定义 MirrorMaker 使用的自定义 Kafka Connect 设置。</p>
<pre><code class="language-text"># Setting Kafka Connect defaults for MirrorMaker
tasks.max = 5
</code></pre>
<p>大多数默认的 Kafka Connect 设置对于开箱即用的 MirrorMaker 都能很好地工作，但<code>tasks.max</code>. 为了在多个 MirrorMaker 进程之间均匀分配工作负载，建议根据可用硬件资源和要复制的主题分区总数 设置<code>tasks.max</code>为至少（最好更高）。<code>2</code></p>
<p><em>您可以进一步自定义每个源或目标集群</em> 的 MirrorMaker 的 Kafka Connect 设置（更准确地说，您可以“每个连接器”指定 Kafka Connect 工作线程级别的配置设置）。<code>{cluster}.{config_name}</code>使用MirrorMaker 配置文件中 的格式。</p>
<p>示例：为<code>us-west</code>集群定义自定义连接器设置。</p>
<pre><code class="language-text"># us-west custom settings
us-west.offset.storage.topic = my-mirrormaker-offsets
</code></pre>
<p>MirrorMaker 内部使用 Kafka 生产者、消费者和管理客户端。通常需要为这些客户端进行自定义设置。要覆盖默认值，请在 MirrorMaker 配置文件中使用以下格式：</p>
<ul>
<li><code>{source}.consumer.{consumer_config_name}</code></li>
<li><code>{target}.producer.{producer_config_name}</code></li>
<li><code>{source_or_target}.admin.{admin_config_name}</code></li>
</ul>
<p>示例：定义自定义生产者、消费者、管理客户端设置。</p>
<pre><code class="language-text"># us-west cluster (from which to consume)
us-west.consumer.isolation.level = read_committed
us-west.admin.bootstrap.servers = broker57-primary:9092

# us-east cluster (to which to produce)
us-east.producer.compression.type = gzip
us-east.producer.buffer.memory = 32768
us-east.admin.bootstrap.servers = broker8-secondary:9092
</code></pre>
<h4 id="exactly-once"><a href="https://kafka.apache.org/documentation/#georeplication-exactly_once">Exactly once</a></h4>
<p>从3.5.0版本开始，专用的MirrorMaker集群支持精确一次语义。</p>
<p>对于新的MirrorMaker集群，将 <code>exactly.once.source.support</code>属性设置为启用所有目标Kafka集群，这些集群应该使用精确一次语义写入。例如，要为集群us-east的写入启用精确一次，可以使用以下配置：</p>
<pre><code class="language-text">us-east.exactly.once.source.support = enabled
</code></pre>
<p>对于现有的MirrorMaker集群，需要两步升级。与其立即将theexactly<code>exactly.once.source.support</code>属性设置为启用，不如首先将其设置为在集群中的所有节点上<code>preparing</code>。一旦完成，可以在第二轮重新启动中将其设置为在集群中的所有节点上启用。</p>
<p>无论哪种情况，都需要启用MirrorMaker节点之间的集群内通信，如<a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-710%3A+Full+support+for+distributed+mode+in+dedicated+MirrorMaker+2.0+clusters">KIP-710</a>所述。为此，<code>dedicated.mode.enable.internal.rest</code>属性必须设置为<code>true</code>。此外，<a href="https://kafka.apache.org/documentation/#connectconfigs">可用于Kafka Connect</a>的许多REST相关<a href="https://kafka.apache.org/documentation/#connectconfigs">配置属性</a>可以指定MirrorMaker配置。例如，要启用MirrorMaker集群中与本地计算机80端口80上监听的每个节点进行集群内部通信，应将以下内容添加到MirrorMaker配置文件中：</p>
<pre><code class="language-text">dedicated.mode.enable.internal.rest = true
listeners = http://localhost:8080
</code></pre>
<p>运行MirrorMaker时，还建议从已复制的数据中筛选已中止事务中的记录。为此，请确保将用于从源集群读取的使用者配置为隔离级别设置为read_committed。如果从us west集群复制数据，则可以通过将以下内容添加到MirrorMaker配置文件中，对从该集群读取的所有复制流执行此操作：</p>
<p><strong>请注意，如果在生产环境中启用集群内部通信，强烈建议保护每个MirrorMaker节点带来的REST服务器。有关如何实现此操作的信息<a href="https://kafka.apache.org/documentation/#connectconfigs">，</a>请参阅<a href="https://kafka.apache.org/documentation/#connectconfigs">Kafka Connect</a>的<a href="https://kafka.apache.org/documentation/#connectconfigs">配置属性</a>。</strong></p>
<p>还建议在运行 MirrorMaker 时从复制数据中过滤掉来自已中止事务的记录。 为此，请确保用于从源集群读取的使用者配置为 <code>isolation.level</code> 设置为 <code>read_committed</code>。 如果从集群 <code>us-west</code> 复制数据，则可以通过将以下内容添加到 MirrorMaker 配置文件来对从该集群读取的所有复制流完成此操作：</p>
<pre><code class="language-text">us-west.consumer.isolation.level = read_committed
</code></pre>
<p>最后一点，在引擎盖下，MirrorMaker使用Kafka Connect源连接器来复制数据。有关此类连接器的精确一次支持的更多信息，请参阅<a href="https://kafka.apache.org/documentation/#connect_exactlyoncesource">相关文档页面</a>。</p>
<h4 id="_20"><a href="https://kafka.apache.org/documentation/#georeplication-flow-create">创建和启用复制流</a></h4>
<p>要定义复制流，您必须首先在 MirrorMaker 配置文件中定义相应的源和目标 Kafka 集群。</p>
<ul>
<li><code>clusters</code>（必需）：以逗号分隔的 Kafka 集群“别名”列表</li>
<li><code>{clusterAlias}.bootstrap.servers</code>（必填）：特定集群的连接信息；“引导”Kafka broker的逗号分隔列表</li>
</ul>
<p>示例：定义两个集群别名<code>primary</code>和<code>secondary</code>，包括它们的连接信息。</p>
<pre><code class="language-text">clusters = primary, secondary
primary.bootstrap.servers = broker10-primary:9092,broker-11-primary:9092
secondary.bootstrap.servers = broker5-secondary:9092,broker6-secondary:9092
</code></pre>
<p><code>{source}-&gt;{target}.enabled = true</code>其次，您必须根据需要 显式启用各个复制流。请记住，流是定向的：如果需要双向（双向）复制，则必须启用两个方向的流。</p>
<pre><code class="language-text"># Enable replication from primary to secondary
primary-&gt;secondary.enabled = true
</code></pre>
<p>默认情况下，复制流会将除少数特殊主题和使用者组之外的所有内容从源集群复制到目标集群，并自动检测任何新创建的主题和组。目标集群中复制主题的名称将以源集群的名称为前缀（请参阅下面的进一步部分）。例如，<code>foo</code>源集群中的主题将被复制到目标集群中<code>us-west</code>命名的主题。 <code>us-west.foo``us-east</code></p>
<p>后续部分解释如何根据您的需要自定义此基本设置。</p>
<h4 id="_21"><a href="https://kafka.apache.org/documentation/#georeplication-flow-configure">配置复制流</a></h4>
<p>复制流的配置是顶级默认设置（例如<code>topics</code>）的组合，在其之上应用特定于流的设置（如果有）（例如<code>us-west-&gt;us-east.topics</code>）。要更改顶级默认设置，请将相应的顶级设置添加到 MirrorMaker 配置文件中。要仅覆盖特定复制流的默认值，请使用语法 format <code>{source}-&gt;{target}.{config.name}</code>。</p>
<p>最重要的设置是：</p>
<ul>
<li><code>topics</code>：主题列表或正则表达式，定义源集群中要复制的主题（默认值<code>topics = .*</code>：）</li>
<li><code>topics.exclude</code>：主题列表或正则表达式，用于随后排除与设置匹配的主题<code>topics</code>（默认值<code>topics.exclude = .*[\-.]internal, .*.replica, __.*</code>：）</li>
<li><code>groups</code>：主题或正则表达式列表，定义源集群中要复制的消费者组（默认值<code>groups = .*</code>：）</li>
<li><code>groups.exclude</code>：主题列表或正则表达式，用于随后排除与设置匹配的消费者组<code>groups</code>（默认值<code>groups.exclude = console-consumer-.*, connect-.*, __.*</code>：）</li>
<li><code>{source}-&gt;{target}.enable</code>：设置<code>true</code>为启用复制流（默认值<code>false</code>：）</li>
</ul>
<p>例子：</p>
<pre><code class="language-text"># Custom top-level defaults that apply to all replication flows
topics = .*
groups = consumer-group1, consumer-group2

# Don't forget to enable a flow!
us-west-&gt;us-east.enabled = true

# Custom settings for specific replication flows
us-west-&gt;us-east.topics = foo.*
us-west-&gt;us-east.groups = bar.*
us-west-&gt;us-east.emit.heartbeats = false
</code></pre>
<p>支持其他配置设置，下面列出了其中一些。在大多数情况下，您可以将这些设置保留为默认值。有关更多详细信息，请参阅<a href="https://github.com/apache/kafka/blob/trunk/connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorMakerConfig.java">MirrorMakerConfig</a>和<a href="https://github.com/apache/kafka/blob/trunk/connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorConnectorConfig.java">MirrorConnectorConfig</a>。</p>
<ul>
<li><code>refresh.topics.enabled</code>：是否定期检查源集群中的新主题（默认：true）</li>
<li><code>refresh.topics.interval.seconds</code>：在源集群中检查新主题的频率；低于默认值可能会导致性能下降（默认值：600，每十分钟）</li>
<li><code>refresh.groups.enabled</code>：是否定期检查源集群中是否有新的消费者组（默认：true）</li>
<li><code>refresh.groups.interval.seconds</code>：检查源集群中新消费者组的频率；低于默认值可能会导致性能下降（默认值：600，每十分钟）</li>
<li><code>sync.topic.configs.enabled</code>：是否从源集群复制主题配置（默认：true）</li>
<li><code>sync.topic.acls.enabled</code>：是否同步源集群的ACL（默认：true）</li>
<li><code>emit.heartbeats.enabled</code>：是否定期发出心跳（默认：true）</li>
<li><code>emit.heartbeats.interval.seconds</code>：发出心跳的频率（默认值：1，每隔一秒）</li>
<li><code>heartbeats.topic.replication.factor</code>：MirrorMaker内部心跳主题的复制因子（默认：3）</li>
<li><code>emit.checkpoints.enabled</code>：是否定期发出 MirrorMaker 的消费者偏移量（默认值：true）</li>
<li><code>emit.checkpoints.interval.seconds</code>：发出检查点的频率（默认值：60，每分钟）</li>
<li><code>checkpoints.topic.replication.factor</code>：MirrorMaker 内部检查点主题的复制因子（默认值：3）</li>
<li><code>sync.group.offsets.enabled``__consumer_offsets</code>：只要该组中没有活动消费者连接到目标集群，是否 定期将复制的消费者组（在源集群中）的翻译偏移量写入目标集群中的主题（默认值：false）</li>
<li><code>sync.group.offsets.interval.seconds</code>：消费者组偏移量同步的频率（默认值：60，每分钟）</li>
<li><code>offset-syncs.topic.replication.factor</code>：MirrorMaker内部偏移同步主题的复制因子（默认值：3）</li>
</ul>
<h4 id="_22"><a href="https://kafka.apache.org/documentation/#georeplication-flow-secure">保护复制流</a></h4>
<p><a href="https://kafka.apache.org/documentation/#connectconfigs">MirrorMaker 支持与 Kafka Connect</a> 相同的安全设置，因此请参阅链接部分以获取更多信息。</p>
<p>示例：加密 MirrorMaker 与<code>us-east</code>集群之间的通信。</p>
<pre><code class="language-text">us-east.security.protocol=SSL
us-east.ssl.truststore.location=/path/to/truststore.jks
us-east.ssl.truststore.password=my-secret-password
us-east.ssl.keystore.location=/path/to/keystore.jks
us-east.ssl.keystore.password=my-secret-password
us-east.ssl.key.password=my-secret-password
</code></pre>
<h4 id="_23"><a href="https://kafka.apache.org/documentation/#georeplication-topic-naming">目标集群中复制主题的自定义命名</a></h4>
<p>目标集群中的复制主题（有时称为<em>远程</em>主题）根据复制策略进行重命名。MirrorMaker 使用此策略来确保来自不同集群的事件（也称为记录、消息）不会写入同一主题分区。默认情况下，根据<a href="https://github.com/apache/kafka/blob/trunk/connect/mirror-client/src/main/java/org/apache/kafka/connect/mirror/DefaultReplicationPolicy.java">DefaultReplicationPolicy</a>，目标集群中复制主题的名称采用以下格式<code>{source}.{source_topic_name}</code>：</p>
<pre><code class="language-text">us-west         us-east
=========       =================
                bar-topic
foo-topic  --&gt;  us-west.foo-topic
</code></pre>
<p>您可以使用以下设置自定义分隔符（默认：<code>.</code>）<code>replication.policy.separator</code>：</p>
<pre><code class="language-text"># Defining a custom separator
us-west-&gt;us-east.replication.policy.separator = _
</code></pre>
<p>如果您需要进一步控制复制主题的命名方式，您可以在 MirrorMaker 配置中 实现自定义<code>ReplicationPolicy</code>并覆盖<code>replication.policy.class</code>（默认为）。<code>DefaultReplicationPolicy</code></p>
<h4 id="_24"><a href="https://kafka.apache.org/documentation/#georeplication-config-conflicts">防止配置冲突</a></h4>
<p>MirrorMaker 进程通过其目标 Kafka 集群共享配置。当针对同一目标集群运行的 MirrorMaker 进程之间的配置不同时，此行为可能会导致冲突。</p>
<p>例如，以下两个 MirrorMaker 进程将是活泼的：</p>
<pre><code class="language-text"># Configuration of process 1
A-&gt;B.enabled = true
A-&gt;B.topics = foo

# Configuration of process 2
A-&gt;B.enabled = true
A-&gt;B.topics = bar
</code></pre>
<p>在这种情况下，两个进程将通过 cluster 共享配置<code>B</code>，这会导致冲突。根据两个进程中的哪一个被选举为“领导者”，结果将是主题<code>foo</code>或主题<code>bar</code>被复制，但不会同时复制。</p>
<p>因此，在同一目标集群的复制流中保持 MirrorMaker 配置一致非常重要。例如，这可以通过自动化工具或为整个组织使用单个共享的 MirrorMaker 配置文件来实现。</p>
<h4 id="_25"><a href="https://kafka.apache.org/documentation/#georeplication-best-practice">最佳实践：从远程消费，生产到本地</a></h4>
<p>为了最大限度地减少延迟（“生产者滞后”），建议将 MirrorMaker 进程放置在尽可能靠近其目标集群（即它向其生成数据的集群）的位置。这是因为 Kafka 生产者通常比 Kafka 消费者更容易遇到不可靠或高延迟的网络连接。</p>
<pre><code class="language-text">First DC          Second DC
==========        =========================
primary --------- MirrorMaker --&gt; secondary
(remote)                           (local)
</code></pre>
<p>要运行这样的“从远程使用，生成到本地”设置，请在靠近目标集群且最好在与目标集群相同的位置运行 MirrorMaker 进程，并在命令行参数（空白分隔列表）中显式设置这些“本地”<code>--clusters</code>集群集群别名）：</p>
<pre><code class="language-text"># Run in secondary's data center, reading from the remote `primary` cluster
$ ./bin/connect-mirror-maker.sh connect-mirror-maker.properties --clusters secondary
</code></pre>
<p>它<code>--clusters secondary</code>告诉 MirrorMaker 进程给定的集群位于附近，并阻止其复制数据或将配置发送到其他远程位置的集群。</p>
<h4 id="_26"><a href="https://kafka.apache.org/documentation/#georeplication-example-active-passive">示例：主动/被动高可用性部署</a></h4>
<p>以下示例显示了将主题从主 Kafka 环境复制到辅助 Kafka 环境的基本设置，但不从辅助 Kafka 环境复制回主环境。请注意，大多数生产设置都需要进一步配置，例如安全设置。</p>
<pre><code class="language-text"># Unidirectional flow (one-way) from primary to secondary cluster
primary.bootstrap.servers = broker1-primary:9092
secondary.bootstrap.servers = broker2-secondary:9092

primary-&gt;secondary.enabled = true
secondary-&gt;primary.enabled = false

primary-&gt;secondary.topics = foo.*  # only replicate some topics
</code></pre>
<h4 id="_27"><a href="https://kafka.apache.org/documentation/#georeplication-example-active-active">示例：主动/主动高可用性部署</a></h4>
<p>以下示例显示了以两种方式在两个集群之间复制主题的基本设置。请注意，大多数生产设置都需要进一步配置，例如安全设置。</p>
<pre><code class="language-text"># Bidirectional flow (two-way) between us-west and us-east clusters
clusters = us-west, us-east
us-west.bootstrap.servers = broker1-west:9092,broker2-west:9092
Us-east.bootstrap.servers = broker3-east:9092,broker4-east:9092

us-west-&gt;us-east.enabled = true
us-east-&gt;us-west.enabled = true
</code></pre>
<p><em>关于防止复制“循环”的注意事项（其中主题最初从 A 复制到 B，然后复制的主题将再次从 B 复制到 A，依此类推）：只要您在同一个 MirrorMaker 中定义上述</em>流程配置文件中，您不需要显式添加<code>topics.exclude</code>设置来防止两个集群之间的复制循环。</p>
<h4 id="_28"><a href="https://kafka.apache.org/documentation/#georeplication-example-multi-cluster">示例：多集群异地复制</a></h4>
<p>让我们将前面部分中的所有信息放在一个更大的示例中。想象一下，有三个数据中心（西、东、北），每个数据中心有两个 Kafka 集群（例如 、<code>west-1</code>）<code>west-2</code>。本节中的示例显示如何配置 MirrorMaker (1) 以实现每个数据中心内的主动/主动复制，以及 (2) 跨数据中心复制 (XDCR)。</p>
<p>首先，在配置中定义源集群和目标集群及其复制流：</p>
<pre><code class="language-text"># Basic settings
clusters: west-1, west-2, east-1, east-2, north-1, north-2
west-1.bootstrap.servers = ...
west-2.bootstrap.servers = ...
east-1.bootstrap.servers = ...
east-2.bootstrap.servers = ...
north-1.bootstrap.servers = ...
north-2.bootstrap.servers = ...

# Replication flows for Active/Active in West DC
west-1-&gt;west-2.enabled = true
west-2-&gt;west-1.enabled = true

# Replication flows for Active/Active in East DC
east-1-&gt;east-2.enabled = true
east-2-&gt;east-1.enabled = true

# Replication flows for Active/Active in North DC
north-1-&gt;north-2.enabled = true
north-2-&gt;north-1.enabled = true

# Replication flows for XDCR via west-1, east-1, north-1
west-1-&gt;east-1.enabled  = true
west-1-&gt;north-1.enabled = true
east-1-&gt;west-1.enabled  = true
east-1-&gt;north-1.enabled = true
north-1-&gt;west-1.enabled = true
north-1-&gt;east-1.enabled = true
</code></pre>
<p>然后，在每个数据中心中，启动一个或多个 MirrorMaker，如下所示：</p>
<pre><code class="language-text"># In West DC:
$ ./bin/connect-mirror-maker.sh connect-mirror-maker.properties --clusters west-1 west-2

# In East DC:
$ ./bin/connect-mirror-maker.sh connect-mirror-maker.properties --clusters east-1 east-2

# In North DC:
$ ./bin/connect-mirror-maker.sh connect-mirror-maker.properties --clusters north-1 north-2
</code></pre>
<p>通过此配置，任何集群生成的记录都将在数据中心内复制，并跨到其他数据中心。通过提供<code>--clusters</code>参数，我们确保每个 MirrorMaker 进程仅向附近的集群生成数据。</p>
<p><em>注意：</em><code>--clusters</code>从技术上讲，此处不需要该参数。没有它，MirrorMaker 也能正常工作。但是，吞吐量可能会受到数据中心之间“生产者滞后”的影响，并且您可能会产生不必要的数据传输成本。</p>
<h3 id="_29"><a href="https://kafka.apache.org/documentation/#georeplication-starting">开始异地复制</a></h3>
<p>您可以根据需要运行任意数量的 MirrorMaker 进程（例如：节点、服务器）。由于 MirrorMaker 基于 Kafka Connect，因此配置为复制相同 Kafka 集群的 MirrorMaker 进程在分布式设置中运行：它们将找到彼此、共享配置（请参阅下面的部分）、负载平衡其工作等等。例如，如果您想要提高复制流的吞吐量，一种选择是并行运行其他 MirrorMaker 进程。</p>
<p>要启动 MirrorMaker 进程，请运行以下命令：</p>
<pre><code class="language-text">$ ./bin/connect-mirror-maker.sh connect-mirror-maker.properties
</code></pre>
<p>启动后，MirrorMaker 进程可能需要几分钟时间才开始复制数据。</p>
<p>或者，如前所述，您可以设置参数<code>--clusters</code>以确保 MirrorMaker 进程仅向附近的集群生成数据。</p>
<pre><code class="language-text"># Note: The cluster alias us-west must be defined in the configuration file
$ ./bin/connect-mirror-maker.sh connect-mirror-maker.properties \
            --clusters us-west
</code></pre>
<p><em>测试使用者组复制时请注意：</em>默认情况下，MirrorMaker 不会复制该 <code>kafka-console-consumer.sh</code>工具创建的使用者组，您可以使用该工具在命令行上测试 MirrorMaker 设置。如果您确实也想复制这些使用者组，请<code>groups.exclude</code>相应地设置配置（默认值<code>groups.exclude = console-consumer-.*, connect-.*, __.*</code>：）。请记住在完成测试后再次更新配置。</p>
<h3 id="_30"><a href="https://kafka.apache.org/documentation/#georeplication-stopping">停止异地复制</a></h3>
<p>您可以通过使用以下命令发送 SIGTERM 信号来停止正在运行的 MirrorMaker 进程：</p>
<pre><code class="language-text">$ kill &lt;MirrorMaker pid&gt;
</code></pre>
<h3 id="_31"><a href="https://kafka.apache.org/documentation/#georeplication-apply-config-changes">应用配置更改</a></h3>
<p>要使配置更改生效，必须重新启动 MirrorMaker 进程。</p>
<h3 id="_32"><a href="https://kafka.apache.org/documentation/#georeplication-monitoring">监控异地复制</a></h3>
<p>建议监控 MirrorMaker 进程，以确保所有定义的复制流程均正常启动并运行。MirrorMaker 基于 Connect 框架构建，并继承了 Connect 的所有指标，例如<code>source-record-poll-rate</code>. 此外，MirrorMaker 在<code>kafka.connect.mirror</code>指标组下生成自己的指标。指标带有以下属性标记：</p>
<ul>
<li><code>source</code>：源集群的别名（例如，<code>primary</code>）</li>
<li><code>target</code>：目标集群的别名（例如，<code>secondary</code>）</li>
<li><code>topic</code>：目标集群上的复制主题</li>
<li><code>partition</code>：正在复制的分区</li>
</ul>
<p>跟踪每个复制主题的指标。可以从主题名称推断出源集群。例如，复制<code>topic1</code>将<code>primary-&gt;secondary</code>产生如下指标：</p>
<ul>
<li><code>target=secondary</code></li>
<li><code>topic=primary.topic1</code></li>
<li><code>partition=1</code></li>
</ul>
<p>发出以下指标：</p>
<pre><code class="language-text"># MBean: kafka.connect.mirror:type=MirrorSourceConnector,target=([-.w]+),topic=([-.w]+),partition=([0-9]+)

record-count            # number of records replicated source -&gt; target
record-age-ms           # age of records when they are replicated
record-age-ms-min
record-age-ms-max
record-age-ms-avg
replication-latency-ms  # time it takes records to propagate source-&gt;target
replication-latency-ms-min
replication-latency-ms-max
replication-latency-ms-avg
byte-rate               # average number of bytes/sec in replicated records

# MBean: kafka.connect.mirror:type=MirrorCheckpointConnector,source=([-.w]+),target=([-.w]+)

checkpoint-latency-ms   # time it takes to replicate consumer offsets
checkpoint-latency-ms-min
checkpoint-latency-ms-max
checkpoint-latency-ms-avg
</code></pre>
<p>这些指标不区分创建时间和日志追加时间戳。</p>
<h2 id="64"><a href="https://kafka.apache.org/documentation/#multitenancy">6.4 多租户</a></h2>
<h3 id="_33"><a href="https://kafka.apache.org/documentation/#multitenancy-overview">多租户概述</a></h3>
<p>作为一个高度可扩展的事件流平台，Kafka 被许多用户用作他们的中枢神经系统，实时连接来自不同团队和业务线的各种不同系统和应用程序。这种多租户集群环境需要适当的控制和管理，以确保这些不同需求的和平共存。本节重点介绍设置此类共享环境的功能和最佳实践，这将帮助您操作满足 SLA/OLA 的集群，并最大限度地减少“吵闹的邻居”造成的潜在附带损害。</p>
<p>多租户是一个多方面的主题，包括但不限于：</p>
<ul>
<li>为租户创建用户空间（有时称为命名空间）</li>
<li>使用数据保留策略等配置主题</li>
<li>通过加密、身份验证和授权来保护主题和集群</li>
<li>通过配额和费率限制隔离租户</li>
<li>监控与计量</li>
<li>集群间数据共享（参见异地复制）</li>
</ul>
<h3 id="_34"><a href="https://kafka.apache.org/documentation/#multitenancy-topic-naming">使用主题命名为租户创建用户空间（命名空间）</a></h3>
<p>操作多租户集群的 Kafka 管理员通常需要为每个租户定义用户空间。就本节而言，“用户空间”是主题的集合，这些主题在单个实体或用户的管理下组合在一起。</p>
<p>在Kafka中，数据的主要单位是主题。用户可以创建并命名每个主题。他们也可以删除它们，但无法直接重命名主题。相反，要重命名主题，用户必须创建新主题，将消息从原始主题移动到新主题，然后删除原始主题。考虑到这一点，建议基于分层主题命名结构来定义逻辑空间。然后，此设置可以与安全功能（例如前缀 ACL）相结合，以隔离不同的空间和租户，同时最大限度地减少保护集群中数据的管理开销。</p>
<p>这些逻辑用户空间可以通过不同的方式进行分组，具体选择取决于您的组织更喜欢如何使用 Kafka 集群。最常见的分组如下。</p>
<p><em>按团队或组织单位：</em>在这里，团队是主要的聚合者。在团队是 Kafka 基础设施主要用户的组织中，这可能是最好的分组。</p>
<p>主题命名结构示例：</p>
<ul>
<li><code>&lt;organization&gt;.&lt;team&gt;.&lt;dataset&gt;.&lt;event-name&gt;</code><br />
    (e.g., "acme.infosec.telemetry.logins")</li>
</ul>
<p><em>按项目或产品：</em>在这里，一个团队管理多个项目。每个项目的凭据都不同，因此所有控件和设置将始终与项目相关。</p>
<p>主题命名结构示例：</p>
<ul>
<li><code>&lt;project&gt;.&lt;product&gt;.&lt;event-name&gt;</code><br />
    (e.g., "mobility.payments.suspicious")</li>
</ul>
<p>某些信息通常不应放在主题名称中，例如可能随时间变化的信息（例如，目标消费者的名称）或者是其他地方可用的技术细节或元数据（例如，主题的分区）计数和其他配置设置）。</p>
<p>要强制实施主题命名结构，可以使用以下几个选项：</p>
<ul>
<li>使用<a href="https://kafka.apache.org/documentation/#security_authz">前缀 ACL</a>（参见<a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-290%3A+Support+for+Prefixed+ACLs">KIP-290</a>）强制主题名称使用公共前缀。例如，团队 A 可能只被允许创建名称以 开头的主题<code>payments.teamA.</code>。</li>
<li>定义自定义<code>CreateTopicPolicy</code>（参见<a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-108%3A+Create+Topic+Policy">KIP-108</a>和设置<a href="https://kafka.apache.org/documentation/#brokerconfigs_create.topic.policy.class.name">create.topic.policy.class.name</a>）以强制执行严格的命名模式。这些策略提供了最大的灵活性，并且可以涵盖复杂的模式和规则来满足组织的需求。</li>
<li>通过使用 ACL 拒绝普通用户禁用主题创建，然后依靠外部进程代表用户创建主题（例如，脚本或您最喜欢的自动化工具包）。</li>
<li><code>auto.create.topics.enable=false</code>通过在代理配置中进行设置来禁用 Kafka 功能以按需自动创建主题也可能很有用。请注意，您不应仅仅依赖此选项。</li>
</ul>
<h3 id="_35"><a href="https://kafka.apache.org/documentation/#multitenancy-topic-configs">配置主题：数据保留等</a></h3>
<p>Kafka 的配置由于其精细的粒度而非常灵活，并且它支持大量的<a href="https://kafka.apache.org/documentation/#topicconfigs">按主题配置设置</a>，以帮助管理员设置多租户集群。例如，管理员通常需要定义数据保留策略，以控制数据在主题中存储的数量和/或多长时间，并使用诸如<a href="https://kafka.apache.org/documentation/#retention.bytes">retention.bytes</a>（大小）和<a href="https://kafka.apache.org/documentation/#retention.ms">retention.ms</a>（时间）等设置。这限制了集群内的存储消耗，并有助于遵守 GDPR 等法律要求。</p>
<h3 id="_36"><a href="https://kafka.apache.org/documentation/#multitenancy-security">保护集群和主题：身份验证、授权、加密</a></h3>
<p>由于该文档有专门的一章介绍适用于任何 Kafka 部署的<a href="https://kafka.apache.org/documentation/#security">安全性</a>，因此本节重点介绍多租户环境的其他注意事项。</p>
<p>Kafka 的安全设置分为三个主要类别，这与管理员保护其他客户端-服务器数据系统（如关系数据库和传统消息系统）的方式类似。</p>
<ol>
<li>对 Kafka 代理和 Kafka 客户端之间、代理之间、代理和 ZooKeeper 节点之间以及代理和其他可选工具之间传输的数据进行<strong>加密。</strong></li>
<li>对从 Kafka 客户端和应用程序到 Kafka 代理的连接以及从 Kafka 代理到 ZooKeeper 节点的连接进行<strong>身份验证。</strong></li>
<li>对主题的创建、删除、更改配置等客户端操作<strong>进行授权；</strong>将事件写入主题或从主题读取事件；创建和删除 ACL。管理员还可以定义自定义策略以实施其他限制，例如<code>CreateTopicPolicy</code>and <code>AlterConfigPolicy</code>（请参阅<a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-108%3A+Create+Topic+Policy">KIP-108</a>和设置<a href="https://kafka.apache.org/documentation/#brokerconfigs_create.topic.policy.class.name">create.topic.policy.class.name</a>、<a href="https://kafka.apache.org/documentation/#brokerconfigs_alter.config.policy.class.name">alter.config.policy.class.name</a>）。</li>
</ol>
<p>当保护多租户 Kafka 环境时，最常见的管理任务是第三类（授权），即管理用户/客户端权限，授予或拒绝对某些主题的访问，从而授予或拒绝对集群内用户存储的数据的访问。该任务主要通过<a href="https://kafka.apache.org/documentation/#security_authz">访问控制列表（ACL）的设置</a>来执行。在这里，多租户环境的管理员特别受益于将分层主题命名结构放在适当的位置（如上一节所述），因为他们可以通过前缀 ACL 方便地控制对主题的访问（<code>--resource-pattern-type Prefixed</code>）。这大大减少了多租户环境中保护主题的管理开销：管理员可以在更高的开发便利性（更宽松的权限，使用更少和更广泛的 ACL）与更严格的安全性（更严格的权限，使用更多和更广泛的 ACL）之间进行权衡。更窄的 ACL）。</p>
<p>在以下示例中，用户 Alice（ACME 公司 InfoSec 团队的新成员）被授予对名称以“acme.infosec.”开头的所有主题的写入权限，例如“acme.infosec.telemetry.logins”和“acme.infosec.logins”。 infosec.syslogs.events”。</p>
<pre><code class="language-text"># Grant permissions to user Alice
$ bin/kafka-acls.sh \
    --bootstrap-server broker1:9092 \
    --add --allow-principal User:Alice \
    --producer \
    --resource-pattern-type prefixed --topic acme.infosec.
</code></pre>
<p>您可以类似地使用此方法来隔离同一共享集群上的不同客户。</p>
<h3 id="_37"><a href="https://kafka.apache.org/documentation/#multitenancy-isolation">隔离租户：配额、速率限制、限制</a></h3>
<p>多租户集群通常应配置<a href="https://kafka.apache.org/documentation/#design_quotas">配额</a>，以防止用户（租户）占用过多集群资源，例如当他们尝试写入或读取大量数据时，或以过高的速率向代理创建请求时。这可能会导致网络饱和、垄断代理资源并影响其他客户端——所有这些都是您希望在共享环境中避免的。</p>
<p><strong>客户端配额：</strong> Kafka 支持不同类型的（每用户主体）客户端配额。由于客户端的配额与客户端写入或读取哪个主题无关，因此它们是在多租户集群中分配资源的便捷且有效的工具。<a href="https://kafka.apache.org/documentation/#design_quotascpu">例如，请求速率配额通过限制代理在</a><a href="https://kafka.apache.org/protocol.html">请求处理路径</a>上花费的时间来帮助限制用户对代理 CPU 使用率的影响<a href="https://kafka.apache.org/protocol.html"></a>在许多情况下，在多租户集群中，使用请求速率配额隔离用户比设置传入/传出网络带宽配额影响更大，因为用于处理请求的代理 CPU 使用率过高会降低有效带宽broker可以提供服务。此外，管理员还可以定义主题操作的配额（例如创建、删除和更改），以防止 Kafka 集群因高并发主题操作而不堪重负（请参阅<a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-599%3A+Throttle+Create+Topic%2C+Create+Partition+and+Delete+Topic+Operations">KIP-599</a>和配额类型<code>controller_mutation_rate</code>）。</p>
<p><strong>服务器配额：</strong> Kafka还支持不同类型的broker端配额。<a href="https://kafka.apache.org/documentation/#brokerconfigs_max.connection.creation.rate">例如，管理员可以设置代理接受新连接的</a>速率限制、设置<a href="https://kafka.apache.org/documentation/#brokerconfigs_max.connections">每个代理的最大连接数或设置允许</a><a href="https://kafka.apache.org/documentation/#brokerconfigs_max.connections.per.ip">来自特定 IP 地址的</a>最大连接数。</p>
<p>欲了解更多信息，请参阅<a href="https://kafka.apache.org/documentation/#design_quotas">配额概述</a>和<a href="https://kafka.apache.org/documentation/#quotas">如何设置配额</a>。</p>
<h3 id="_38"><a href="https://kafka.apache.org/documentation/#multitenancy-monitoring">监控与计量</a></h3>
<p><a href="https://kafka.apache.org/documentation/#monitoring">监控</a>是一个更广泛的主题，在文档的<a href="https://kafka.apache.org/documentation/#monitoring">其他部分</a>中有介绍。任何 Kafka 环境（尤其是多租户环境）的管理员都应根据这些说明设置监控。Kafka 支持广泛的指标，例如身份验证尝试失败率、请求延迟、消费者滞后、消费者组总数、上一节中描述的配额指标等等。</p>
<p>例如，可以将监控配置为跟踪主题分区的大小（使用 JMX 指标<code>kafka.log.Log.Size.&lt;TOPIC-NAME&gt;</code>），从而跟踪主题中存储的数据的总大小。然后，您可以定义当共享集群上的租户即将使用过多存储空间时发出的警报。</p>
<h3 id="_39"><a href="https://kafka.apache.org/documentation/#multitenancy-georeplication">多租户和地理复制</a></h3>
<p>Kafka 允许您跨不同集群共享数据，这些集群可能位于不同的地理区域、数据中心等。除了灾难恢复等用例之外，当多租户设置需要集群间数据共享时，此功能非常有用。有关详细信息， 请参阅<a href="https://kafka.apache.org/documentation/#georeplication">异地复制（跨集群数据镜像）</a>部分。</p>
<h3 id="_40"><a href="https://kafka.apache.org/documentation/#multitenancy-more">进一步的考虑</a></h3>
<p><strong>数据合约：</strong>您可能需要使用事件模式在集群中的数据生产者和消费者之间定义数据契约。这确保写入 Kafka 的事件始终可以再次正确读取，并防止写入格式错误或损坏的事件。实现此目标的最佳方法是与集群一起部署所谓的模式注册表。（Kafka 不包含模式注册表，但有可用的第三方实现。）模式注册表管理事件模式并将模式映射到主题，以便生产者知道哪些主题正在接受哪些类型（模式）的事件，以及消费者知道如何读取和解析主题中的事件。一些注册表实现提供了更多功能，例如架构演变、存储所有架构的历史记录以及架构兼容性设置。</p>
<h2 id="65-kafka"><a href="https://kafka.apache.org/documentation/#config">6.5 Kafka配置</a></h2>
<h3 id="_41"><a href="https://kafka.apache.org/documentation/#clientconfig">重要的客户端配置</a></h3>
<p>最重要的生产者配置是：</p>
<ul>
<li>确认</li>
<li>压缩</li>
<li>批量大小</li>
</ul>
<p>最重要的消费者配置是获取大小。</p>
<p>所有配置都记录在<a href="https://kafka.apache.org/documentation/#configuration">配置</a>部分中。</p>
<h3 id="_42"><a href="https://kafka.apache.org/documentation/#prodconfig">生产服务器配置</a></h3>
<p>以下是生产服务器配置示例：</p>
<pre><code class="language-text"># ZooKeeper
zookeeper.connect=[list of ZooKeeper servers]

# Log configuration
num.partitions=8
default.replication.factor=3
log.dir=[List of directories. Kafka should have its own dedicated disk(s) or SSD(s).]

# Other configurations
broker.id=[An integer. Start with 0 and increment by 1 for each new broker.]
listeners=[list of listeners]
auto.create.topics.enable=false
min.insync.replicas=2
queued.max.requests=[number of concurrent requests]
</code></pre>
<p>我们的客户端配置在不同的用例之间存在很大差异。</p>
<h2 id="66-java"><a href="https://kafka.apache.org/documentation/#java">6.6 Java版本</a></h2>
<p>支持 Java 8、Java 11 和 Java 17。请注意，自 Apache Kafka 3.0 起，Java 8 支持已被弃用，并将在 Apache Kafka 4.0 中删除。如果启用 TLS，Java 11 及更高版本的性能会显着提高，因此强烈推荐它们（它们还包括许多其他性能改进：G1GC、CRC32C、紧凑字符串、线程本地握手等）。从安全角度来看，我们推荐最新发布的补丁版本，因为旧的免费版本已披露了安全漏洞。使用基于 OpenJDK 的 Java 实现（包括 Oracle JDK）运行 Kafka 的典型参数是：</p>
<pre><code class="language-text">-Xmx6g -Xms6g -XX:MetaspaceSize=96m -XX:+UseG1GC
-XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:G1HeapRegionSize=16M
-XX:MinMetaspaceFreeRatio=50 -XX:MaxMetaspaceFreeRatio=80 -XX:+ExplicitGCInvokesConcurrent
</code></pre>
<p>作为参考，以下是使用上述 Java 参数的 LinkedIn 最繁忙集群之一（高峰期）的统计数据：</p>
<ul>
<li>60 名broker</li>
<li>50k 分区（复制因子 2）</li>
<li>800k 消息/秒</li>
<li>入站 300 MB/秒，出站 1 GB/秒以上</li>
</ul>
<p>该集群中的所有代理的 90% GC 暂停时间约为 21 毫秒，每秒少于 1 次年轻 GC。</p>
<h2 id="67"><a href="https://kafka.apache.org/documentation/#hwandos">6.7 硬件和操作系统</a></h2>
<p>我们使用具有 24GB 内存的双四核 Intel Xeon 机器。</p>
<p>您需要足够的内存来缓冲活动的读取器和写入器。您可以通过假设您希望能够缓冲 30 秒并将内存需求计算为 write_throughput*30 来对内存需求进行粗略估计。</p>
<p>磁盘吞吐量很重要。我们有 8x7200 rpm SATA 驱动器。一般来说磁盘吞吐量是性能瓶颈，磁盘越多越好。根据您配置刷新行为的方式，您可能会也可能不会从更昂贵的磁盘中受益（如果您经常强制刷新，那么更高 RPM 的 SAS 驱动器可能会更好）。</p>
<h3 id="_43"><a href="https://kafka.apache.org/documentation/#os">操作系统</a></h3>
<p>Kafka应该可以在任何unix系统上运行良好，并且已经在Linux和Solaris上进行了测试。</p>
<p>我们发现在 Windows 上运行时存在一些问题，并且 Windows 目前不是一个得到良好支持的平台，但我们很乐意对此进行更改。</p>
<p>它不太可能需要太多操作系统级别的调整，但存在三个潜在重要的操作系统级别配置：</p>
<ul>
<li>文件描述符限制：Kafka 使用文件描述符来表示日志段和打开的连接。如果代理托管许多分区，请考虑代理除了代理建立的连接数之外，还至少需要 (number_of_partitions)*(partition_size/segment_size) 来跟踪所有日志段。我们建议代理进程至少使用 100000 个允许的文件描述符作为起点。注意：mmap() 函数添加对与文件描述符 fildes 关联的文件的额外引用，该文件描述符上的后续 close() 不会删除该引用。当不再有到该文件的映射时，该引用将被删除。</li>
<li>最大套接字缓冲区大小：可以增加以实现数据中心之间的高性能数据传输，如此处<a href="http://www.psc.edu/index.php/networking/641-tcp-tune">所述</a>。</li>
<li>进程可以拥有的内存映射区域的最大数量（也称为 vm.max_map_count）。<a href="http://kernel.org/doc/Documentation/sysctl/vm.txt">请参阅 Linux 内核文档</a>。在考虑代理可能拥有的最大分区数时，您应该留意这个操作系统级别的属性。默认情况下，在许多 Linux 系统上，vm.max_map_count 的值约为 65535。每个分区分配的每个日志段都需要一对索引/时间索引文件，并且每个文件消耗 1 个映射区域。换句话说，每个日志段使用2个地图区域。因此，每个分区至少需要 2 个映射区域，只要它托管单个日志段即可。也就是说，在代理上创建 50000 个分区将导致分配 100000 个映射区域，并可能在具有默认 vm.max_map_count 的系统上导致代理崩溃并出现 OutOfMemoryError（映射失败）。请记住，每个分区的日志段数根据段大小、负载强度、保留策略以及通常情况下的变化而变化</li>
</ul>
<h3 id="_44"><a href="https://kafka.apache.org/documentation/#diskandfs">磁盘和文件系统</a></h3>
<p>我们建议使用多个驱动器来获得良好的吞吐量，并且不要与应用程序日志或其他操作系统文件系统活动共享用于 Kafka 数据的相同驱动器，以确保良好的延迟。您可以将这些驱动器一起 RAID 到单个卷中，也可以将每个驱动器格式化并安装为其自己的目录。由于 Kafka 具有复制功能，因此 RAID 提供的冗余也可以在应用程序级别提供。这种选择有几个权衡。</p>
<p>如果配置多个数据目录，分区将循环分配给数据目录。每个分区将完全位于一个数据目录中。如果分区之间的数据没有很好地平衡，这可能会导致磁盘之间的负载不平衡。</p>
<p>RAID 在平衡磁盘之间的负载方面可能会做得更好（尽管看起来并不总是如此），因为它在较低级别上平衡负载。RAID 的主要缺点是它通常会严重影响写入吞吐量并减少可用磁盘空间。</p>
<p>RAID 的另一个潜在好处是能够容忍磁盘故障。然而，我们的经验是，重建 RAID 阵列的 I/O 密集程度很高，以至于它会有效地禁用服务器，因此这并不能提供太多真正的可用性改进。</p>
<h3 id="_45"><a href="https://kafka.apache.org/documentation/#appvsosflush">应用程序与操作系统刷新管理</a></h3>
<p>Kafka 总是立即将所有数据写入文件系统，并支持配置刷新策略的功能，该策略控制何时使用刷新将数据强制从操作系统缓存中取出并写入磁盘。可以控制此刷新策略，以在一段时间后或在写入一定数量的消息后强制将数据写入磁盘。此配置有多种选择。</p>
<p>Kafka 最终必须调用 fsync 才能知道数据已刷新。当从任何未知的 fsync 日志段的崩溃中恢复时，Kafka 将通过检查其 CRC 来检查每条消息的完整性，并重建随附的偏移量索引文件，作为启动时执行的恢复过程的一部分。</p>
<p>请注意，Kafka 中的持久性不需要将数据同步到磁盘，因为故障节点始终会从其副本中恢复。</p>
<p>我们建议使用默认刷新设置，完全禁用应用程序 fsync。这意味着依赖操作系统完成的后台刷新和 Kafka 自己的后台刷新。这为大多数用途提供了最好的特性：无需调节旋钮、出色的吞吐量和延迟以及完全恢复保证。我们通常认为复制提供的保证比同步到本地磁盘更强，但是偏执者可能仍然更喜欢两者，并且仍然支持应用程序级 fsync 策略。</p>
<p>使用应用程序级别刷新设置的缺点是，它的磁盘使用模式效率较低（它给操作系统重新排序写入的余地较小），并且可能会引入延迟，因为大多数 Linux 文件系统中的 fsync 会阻止写入文件，而后台刷新执行更细粒度的页面级锁定。</p>
<p>一般来说，您不需要对文件系统进行任何低级调整，但在接下来的几节中，我们将讨论其中的一些内容，以防它有用。</p>
<h3 id="linux"><a href="https://kafka.apache.org/documentation/#linuxflush">了解 Linux 操作系统刷新行为</a></h3>
<p>在 Linux 中，写入文件系统的数据保留在<a href="http://en.wikipedia.org/wiki/Page_cache">页面缓存</a>中，直到必须将其写出到磁盘（由于应用程序级 fsync 或操作系统自身的刷新策略）。数据的刷新是由一组称为 pdflush 的后台线程（或在 2.6.32 后的内核中的“flusher 线程”）完成的。</p>
<p>Pdflush 有一个可配置的策略，可以控制缓存中可以保留多少脏数据以及必须将其写回磁盘之前的时间。<a href="http://web.archive.org/web/20160518040713/http://www.westnet.com/~gsmith/content/linux-pdflush.htm">此处</a>描述了该策略。当 Pdflush 无法跟上数据写入的速率时，最终会导致写入过程阻塞，从而产生写入延迟，从而减慢数据的积累。</p>
<p>您可以通过以下方式查看操作系统内存使用的当前状态</p>
<p>&gt; 猫 /proc/meminfo</p>
<p>这些值的含义在上面的链接中有描述。</p>
<p>与进程内缓存相比，使用页面缓存来存储将写出到磁盘的数据有几个优点：</p>
<ul>
<li>I/O 调度程序会将连续的小写入批量合并为更大的物理写入，从而提高吞吐量。</li>
<li>I/O 调度程序将尝试重新排序写入，以最大限度地减少磁盘头的移动，从而提高吞吐量。</li>
<li>它会自动使用机器上的所有可用内存</li>
</ul>
<h3 id="_46"><a href="https://kafka.apache.org/documentation/#filesystems">文件系统选择</a></h3>
<p>Kafka 使用磁盘上的常规文件，因此它对特定文件系统没有硬依赖。然而，使用最多的两个文件系统是 EXT4 和 XFS。从历史上看，EXT4 的使用量较多，但最近对 XFS 文件系统的改进表明，它对于 Kafka 工作负载具有更好的性能特征，且不影响稳定性。</p>
<p>使用各种文件系统创建和挂载选项，在具有大量消息负载的集群上执行比较测试。Kafka 中受监控的主要指标是“请求本地时间”，表示追加操作所花费的时间。XFS 带来了更好的本地时间（对于最佳 EXT4 配置，本地时间为 160 毫秒，而 250 毫秒以上），并且平均等待时间更短。XFS 性能还显示磁盘性能的变化较小。</p>
<h4 id="_47"><a href="https://kafka.apache.org/documentation/#generalfs">一般文件系统注释</a></h4>
<p>对于用于数据目录的任何文件系统，在 Linux 系统上，建议在挂载时使用以下选项：</p>
<ul>
<li>noatime：此选项禁止在读取文件时更新文件的 atime（上次访问时间）属性。这可以消除大量的文件系统写入，特别是在引导消费者的情况下。Kafka 根本不依赖 atime 属性，因此禁用它是安全的。</li>
</ul>
<h4 id="xfs"><a href="https://kafka.apache.org/documentation/#xfs">XFS 注释</a></h4>
<p>XFS 文件系统具有大量自动调整功能，因此无论是在文件系统创建时还是在挂载时，都不需要对默认设置进行任何更改。唯一值得考虑的调整参数是：</p>
<ul>
<li>Largeio：这会影响 stat 调用报告的首选 I/O 大小。虽然这可以在较大的磁盘写入上实现更高的性能，但实际上它对性能的影响很小或没有影响。</li>
<li>nobarrier：对于具有电池支持缓存的底层设备，此选项可以通过禁用定期写入刷新来提供更高的性能。但是，如果底层设备表现良好，它将向文件系统报告它不需要刷新，并且此选项将不起作用。</li>
</ul>
<h4 id="ext4"><a href="https://kafka.apache.org/documentation/#ext4">EXT4注释</a></h4>
<p>EXT4 是 Kafka 数据目录的一个可用的文件系统选择，但是要充分利用它的性能需要调整多个安装选项。此外，这些选项在故障情况下通常是不安全的，并且会导致更多的数据丢失和损坏。对于单个代理故障，这并不是什么大问题，因为可以擦除磁盘并从集群重建副本。在多次故障的情况下，例如断电，这可能意味着底层文件系统（以及数据）损坏且难以恢复。可以调整以下选项：</p>
<ul>
<li>data=writeback：Ext4 默认为 data=ordered，这对某些写入设置了强顺序。Kafka 不需要这种排序，因为它对所有未刷新的日志进行非常偏执的数据恢复。此设置消除了排序限制，并且似乎显着减少了延迟。</li>
<li>禁用日志记录：日志记录是一种权衡：它使服务器崩溃后重新启动速度更快，但它引入了大量额外的锁定，从而增加了写入性能的差异。那些不关心重新启动时间并希望减少写入延迟峰值的主要来源的人可以完全关闭日志记录。</li>
<li>commit=num_secs：这会调整 ext4 提交其元数据日志的频率。将其设置为较低的值可以减少崩溃期间未刷新数据的丢失。将其设置为更高的值将提高吞吐量。</li>
<li>nobh：此设置控制使用 data=writeback 模式时的附加排序保证。这对于 Kafka 来说应该是安全的，因为我们不依赖于写入顺序并提高了吞吐量和延迟。</li>
<li>delalloc：延迟分配意味着文件系统在物理写入发生之前避免分配任何块。这允许 ext4 分配较大的范围而不是较小的页面，并有助于确保数据按顺序写入。此功能对于吞吐量非常有用。它似乎确实涉及文件系统中的一些锁定，这增加了一些延迟差异。</li>
</ul>
<h3 id="kraft"><a href="https://kafka.apache.org/documentation/#replace_disk">更换 KRaft 控制器磁盘</a></h3>
<p><code>metadata.log.dir</code>当 Kafka 配置为使用 KRaft 时，控制器将集群元数据存储在-- 或第一个日志目录（如果<code>metadata.log.dir</code>未配置）中指定的目录中。<code>metadata.log.dir</code>有关详细信息，请参阅文档。</p>
<p>如果由于硬件故障或需要更换硬件而导致集群元数据目录中的数据丢失，则在配置新的控制器节点时应小心。在大多数控制器拥有所有提交的数据之前，不应格式化并启动新的控制器节点。要确定大多数控制器是否具有已提交的数据，请运行该<code>kafka-metadata-quorum.sh</code>工具来描述复制状态：</p>
<pre><code class="language-bash"> &gt; bin/kafka-metadata-quorum.sh --bootstrap-server broker_host:port describe --replication
 NodeId  LogEndOffset    Lag     LastFetchTimestamp      LastCaughtUpTimestamp   Status
 1       25806           0       1662500992757           1662500992757           Leader
 ...     ...             ...     ...                     ...                     ...

</code></pre>
<p>检查并等待，直到<code>Lag</code>对于大多数控制器来说都很小。如果领导者的结束偏移量没有增加，则可以等到滞后为 0 时才获得多数；否则，您可以选择最新的领导者末端偏移量并等待所有副本都到达它。检查并等待，直到大多数控制器的<code>LastFetchTimestamp</code>和彼此接近。<code>LastCaughtUpTimestamp</code>此时，格式化控制器的元数据日志目录会更安全。这可以通过运行命令来完成<code>kafka-storage.sh</code>。</p>
<pre><code class="language-bash"> &gt; bin/kafka-storage.sh format --cluster-id uuid --config server_properties
</code></pre>
<p>上面的命令可能<code>bin/kafka-storage.sh format</code>会失败并显示类似 的消息<code>Log directory ... is already formatted</code>。当使用组合模式并且仅丢失元数据日志目录而不丢失其他目录时，可能会发生这种情况。在这种情况下并且只有在这种情况下，您才能运行<code>kafka-storage.sh format</code>带有该<code>--ignore-formatted</code>选项的命令。</p>
<p>格式化日志目录后启动 KRaft 控制器。</p>
<pre><code class="language-bash"> &gt; /bin/kafka-server-start.sh server_properties
</code></pre>
<h2 id="68"><a href="https://kafka.apache.org/documentation/#monitoring">6.8 监控</a></h2>
<p>Kafka 使用 Yammer Metrics 在服务器中进行指标报告。Java 客户端使用 Kafka Metrics，这是一个内置指标注册表，可最大程度地减少客户端应用程序中的传递依赖性。两者都通过 JMX 公开指标，并且可以配置为使用可插入统计报告器报告统计信息以连接到您的监控系统。</p>
<p>所有 Kafka 速率指标都有一个相应的累积计数指标，后缀为<code>-total</code>。例如， <code>records-consumed-rate</code>有一个名为 的相应指标<code>records-consumed-total</code>。</p>
<p>查看可用指标的最简单方法是启动 jconsole 并将其指向正在运行的 kafka 客户端或服务器；这将允许使用 JMX 浏览所有指标。</p>
<h3 id="jmx"><a href="https://kafka.apache.org/documentation/#remote_jmx">使用 JMX 进行远程监控的安全注意事项</a></h3>
<p>Apache Kafka 默认禁用远程 JMX。<code>JMX_PORT</code>您可以通过为使用 CLI启动的进程设置环境变量或标准 Java 系统属性来以编程方式启用远程 JMX，从而使用 JMX 启用远程监控 。在生产场景中启用远程 JMX 时，您必须启用安全性，以确保未经授权的用户无法监视或控制您的代理或应用程序以及它们运行的​​平台。<code>KAFKA_JMX_OPTS</code>请注意，默认情况下，Kafka 中的 JMX 身份验证处于禁用状态，并且必须通过为使用 CLI 启动的进程设置环境变量或设置适当的 Java 系统属性来覆盖生产部署的安全配置 。请参阅 <a href="https://docs.oracle.com/javase/8/docs/technotes/guides/management/agent.html">使用 JMX 技术进行监控和管理</a> 有关保护 JMX 的详细信息。</p>
<p>我们对以下指标进行绘图和警报：</p>
<table>
<thead>
<tr>
<th>DESCRIPTION</th>
<th>MBEAN NAME</th>
<th>NORMAL VALUE</th>
</tr>
</thead>
<tbody>
<tr>
<td>Message in rate</td>
<td>kafka.server:type=BrokerTopicMetrics,name=MessagesInPerSec,topic=([-.\w]+)</td>
<td>Incoming message rate per topic. Omitting 'topic=(...)' will yield the all-topic rate.</td>
</tr>
<tr>
<td>Byte in rate from clients</td>
<td>kafka.server:type=BrokerTopicMetrics,name=BytesInPerSec,topic=([-.\w]+)</td>
<td>Byte in (from the clients) rate per topic. Omitting 'topic=(...)' will yield the all-topic rate.</td>
</tr>
<tr>
<td>Byte in rate from other brokers</td>
<td>kafka.server:type=BrokerTopicMetrics,name=ReplicationBytesInPerSec</td>
<td>Byte in (from the other brokers) rate across all topics.</td>
</tr>
<tr>
<td>Controller Request rate from Broker</td>
<td>kafka.controller:type=ControllerChannelManager,name=RequestRateAndQueueTimeMs,brokerId=([0-9]+)</td>
<td>The rate (requests per second) at which the ControllerChannelManager takes requests from the queue of the given broker. And the time it takes for a request to stay in this queue before it is taken from the queue.</td>
</tr>
<tr>
<td>Controller Event queue size</td>
<td>kafka.controller:type=ControllerEventManager,name=EventQueueSize</td>
<td>Size of the ControllerEventManager's queue.</td>
</tr>
<tr>
<td>Controller Event queue time</td>
<td>kafka.controller:type=ControllerEventManager,name=EventQueueTimeMs</td>
<td>Time that takes for any event (except the Idle event) to wait in the ControllerEventManager's queue before being processed</td>
</tr>
<tr>
<td>Request rate</td>
<td>kafka.network:type=RequestMetrics,name=RequestsPerSec,request={Produce</td>
<td>FetchConsumer</td>
</tr>
<tr>
<td>Error rate</td>
<td>kafka.network:type=RequestMetrics,name=ErrorsPerSec,request=([-.\w]+),error=([-.\w]+)</td>
<td>Number of errors in responses counted per-request-type, per-error-code. If a response contains multiple errors, all are counted. error=NONE indicates successful responses.</td>
</tr>
<tr>
<td>Produce request rate</td>
<td>kafka.server:type=BrokerTopicMetrics,name=TotalProduceRequestsPerSec,topic=([-.\w]+)</td>
<td>Produce request rate per topic. Omitting 'topic=(...)' will yield the all-topic rate.</td>
</tr>
<tr>
<td>Fetch request rate</td>
<td>kafka.server:type=BrokerTopicMetrics,name=TotalFetchRequestsPerSec,topic=([-.\w]+)</td>
<td>Fetch request (from clients or followers) rate per topic. Omitting 'topic=(...)' will yield the all-topic rate.</td>
</tr>
<tr>
<td>Failed produce request rate</td>
<td>kafka.server:type=BrokerTopicMetrics,name=FailedProduceRequestsPerSec,topic=([-.\w]+)</td>
<td>Failed Produce request rate per topic. Omitting 'topic=(...)' will yield the all-topic rate.</td>
</tr>
<tr>
<td>Failed fetch request rate</td>
<td>kafka.server:type=BrokerTopicMetrics,name=FailedFetchRequestsPerSec,topic=([-.\w]+)</td>
<td>Failed Fetch request (from clients or followers) rate per topic. Omitting 'topic=(...)' will yield the all-topic rate.</td>
</tr>
<tr>
<td>Request size in bytes</td>
<td>kafka.network:type=RequestMetrics,name=RequestBytes,request=([-.\w]+)</td>
<td>Size of requests for each request type.</td>
</tr>
<tr>
<td>Temporary memory size in bytes</td>
<td>kafka.network:type=RequestMetrics,name=TemporaryMemoryBytes,request={Produce</td>
<td>Fetch}</td>
</tr>
<tr>
<td>Message conversion time</td>
<td>kafka.network:type=RequestMetrics,name=MessageConversionsTimeMs,request={Produce</td>
<td>Fetch}</td>
</tr>
<tr>
<td>Message conversion rate</td>
<td>kafka.server:type=BrokerTopicMetrics,name={Produce</td>
<td>Fetch}MessageConversionsPerSec,topic=([-.\w]+)</td>
</tr>
<tr>
<td>Request Queue Size</td>
<td>kafka.network:type=RequestChannel,name=RequestQueueSize</td>
<td>Size of the request queue.</td>
</tr>
<tr>
<td>Byte out rate to clients</td>
<td>kafka.server:type=BrokerTopicMetrics,name=BytesOutPerSec,topic=([-.\w]+)</td>
<td>Byte out (to the clients) rate per topic. Omitting 'topic=(...)' will yield the all-topic rate.</td>
</tr>
<tr>
<td>Byte out rate to other brokers</td>
<td>kafka.server:type=BrokerTopicMetrics,name=ReplicationBytesOutPerSec</td>
<td>Byte out (to the other brokers) rate across all topics</td>
</tr>
<tr>
<td>Rejected byte rate</td>
<td>kafka.server:type=BrokerTopicMetrics,name=BytesRejectedPerSec,topic=([-.\w]+)</td>
<td>Rejected byte rate per topic, due to the record batch size being greater than max.message.bytes configuration. Omitting 'topic=(...)' will yield the all-topic rate.</td>
</tr>
<tr>
<td>Message validation failure rate due to no key specified for compacted topic</td>
<td>kafka.server:type=BrokerTopicMetrics,name=NoKeyCompactedTopicRecordsPerSec</td>
<td>0</td>
</tr>
<tr>
<td>Message validation failure rate due to invalid magic number</td>
<td>kafka.server:type=BrokerTopicMetrics,name=InvalidMagicNumberRecordsPerSec</td>
<td>0</td>
</tr>
<tr>
<td>Message validation failure rate due to incorrect crc checksum</td>
<td>kafka.server:type=BrokerTopicMetrics,name=InvalidMessageCrcRecordsPerSec</td>
<td>0</td>
</tr>
<tr>
<td>Message validation failure rate due to non-continuous offset or sequence number in batch</td>
<td>kafka.server:type=BrokerTopicMetrics,name=InvalidOffsetOrSequenceRecordsPerSec</td>
<td>0</td>
</tr>
<tr>
<td>Log flush rate and time</td>
<td>kafka.log:type=LogFlushStats,name=LogFlushRateAndTimeMs</td>
<td></td>
</tr>
<tr>
<td># of offline log directories</td>
<td>kafka.log:type=LogManager,name=OfflineLogDirectoryCount</td>
<td>0</td>
</tr>
<tr>
<td>Leader election rate</td>
<td>kafka.controller:type=ControllerStats,name=LeaderElectionRateAndTimeMs</td>
<td>non-zero when there are broker failures</td>
</tr>
<tr>
<td>Unclean leader election rate</td>
<td>kafka.controller:type=ControllerStats,name=UncleanLeaderElectionsPerSec</td>
<td>0</td>
</tr>
<tr>
<td>Is controller active on broker</td>
<td>kafka.controller:type=KafkaController,name=ActiveControllerCount</td>
<td>only one broker in the cluster should have 1</td>
</tr>
<tr>
<td>Pending topic deletes</td>
<td>kafka.controller:type=KafkaController,name=TopicsToDeleteCount</td>
<td></td>
</tr>
<tr>
<td>Pending replica deletes</td>
<td>kafka.controller:type=KafkaController,name=ReplicasToDeleteCount</td>
<td></td>
</tr>
<tr>
<td>Ineligible pending topic deletes</td>
<td>kafka.controller:type=KafkaController,name=TopicsIneligibleToDeleteCount</td>
<td></td>
</tr>
<tr>
<td>Ineligible pending replica deletes</td>
<td>kafka.controller:type=KafkaController,name=ReplicasIneligibleToDeleteCount</td>
<td></td>
</tr>
<tr>
<td># of under replicated partitions (</td>
<td>ISR</td>
<td>&lt;</td>
</tr>
<tr>
<td># of under minIsr partitions (</td>
<td>ISR</td>
<td>&lt; min.insync.replicas)</td>
</tr>
<tr>
<td># of at minIsr partitions (</td>
<td>ISR</td>
<td>= min.insync.replicas)</td>
</tr>
<tr>
<td>Producer Id counts</td>
<td>kafka.server:type=ReplicaManager,name=ProducerIdCount</td>
<td>Count of all producer ids created by transactional and idempotent producers in each replica on the broker</td>
</tr>
<tr>
<td>Partition counts</td>
<td>kafka.server:type=ReplicaManager,name=PartitionCount</td>
<td>mostly even across brokers</td>
</tr>
<tr>
<td>Offline Replica counts</td>
<td>kafka.server:type=ReplicaManager,name=OfflineReplicaCount</td>
<td>0</td>
</tr>
<tr>
<td>Leader replica counts</td>
<td>kafka.server:type=ReplicaManager,name=LeaderCount</td>
<td>mostly even across brokers</td>
</tr>
<tr>
<td>ISR shrink rate</td>
<td>kafka.server:type=ReplicaManager,name=IsrShrinksPerSec</td>
<td>If a broker goes down, ISR for some of the partitions will shrink. When that broker is up again, ISR will be expanded once the replicas are fully caught up. Other than that, the expected value for both ISR shrink rate and expansion rate is 0.</td>
</tr>
<tr>
<td>ISR expansion rate</td>
<td>kafka.server:type=ReplicaManager,name=IsrExpandsPerSec</td>
<td>See above</td>
</tr>
<tr>
<td>Failed ISR update rate</td>
<td>kafka.server:type=ReplicaManager,name=FailedIsrUpdatesPerSec</td>
<td>0</td>
</tr>
<tr>
<td>Max lag in messages btw follower and leader replicas</td>
<td>kafka.server:type=ReplicaFetcherManager,name=MaxLag,clientId=Replica</td>
<td>lag should be proportional to the maximum batch size of a produce request.</td>
</tr>
<tr>
<td>Lag in messages per follower replica</td>
<td>kafka.server:type=FetcherLagMetrics,name=ConsumerLag,clientId=([-.\w]+),topic=([-.\w]+),partition=([0-9]+)</td>
<td>lag should be proportional to the maximum batch size of a produce request.</td>
</tr>
<tr>
<td>Requests waiting in the producer purgatory</td>
<td>kafka.server:type=DelayedOperationPurgatory,name=PurgatorySize,delayedOperation=Produce</td>
<td>non-zero if ack=-1 is used</td>
</tr>
<tr>
<td>Requests waiting in the fetch purgatory</td>
<td>kafka.server:type=DelayedOperationPurgatory,name=PurgatorySize,delayedOperation=Fetch</td>
<td>size depends on fetch.wait.max.ms in the consumer</td>
</tr>
<tr>
<td>Request total time</td>
<td>kafka.network:type=RequestMetrics,name=TotalTimeMs,request={Produce</td>
<td>FetchConsumer</td>
</tr>
<tr>
<td>Time the request waits in the request queue</td>
<td>kafka.network:type=RequestMetrics,name=RequestQueueTimeMs,request={Produce</td>
<td>FetchConsumer</td>
</tr>
<tr>
<td>Time the request is processed at the leader</td>
<td>kafka.network:type=RequestMetrics,name=LocalTimeMs,request={Produce</td>
<td>FetchConsumer</td>
</tr>
<tr>
<td>Time the request waits for the follower</td>
<td>kafka.network:type=RequestMetrics,name=RemoteTimeMs,request={Produce</td>
<td>FetchConsumer</td>
</tr>
<tr>
<td>Time the request waits in the response queue</td>
<td>kafka.network:type=RequestMetrics,name=ResponseQueueTimeMs,request={Produce</td>
<td>FetchConsumer</td>
</tr>
<tr>
<td>Time to send the response</td>
<td>kafka.network:type=RequestMetrics,name=ResponseSendTimeMs,request={Produce</td>
<td>FetchConsumer</td>
</tr>
<tr>
<td>Number of messages the consumer lags behind the producer by. Published by the consumer, not broker.</td>
<td>kafka.consumer:type=consumer-fetch-manager-metrics,client-id={client-id} Attribute: records-lag-max</td>
<td></td>
</tr>
<tr>
<td>The average fraction of time the network processors are idle</td>
<td>kafka.network:type=SocketServer,name=NetworkProcessorAvgIdlePercent</td>
<td>between 0 and 1, ideally &gt; 0.3</td>
</tr>
<tr>
<td>The number of connections disconnected on a processor due to a client not re-authenticating and then using the connection beyond its expiration time for anything other than re-authentication</td>
<td>kafka.server:type=socket-server-metrics,listener=[SASL_PLAINTEXT</td>
<td>SASL_SSL],networkProcessor=&lt;#&gt;,name=expired-connections-killed-count</td>
</tr>
<tr>
<td>The total number of connections disconnected, across all processors, due to a client not re-authenticating and then using the connection beyond its expiration time for anything other than re-authentication</td>
<td>kafka.network:type=SocketServer,name=ExpiredConnectionsKilledCount</td>
<td>ideally 0 when re-authentication is enabled, implying there are no longer any older, pre-2.2.0 clients connecting to this broker</td>
</tr>
<tr>
<td>The average fraction of time the request handler threads are idle</td>
<td>kafka.server:type=KafkaRequestHandlerPool,name=RequestHandlerAvgIdlePercent</td>
<td>between 0 and 1, ideally &gt; 0.3</td>
</tr>
<tr>
<td>Bandwidth quota metrics per (user, client-id), user or client-id</td>
<td>kafka.server:type={Produce</td>
<td>Fetch},user=([-.\w]+),client-id=([-.\w]+)</td>
</tr>
<tr>
<td>Request quota metrics per (user, client-id), user or client-id</td>
<td>kafka.server:type=Request,user=([-.\w]+),client-id=([-.\w]+)</td>
<td>Two attributes. throttle-time indicates the amount of time in ms the client was throttled. Ideally = 0. request-time indicates the percentage of time spent in broker network and I/O threads to process requests from client group. For (user, client-id) quotas, both user and client-id are specified. If per-client-id quota is applied to the client, user is not specified. If per-user quota is applied, client-id is not specified.</td>
</tr>
<tr>
<td>Requests exempt from throttling</td>
<td>kafka.server:type=Request</td>
<td>exempt-throttle-time indicates the percentage of time spent in broker network and I/O threads to process requests that are exempt from throttling.</td>
</tr>
<tr>
<td>ZooKeeper client request latency</td>
<td>kafka.server:type=ZooKeeperClientMetrics,name=ZooKeeperRequestLatencyMs</td>
<td>Latency in milliseconds for ZooKeeper requests from broker.</td>
</tr>
<tr>
<td>ZooKeeper connection status</td>
<td>kafka.server:type=SessionExpireListener,name=SessionState</td>
<td>Connection status of broker's ZooKeeper session which may be one of Disconnected</td>
</tr>
<tr>
<td>Max time to load group metadata</td>
<td>kafka.server:type=group-coordinator-metrics,name=partition-load-time-max</td>
<td>maximum time, in milliseconds, it took to load offsets and group metadata from the consumer offset partitions loaded in the last 30 seconds (including time spent waiting for the loading task to be scheduled)</td>
</tr>
<tr>
<td>Avg time to load group metadata</td>
<td>kafka.server:type=group-coordinator-metrics,name=partition-load-time-avg</td>
<td>average time, in milliseconds, it took to load offsets and group metadata from the consumer offset partitions loaded in the last 30 seconds (including time spent waiting for the loading task to be scheduled)</td>
</tr>
<tr>
<td>Max time to load transaction metadata</td>
<td>kafka.server:type=transaction-coordinator-metrics,name=partition-load-time-max</td>
<td>maximum time, in milliseconds, it took to load transaction metadata from the consumer offset partitions loaded in the last 30 seconds (including time spent waiting for the loading task to be scheduled)</td>
</tr>
<tr>
<td>Avg time to load transaction metadata</td>
<td>kafka.server:type=transaction-coordinator-metrics,name=partition-load-time-avg</td>
<td>average time, in milliseconds, it took to load transaction metadata from the consumer offset partitions loaded in the last 30 seconds (including time spent waiting for the loading task to be scheduled)</td>
</tr>
<tr>
<td>Rate of transactional verification errors</td>
<td>kafka.server:type=AddPartitionsToTxnManager,name=VerificationFailureRate</td>
<td>Rate of verifications that returned in failure either from the AddPartitionsToTxn API response or through errors in the AddPartitionsToTxnManager. In steady state 0, but transient errors are expected during rolls and reassignments of the transactional state partition.</td>
</tr>
<tr>
<td>Time to verify a transactional request</td>
<td>kafka.server:type=AddPartitionsToTxnManager,name=VerificationTimeMs</td>
<td>The amount of time queueing while a possible previous request is in-flight plus the round trip to the transaction coordinator to verify (or not verify)</td>
</tr>
<tr>
<td>Consumer Group Offset Count</td>
<td>kafka.server:type=GroupMetadataManager,name=NumOffsets</td>
<td>Total number of committed offsets for Consumer Groups</td>
</tr>
<tr>
<td>Consumer Group Count</td>
<td>kafka.server:type=GroupMetadataManager,name=NumGroups</td>
<td>Total number of Consumer Groups</td>
</tr>
<tr>
<td>Consumer Group Count, per State</td>
<td>kafka.server:type=GroupMetadataManager,name=NumGroups[PreparingRebalance,CompletingRebalance,Empty,Stable,Dead]</td>
<td>The number of Consumer Groups in each state: PreparingRebalance, CompletingRebalance, Empty, Stable, Dead</td>
</tr>
<tr>
<td>Number of reassigning partitions</td>
<td>kafka.server:type=ReplicaManager,name=ReassigningPartitions</td>
<td>The number of reassigning leader partitions on a broker.</td>
</tr>
<tr>
<td>Outgoing byte rate of reassignment traffic</td>
<td>kafka.server:type=BrokerTopicMetrics,name=ReassignmentBytesOutPerSec</td>
<td>0; non-zero when a partition reassignment is in progress.</td>
</tr>
<tr>
<td>Incoming byte rate of reassignment traffic</td>
<td>kafka.server:type=BrokerTopicMetrics,name=ReassignmentBytesInPerSec</td>
<td>0; non-zero when a partition reassignment is in progress.</td>
</tr>
<tr>
<td>Size of a partition on disk (in bytes)</td>
<td>kafka.log:type=Log,name=Size,topic=([-.\w]+),partition=([0-9]+)</td>
<td>The size of a partition on disk, measured in bytes.</td>
</tr>
<tr>
<td>Number of log segments in a partition</td>
<td>kafka.log:type=Log,name=NumLogSegments,topic=([-.\w]+),partition=([0-9]+)</td>
<td>The number of log segments in a partition.</td>
</tr>
<tr>
<td>First offset in a partition</td>
<td>kafka.log:type=Log,name=LogStartOffset,topic=([-.\w]+),partition=([0-9]+)</td>
<td>The first offset in a partition.</td>
</tr>
<tr>
<td>Last offset in a partition</td>
<td>kafka.log:type=Log,name=LogEndOffset,topic=([-.\w]+),partition=([0-9]+)</td>
<td>The last offset in a partition.</td>
</tr>
</tbody>
</table>
<h3 id="tiered-storage-monitoring"><a href="https://kafka.apache.org/documentation/#tiered_storage_monitoring">Tiered Storage Monitoring</a></h3>
<p>以下一组指标可用于监控分层存储功能：</p>
<table>
<thead>
<tr>
<th>METRIC/ATTRIBUTE NAME</th>
<th>DESCRIPTION</th>
<th>MBEAN NAME</th>
</tr>
</thead>
<tbody>
<tr>
<td>Remote Fetch Bytes Per Sec</td>
<td>Rate of bytes read from remote storage per topic. Omitting 'topic=(...)' will yield the all-topic rate</td>
<td>kafka.server:type=BrokerTopicMetrics,name=RemoteFetchBytesPerSec,topic=([-.\w]+)</td>
</tr>
<tr>
<td>Remote Fetch Requests Per Sec</td>
<td>Rate of read requests from remote storage per topic. Omitting 'topic=(...)' will yield the all-topic rate</td>
<td>kafka.server:type=BrokerTopicMetrics,name=RemoteFetchRequestsPerSec,topic=([-.\w]+)</td>
</tr>
<tr>
<td>Remote Fetch Errors Per Sec</td>
<td>Rate of read errors from remote storage per topic. Omitting 'topic=(...)' will yield the all-topic rate</td>
<td>kafka.server:type=BrokerTopicMetrics,name=RemoteFetchErrorsPerSec,topic=([-.\w]+)</td>
</tr>
<tr>
<td>Remote Copy Bytes Per Sec</td>
<td>Rate of bytes copied to remote storage per topic. Omitting 'topic=(...)' will yield the all-topic rate</td>
<td>kafka.server:type=BrokerTopicMetrics,name=RemoteCopyBytesPerSec,topic=([-.\w]+)</td>
</tr>
<tr>
<td>Remote Copy Requests Per Sec</td>
<td>Rate of write requests to remote storage per topic. Omitting 'topic=(...)' will yield the all-topic rate</td>
<td>kafka.server:type=BrokerTopicMetrics,name=RemoteCopyRequestsPerSec,topic=([-.\w]+)</td>
</tr>
<tr>
<td>Remote Copy Errors Per Sec</td>
<td>Rate of write errors from remote storage per topic. Omitting 'topic=(...)' will yield the all-topic rate</td>
<td>kafka.server:type=BrokerTopicMetrics,name=RemoteCopyErrorsPerSec,topic=([-.\w]+)</td>
</tr>
<tr>
<td>RemoteLogReader Task Queue Size</td>
<td>Size of the queue holding remote storage read tasks</td>
<td>org.apache.kafka.storage.internals.log:type=RemoteStorageThreadPool,name=RemoteLogReaderTaskQueueSize</td>
</tr>
<tr>
<td>RemoteLogReader Avg Idle Percent</td>
<td>Average idle percent of thread pool for processing remote storage read tasks</td>
<td>org.apache.kafka.storage.internals.log:type=RemoteStorageThreadPool,name=RemoteLogReaderAvgIdlePercent</td>
</tr>
<tr>
<td>RemoteLogManager Tasks Avg Idle Percent</td>
<td>Average idle percent of thread pool for copying data to remote storage</td>
<td>kafka.log.remote:type=RemoteLogManager,name=RemoteLogManagerTasksAvgIdlePercent</td>
</tr>
</tbody>
</table>
<h3 id="kraft_1"><a href="https://kafka.apache.org/documentation/#kraft_monitoring">Kraft 监控指标</a></h3>
<p>允许监控 KRaft 仲裁和元数据日志的指标集。<br />
请注意，一些公开的指标取决于节点的角色，如<code>process.roles</code></p>
<h4 id="kraft-quorum-monitoring-metrics"><a href="https://kafka.apache.org/documentation/#kraft_quorum_monitoring">KRaft Quorum Monitoring Metrics</a></h4>
<p>这些指标在 KRaft 集群中的控制器和代理上报告</p>
<table>
<thead>
<tr>
<th>METRIC/ATTRIBUTE NAME</th>
<th>DESCRIPTION</th>
<th>MBEAN NAME</th>
</tr>
</thead>
<tbody>
<tr>
<td>Current State</td>
<td>The current state of this member; possible values are leader, candidate, voted, follower, unattached, observer.</td>
<td>kafka.server:type=raft-metrics,name=current-state</td>
</tr>
<tr>
<td>Current Leader</td>
<td>The current quorum leader's id; -1 indicates unknown.</td>
<td>kafka.server:type=raft-metrics,name=current-leader</td>
</tr>
<tr>
<td>Current Voted</td>
<td>The current voted leader's id; -1 indicates not voted for anyone.</td>
<td>kafka.server:type=raft-metrics,name=current-vote</td>
</tr>
<tr>
<td>Current Epoch</td>
<td>The current quorum epoch.</td>
<td>kafka.server:type=raft-metrics,name=current-epoch</td>
</tr>
<tr>
<td>High Watermark</td>
<td>The high watermark maintained on this member; -1 if it is unknown.</td>
<td>kafka.server:type=raft-metrics,name=high-watermark</td>
</tr>
<tr>
<td>Log End Offset</td>
<td>The current raft log end offset.</td>
<td>kafka.server:type=raft-metrics,name=log-end-offset</td>
</tr>
<tr>
<td>Number of Unknown Voter Connections</td>
<td>Number of unknown voters whose connection information is not cached. This value of this metric is always 0.</td>
<td>kafka.server:type=raft-metrics,name=number-unknown-voter-connections</td>
</tr>
<tr>
<td>Average Commit Latency</td>
<td>The average time in milliseconds to commit an entry in the raft log.</td>
<td>kafka.server:type=raft-metrics,name=commit-latency-avg</td>
</tr>
<tr>
<td>Maximum Commit Latency</td>
<td>The maximum time in milliseconds to commit an entry in the raft log.</td>
<td>kafka.server:type=raft-metrics,name=commit-latency-max</td>
</tr>
<tr>
<td>Average Election Latency</td>
<td>The average time in milliseconds spent on electing a new leader.</td>
<td>kafka.server:type=raft-metrics,name=election-latency-avg</td>
</tr>
<tr>
<td>Maximum Election Latency</td>
<td>The maximum time in milliseconds spent on electing a new leader.</td>
<td>kafka.server:type=raft-metrics,name=election-latency-max</td>
</tr>
<tr>
<td>Fetch Records Rate</td>
<td>The average number of records fetched from the leader of the raft quorum.</td>
<td>kafka.server:type=raft-metrics,name=fetch-records-rate</td>
</tr>
<tr>
<td>Append Records Rate</td>
<td>The average number of records appended per sec by the leader of the raft quorum.</td>
<td>kafka.server:type=raft-metrics,name=append-records-rate</td>
</tr>
<tr>
<td>Average Poll Idle Ratio</td>
<td>The average fraction of time the client's poll() is idle as opposed to waiting for the user code to process records.</td>
<td>kafka.server:type=raft-metrics,name=poll-idle-ratio-avg</td>
</tr>
<tr>
<td>Current Metadata Version</td>
<td>Outputs the feature level of the current effective metadata version.</td>
<td>kafka.server:type=MetadataLoader,name=CurrentMetadataVersion</td>
</tr>
<tr>
<td>Metadata Snapshot Load Count</td>
<td>The total number of times we have loaded a KRaft snapshot since the process was started.</td>
<td>kafka.server:type=MetadataLoader,name=HandleLoadSnapshotCount</td>
</tr>
<tr>
<td>Latest Metadata Snapshot Size</td>
<td>The total size in bytes of the latest snapshot that the node has generated. If none have been generated yet, this is the size of the latest snapshot that was loaded. If no snapshots have been generated or loaded, this is 0.</td>
<td>kafka.server:type=SnapshotEmitter,name=LatestSnapshotGeneratedBytes</td>
</tr>
<tr>
<td>Latest Metadata Snapshot Age</td>
<td>The interval in milliseconds since the latest snapshot that the node has generated. If none have been generated yet, this is approximately the time delta since the process was started.</td>
<td>kafka.server:type=SnapshotEmitter,name=LatestSnapshotGeneratedAgeMs</td>
</tr>
</tbody>
</table>
<h4 id="kraft-controller-monitoring-metrics"><a href="https://kafka.apache.org/documentation/#kraft_controller_monitoring">KRaft Controller Monitoring Metrics</a></h4>
<table>
<thead>
<tr>
<th>METRIC/ATTRIBUTE NAME</th>
<th>DESCRIPTION</th>
<th>MBEAN NAME</th>
</tr>
</thead>
<tbody>
<tr>
<td>Active Controller Count</td>
<td>The number of Active Controllers on this node. Valid values are '0' or '1'.</td>
<td>kafka.controller:type=KafkaController,name=ActiveControllerCount</td>
</tr>
<tr>
<td>Event Queue Time Ms</td>
<td>A Histogram of the time in milliseconds that requests spent waiting in the Controller Event Queue.</td>
<td>kafka.controller:type=ControllerEventManager,name=EventQueueTimeMs</td>
</tr>
<tr>
<td>Event Queue Processing Time Ms</td>
<td>A Histogram of the time in milliseconds that requests spent being processed in the Controller Event Queue.</td>
<td>kafka.controller:type=ControllerEventManager,name=EventQueueProcessingTimeMs</td>
</tr>
<tr>
<td>Fenced Broker Count</td>
<td>The number of fenced brokers as observed by this Controller.</td>
<td>kafka.controller:type=KafkaController,name=FencedBrokerCount</td>
</tr>
<tr>
<td>Active Broker Count</td>
<td>The number of active brokers as observed by this Controller.</td>
<td>kafka.controller:type=KafkaController,name=ActiveBrokerCount</td>
</tr>
<tr>
<td>Global Topic Count</td>
<td>The number of global topics as observed by this Controller.</td>
<td>kafka.controller:type=KafkaController,name=GlobalTopicCount</td>
</tr>
<tr>
<td>Global Partition Count</td>
<td>The number of global partitions as observed by this Controller.</td>
<td>kafka.controller:type=KafkaController,name=GlobalPartitionCount</td>
</tr>
<tr>
<td>Offline Partition Count</td>
<td>The number of offline topic partitions (non-internal) as observed by this Controller.</td>
<td>kafka.controller:type=KafkaController,name=OfflinePartitionCount</td>
</tr>
<tr>
<td>Preferred Replica Imbalance Count</td>
<td>The count of topic partitions for which the leader is not the preferred leader.</td>
<td>kafka.controller:type=KafkaController,name=PreferredReplicaImbalanceCount</td>
</tr>
<tr>
<td>Metadata Error Count</td>
<td>The number of times this controller node has encountered an error during metadata log processing.</td>
<td>kafka.controller:type=KafkaController,name=MetadataErrorCount</td>
</tr>
<tr>
<td>Last Applied Record Offset</td>
<td>The offset of the last record from the cluster metadata partition that was applied by the Controller.</td>
<td>kafka.controller:type=KafkaController,name=LastAppliedRecordOffset</td>
</tr>
<tr>
<td>Last Committed Record Offset</td>
<td>The offset of the last record committed to this Controller.</td>
<td>kafka.controller:type=KafkaController,name=LastCommittedRecordOffset</td>
</tr>
<tr>
<td>Last Applied Record Timestamp</td>
<td>The timestamp of the last record from the cluster metadata partition that was applied by the Controller.</td>
<td>kafka.controller:type=KafkaController,name=LastAppliedRecordTimestamp</td>
</tr>
<tr>
<td>Last Applied Record Lag Ms</td>
<td>The difference between now and the timestamp of the last record from the cluster metadata partition that was applied by the controller. For active Controllers the value of this lag is always zero.</td>
<td>kafka.controller:type=KafkaController,name=LastAppliedRecordLagMs</td>
</tr>
<tr>
<td>ZooKeeper Write Behind Lag</td>
<td>The amount of lag in records that ZooKeeper is behind relative to the highest committed record in the metadata log. This metric will only be reported by the active KRaft controller.</td>
<td>kafka.controller:type=KafkaController,name=ZkWriteBehindLag</td>
</tr>
<tr>
<td>ZooKeeper Metadata Snapshot Write Time</td>
<td>The number of milliseconds the KRaft controller took reconciling a snapshot into ZooKeeper.</td>
<td>kafka.controller:type=KafkaController,name=ZkWriteSnapshotTimeMs</td>
</tr>
<tr>
<td>ZooKeeper Metadata Delta Write Time</td>
<td>The number of milliseconds the KRaft controller took writing a delta into ZK.</td>
<td>kafka.controller:type=KafkaController,name=ZkWriteDeltaTimeMs</td>
</tr>
<tr>
<td>Timed-out Broker Heartbeat Count</td>
<td>The number of broker heartbeats that timed out on this controller since the process was started. Note that only active controllers handle heartbeats, so only they will see increases in this metric.</td>
<td>kafka.controller:type=KafkaController,name=TimedOutBrokerHeartbeatCount</td>
</tr>
<tr>
<td>Number Of Operations Started In Event Queue</td>
<td>The total number of controller event queue operations that were started. This includes deferred operations.</td>
<td>kafka.controller:type=KafkaController,name=EventQueueOperationsStartedCount</td>
</tr>
<tr>
<td>Number of Operations Timed Out In Event Queue</td>
<td>The total number of controller event queue operations that timed out before they could be performed.</td>
<td>kafka.controller:type=KafkaController,name=EventQueueOperationsTimedOutCount</td>
</tr>
<tr>
<td>Number Of New Controller Elections</td>
<td>Counts the number of times this node has seen a new controller elected. A transition to the "no leader" state is not counted here. If the same controller as before becomes active, that still counts.</td>
<td>kafka.controller:type=KafkaController,name=NewActiveControllersCount</td>
</tr>
</tbody>
</table>
<h4 id="kraft-broker-monitoring-metrics"><a href="https://kafka.apache.org/documentation/#kraft_broker_monitoring">KRaft Broker Monitoring Metrics</a></h4>
<table>
<thead>
<tr>
<th>METRIC/ATTRIBUTE NAME</th>
<th>DESCRIPTION</th>
<th>MBEAN NAME</th>
</tr>
</thead>
<tbody>
<tr>
<td>Last Applied Record Offset</td>
<td>The offset of the last record from the cluster metadata partition that was applied by the broker</td>
<td>kafka.server:type=broker-metadata-metrics,name=last-applied-record-offset</td>
</tr>
<tr>
<td>Last Applied Record Timestamp</td>
<td>The timestamp of the last record from the cluster metadata partition that was applied by the broker.</td>
<td>kafka.server:type=broker-metadata-metrics,name=last-applied-record-timestamp</td>
</tr>
<tr>
<td>Last Applied Record Lag Ms</td>
<td>The difference between now and the timestamp of the last record from the cluster metadata partition that was applied by the broker</td>
<td>kafka.server:type=broker-metadata-metrics,name=last-applied-record-lag-ms</td>
</tr>
<tr>
<td>Metadata Load Error Count</td>
<td>The number of errors encountered by the BrokerMetadataListener while loading the metadata log and generating a new MetadataDelta based on it.</td>
<td>kafka.server:type=broker-metadata-metrics,name=metadata-load-error-count</td>
</tr>
<tr>
<td>Metadata Apply Error Count</td>
<td>The number of errors encountered by the BrokerMetadataPublisher while applying a new MetadataImage based on the latest MetadataDelta.</td>
<td>kafka.server:type=broker-metadata-metrics,name=metadata-apply-error-count</td>
</tr>
</tbody>
</table>
<h3 id="common-monitoring-metrics-for-producerconsumerconnectstreams"><a href="https://kafka.apache.org/documentation/#selector_monitoring">Common monitoring metrics for producer/consumer/connect/streams</a></h3>
<p>以下指标可用于生产者/消费者/连接器/流实例。有关具体指标，请参阅以下部分。</p>
<table>
<thead>
<tr>
<th>METRIC/ATTRIBUTE NAME</th>
<th>DESCRIPTION</th>
<th>MBEAN NAME</th>
</tr>
</thead>
<tbody>
<tr>
<td>connection-close-rate</td>
<td>Connections closed per second in the window.</td>
<td>kafka.[producer</td>
</tr>
<tr>
<td>connection-close-total</td>
<td>Total connections closed in the window.</td>
<td>kafka.[producer</td>
</tr>
<tr>
<td>connection-creation-rate</td>
<td>New connections established per second in the window.</td>
<td>kafka.[producer</td>
</tr>
<tr>
<td>connection-creation-total</td>
<td>Total new connections established in the window.</td>
<td>kafka.[producer</td>
</tr>
<tr>
<td>network-io-rate</td>
<td>The average number of network operations (reads or writes) on all connections per second.</td>
<td>kafka.[producer</td>
</tr>
<tr>
<td>network-io-total</td>
<td>The total number of network operations (reads or writes) on all connections.</td>
<td>kafka.[producer</td>
</tr>
<tr>
<td>outgoing-byte-rate</td>
<td>The average number of outgoing bytes sent per second to all servers.</td>
<td>kafka.[producer</td>
</tr>
<tr>
<td>outgoing-byte-total</td>
<td>The total number of outgoing bytes sent to all servers.</td>
<td>kafka.[producer</td>
</tr>
<tr>
<td>request-rate</td>
<td>The average number of requests sent per second.</td>
<td>kafka.[producer</td>
</tr>
<tr>
<td>request-total</td>
<td>The total number of requests sent.</td>
<td>kafka.[producer</td>
</tr>
<tr>
<td>request-size-avg</td>
<td>The average size of all requests in the window.</td>
<td>kafka.[producer</td>
</tr>
<tr>
<td>request-size-max</td>
<td>The maximum size of any request sent in the window.</td>
<td>kafka.[producer</td>
</tr>
<tr>
<td>incoming-byte-rate</td>
<td>Bytes/second read off all sockets.</td>
<td>kafka.[producer</td>
</tr>
<tr>
<td>incoming-byte-total</td>
<td>Total bytes read off all sockets.</td>
<td>kafka.[producer</td>
</tr>
<tr>
<td>response-rate</td>
<td>Responses received per second.</td>
<td>kafka.[producer</td>
</tr>
<tr>
<td>response-total</td>
<td>Total responses received.</td>
<td>kafka.[producer</td>
</tr>
<tr>
<td>select-rate</td>
<td>Number of times the I/O layer checked for new I/O to perform per second.</td>
<td>kafka.[producer</td>
</tr>
<tr>
<td>select-total</td>
<td>Total number of times the I/O layer checked for new I/O to perform.</td>
<td>kafka.[producer</td>
</tr>
<tr>
<td>io-wait-time-ns-avg</td>
<td>The average length of time the I/O thread spent waiting for a socket ready for reads or writes in nanoseconds.</td>
<td>kafka.[producer</td>
</tr>
<tr>
<td>io-wait-time-ns-total</td>
<td>The total time the I/O thread spent waiting in nanoseconds.</td>
<td>kafka.[producer</td>
</tr>
<tr>
<td>io-waittime-total</td>
<td><strong>*Deprecated*</strong> The total time the I/O thread spent waiting in nanoseconds. Replacement is <code>io-wait-time-ns-total</code>.</td>
<td>kafka.[producer</td>
</tr>
<tr>
<td>io-wait-ratio</td>
<td>The fraction of time the I/O thread spent waiting.</td>
<td>kafka.[producer</td>
</tr>
<tr>
<td>io-time-ns-avg</td>
<td>The average length of time for I/O per select call in nanoseconds.</td>
<td>kafka.[producer</td>
</tr>
<tr>
<td>io-time-ns-total</td>
<td>The total time the I/O thread spent doing I/O in nanoseconds.</td>
<td>kafka.[producer</td>
</tr>
<tr>
<td>iotime-total</td>
<td><strong>*Deprecated*</strong> The total time the I/O thread spent doing I/O in nanoseconds. Replacement is <code>io-time-ns-total</code>.</td>
<td>kafka.[producer</td>
</tr>
<tr>
<td>io-ratio</td>
<td>The fraction of time the I/O thread spent doing I/O.</td>
<td>kafka.[producer</td>
</tr>
<tr>
<td>connection-count</td>
<td>The current number of active connections.</td>
<td>kafka.[producer</td>
</tr>
<tr>
<td>successful-authentication-rate</td>
<td>Connections per second that were successfully authenticated using SASL or SSL.</td>
<td>kafka.[producer</td>
</tr>
<tr>
<td>successful-authentication-total</td>
<td>Total connections that were successfully authenticated using SASL or SSL.</td>
<td>kafka.[producer</td>
</tr>
<tr>
<td>failed-authentication-rate</td>
<td>Connections per second that failed authentication.</td>
<td>kafka.[producer</td>
</tr>
<tr>
<td>failed-authentication-total</td>
<td>Total connections that failed authentication.</td>
<td>kafka.[producer</td>
</tr>
<tr>
<td>successful-reauthentication-rate</td>
<td>Connections per second that were successfully re-authenticated using SASL.</td>
<td>kafka.[producer</td>
</tr>
<tr>
<td>successful-reauthentication-total</td>
<td>Total connections that were successfully re-authenticated using SASL.</td>
<td>kafka.[producer</td>
</tr>
<tr>
<td>reauthentication-latency-max</td>
<td>The maximum latency in ms observed due to re-authentication.</td>
<td>kafka.[producer</td>
</tr>
<tr>
<td>reauthentication-latency-avg</td>
<td>The average latency in ms observed due to re-authentication.</td>
<td>kafka.[producer</td>
</tr>
<tr>
<td>failed-reauthentication-rate</td>
<td>Connections per second that failed re-authentication.</td>
<td>kafka.[producer</td>
</tr>
<tr>
<td>failed-reauthentication-total</td>
<td>Total connections that failed re-authentication.</td>
<td>kafka.[producer</td>
</tr>
<tr>
<td>successful-authentication-no-reauth-total</td>
<td>Total connections that were successfully authenticated by older, pre-2.2.0 SASL clients that do not support re-authentication. May only be non-zero.</td>
<td>kafka.[producer</td>
</tr>
</tbody>
</table>
<h3 id="common-per-broker-metrics-for-producerconsumerconnectstreams"><a href="https://kafka.apache.org/documentation/#common_node_monitoring">Common Per-broker metrics for producer/consumer/connect/streams</a></h3>
<p>以下指标可用于生产者/消费者/连接器/流实例。有关具体指标，请参阅以下部分。</p>
<table>
<thead>
<tr>
<th>METRIC/ATTRIBUTE NAME</th>
<th>DESCRIPTION</th>
<th>MBEAN NAME</th>
</tr>
</thead>
<tbody>
<tr>
<td>outgoing-byte-rate</td>
<td>The average number of outgoing bytes sent per second for a node.</td>
<td>kafka.[producer</td>
</tr>
<tr>
<td>outgoing-byte-total</td>
<td>The total number of outgoing bytes sent for a node.</td>
<td>kafka.[producer</td>
</tr>
<tr>
<td>request-rate</td>
<td>The average number of requests sent per second for a node.</td>
<td>kafka.[producer</td>
</tr>
<tr>
<td>request-total</td>
<td>The total number of requests sent for a node.</td>
<td>kafka.[producer</td>
</tr>
<tr>
<td>request-size-avg</td>
<td>The average size of all requests in the window for a node.</td>
<td>kafka.[producer</td>
</tr>
<tr>
<td>request-size-max</td>
<td>The maximum size of any request sent in the window for a node.</td>
<td>kafka.[producer</td>
</tr>
<tr>
<td>incoming-byte-rate</td>
<td>The average number of bytes received per second for a node.</td>
<td>kafka.[producer</td>
</tr>
<tr>
<td>incoming-byte-total</td>
<td>The total number of bytes received for a node.</td>
<td>kafka.[producer</td>
</tr>
<tr>
<td>request-latency-avg</td>
<td>The average request latency in ms for a node.</td>
<td>kafka.[producer</td>
</tr>
<tr>
<td>request-latency-max</td>
<td>The maximum request latency in ms for a node.</td>
<td>kafka.[producer</td>
</tr>
<tr>
<td>response-rate</td>
<td>Responses received per second for a node.</td>
<td>kafka.[producer</td>
</tr>
<tr>
<td>response-total</td>
<td>Total responses received for a node.</td>
<td>kafka.[producer</td>
</tr>
</tbody>
</table>
<h3 id="producer-monitoring"><a href="https://kafka.apache.org/documentation/#producer_monitoring">Producer monitoring</a></h3>
<p>以下指标可用于生产者实例。</p>
<table>
<thead>
<tr>
<th>METRIC/ATTRIBUTE NAME</th>
<th>DESCRIPTION</th>
<th>MBEAN NAME</th>
</tr>
</thead>
<tbody>
<tr>
<td>waiting-threads</td>
<td>The number of user threads blocked waiting for buffer memory to enqueue their records.</td>
<td>kafka.producer:type=producer-metrics,client-id=([-.\w]+)</td>
</tr>
<tr>
<td>buffer-total-bytes</td>
<td>The maximum amount of buffer memory the client can use (whether or not it is currently used).</td>
<td>kafka.producer:type=producer-metrics,client-id=([-.\w]+)</td>
</tr>
<tr>
<td>buffer-available-bytes</td>
<td>The total amount of buffer memory that is not being used (either unallocated or in the free list).</td>
<td>kafka.producer:type=producer-metrics,client-id=([-.\w]+)</td>
</tr>
<tr>
<td>bufferpool-wait-time</td>
<td>The fraction of time an appender waits for space allocation.</td>
<td>kafka.producer:type=producer-metrics,client-id=([-.\w]+)</td>
</tr>
<tr>
<td>bufferpool-wait-time-total</td>
<td><strong>*Deprecated*</strong> The total time an appender waits for space allocation in nanoseconds. Replacement is <code>bufferpool-wait-time-ns-total</code></td>
<td>kafka.producer:type=producer-metrics,client-id=([-.\w]+)</td>
</tr>
<tr>
<td>bufferpool-wait-time-ns-total</td>
<td>The total time an appender waits for space allocation in nanoseconds.</td>
<td>kafka.producer:type=producer-metrics,client-id=([-.\w]+)</td>
</tr>
<tr>
<td>flush-time-ns-total</td>
<td>The total time the Producer spent in Producer.flush in nanoseconds.</td>
<td>kafka.producer:type=producer-metrics,client-id=([-.\w]+)</td>
</tr>
<tr>
<td>txn-init-time-ns-total</td>
<td>The total time the Producer spent initializing transactions in nanoseconds (for EOS).</td>
<td>kafka.producer:type=producer-metrics,client-id=([-.\w]+)</td>
</tr>
<tr>
<td>txn-begin-time-ns-total</td>
<td>The total time the Producer spent in beginTransaction in nanoseconds (for EOS).</td>
<td>kafka.producer:type=producer-metrics,client-id=([-.\w]+)</td>
</tr>
<tr>
<td>txn-send-offsets-time-ns-total</td>
<td>The total time the Producer spent sending offsets to transactions in nanoseconds (for EOS).</td>
<td>kafka.producer:type=producer-metrics,client-id=([-.\w]+)</td>
</tr>
<tr>
<td>txn-commit-time-ns-total</td>
<td>The total time the Producer spent committing transactions in nanoseconds (for EOS).</td>
<td>kafka.producer:type=producer-metrics,client-id=([-.\w]+)</td>
</tr>
<tr>
<td>txn-abort-time-ns-total</td>
<td>The total time the Producer spent aborting transactions in nanoseconds (for EOS).</td>
<td>kafka.producer:type=producer-metrics,client-id=([-.\w]+)</td>
</tr>
</tbody>
</table>
<h4 id="producer-sender-metrics"><a href="https://kafka.apache.org/documentation/#producer_sender_monitoring">Producer Sender Metrics</a></h4>
<blockquote>
<p><code>kafka.producer:type=producer-metrics,client-id="{client-id}"</code></p>
</blockquote>
<table>
<thead>
<tr>
<th>ATTRIBUTE NAME</th>
<th>DESCRIPTION</th>
</tr>
</thead>
<tbody>
<tr>
<td>batch-size-avg</td>
<td>The average number of bytes sent per partition per-request.</td>
</tr>
<tr>
<td>batch-size-max</td>
<td>The max number of bytes sent per partition per-request.</td>
</tr>
<tr>
<td>batch-split-rate</td>
<td>The average number of batch splits per second</td>
</tr>
<tr>
<td>batch-split-total</td>
<td>The total number of batch splits</td>
</tr>
<tr>
<td>compression-rate-avg</td>
<td>The average compression rate of record batches, defined as the average ratio of the compressed batch size over the uncompressed size.</td>
</tr>
<tr>
<td>metadata-age</td>
<td>The age in seconds of the current producer metadata being used.</td>
</tr>
<tr>
<td>produce-throttle-time-avg</td>
<td>The average time in ms a request was throttled by a broker</td>
</tr>
<tr>
<td>produce-throttle-time-max</td>
<td>The maximum time in ms a request was throttled by a broker</td>
</tr>
<tr>
<td>record-error-rate</td>
<td>The average per-second number of record sends that resulted in errors</td>
</tr>
<tr>
<td>record-error-total</td>
<td>The total number of record sends that resulted in errors</td>
</tr>
<tr>
<td>record-queue-time-avg</td>
<td>The average time in ms record batches spent in the send buffer.</td>
</tr>
<tr>
<td>record-queue-time-max</td>
<td>The maximum time in ms record batches spent in the send buffer.</td>
</tr>
<tr>
<td>record-retry-rate</td>
<td>The average per-second number of retried record sends</td>
</tr>
<tr>
<td>record-retry-total</td>
<td>The total number of retried record sends</td>
</tr>
<tr>
<td>record-send-rate</td>
<td>The average number of records sent per second.</td>
</tr>
<tr>
<td>record-send-total</td>
<td>The total number of records sent.</td>
</tr>
<tr>
<td>record-size-avg</td>
<td>The average record size</td>
</tr>
<tr>
<td>record-size-max</td>
<td>The maximum record size</td>
</tr>
<tr>
<td>records-per-request-avg</td>
<td>The average number of records per request.</td>
</tr>
<tr>
<td>request-latency-avg</td>
<td>The average request latency in ms</td>
</tr>
<tr>
<td>request-latency-max</td>
<td>The maximum request latency in ms</td>
</tr>
<tr>
<td>requests-in-flight</td>
<td>The current number of in-flight requests awaiting a response.</td>
</tr>
</tbody>
</table>
<blockquote>
<p><code>kafka.producer:type=producer-topic-metrics,client-id="{client-id}",topic="{topic}"</code></p>
</blockquote>
<table>
<thead>
<tr>
<th>ATTRIBUTE NAME</th>
<th>DESCRIPTION</th>
</tr>
</thead>
<tbody>
<tr>
<td>byte-rate</td>
<td>The average number of bytes sent per second for a topic.</td>
</tr>
<tr>
<td>byte-total</td>
<td>The total number of bytes sent for a topic.</td>
</tr>
<tr>
<td>compression-rate</td>
<td>The average compression rate of record batches for a topic, defined as the average ratio of the compressed batch size over the uncompressed size.</td>
</tr>
<tr>
<td>record-error-rate</td>
<td>The average per-second number of record sends that resulted in errors for a topic</td>
</tr>
<tr>
<td>record-error-total</td>
<td>The total number of record sends that resulted in errors for a topic</td>
</tr>
<tr>
<td>record-retry-rate</td>
<td>The average per-second number of retried record sends for a topic</td>
</tr>
<tr>
<td>record-retry-total</td>
<td>The total number of retried record sends for a topic</td>
</tr>
<tr>
<td>record-send-rate</td>
<td>The average number of records sent per second for a topic.</td>
</tr>
<tr>
<td>record-send-total</td>
<td>The total number of records sent for a topic.</td>
</tr>
</tbody>
</table>
<h3 id="consumer-monitoring"><a href="https://kafka.apache.org/documentation/#consumer_monitoring">Consumer monitoring</a></h3>
<p>以下指标可用于消费者实例。</p>
<table>
<thead>
<tr>
<th>METRIC/ATTRIBUTE NAME</th>
<th>DESCRIPTION</th>
<th>MBEAN NAME</th>
</tr>
</thead>
<tbody>
<tr>
<td>time-between-poll-avg</td>
<td>The average delay between invocations of poll().</td>
<td>kafka.consumer:type=consumer-metrics,client-id=([-.\w]+)</td>
</tr>
<tr>
<td>time-between-poll-max</td>
<td>The max delay between invocations of poll().</td>
<td>kafka.consumer:type=consumer-metrics,client-id=([-.\w]+)</td>
</tr>
<tr>
<td>last-poll-seconds-ago</td>
<td>The number of seconds since the last poll() invocation.</td>
<td>kafka.consumer:type=consumer-metrics,client-id=([-.\w]+)</td>
</tr>
<tr>
<td>poll-idle-ratio-avg</td>
<td>The average fraction of time the consumer's poll() is idle as opposed to waiting for the user code to process records.</td>
<td>kafka.consumer:type=consumer-metrics,client-id=([-.\w]+)</td>
</tr>
<tr>
<td>committed-time-ns-total</td>
<td>The total time the Consumer spent in committed in nanoseconds.</td>
<td>kafka.consumer:type=consumer-metrics,client-id=([-.\w]+)</td>
</tr>
<tr>
<td>commit-sync-time-ns-total</td>
<td>The total time the Consumer spent committing offsets in nanoseconds (for AOS).</td>
<td>kafka.consumer:type=consumer-metrics,client-id=([-.\w]+)</td>
</tr>
</tbody>
</table>
<h4 id="consumer-group-metrics"><a href="https://kafka.apache.org/documentation/#consumer_group_monitoring">Consumer Group Metrics</a></h4>
<table>
<thead>
<tr>
<th>METRIC/ATTRIBUTE NAME</th>
<th>DESCRIPTION</th>
<th>MBEAN NAME</th>
</tr>
</thead>
<tbody>
<tr>
<td>commit-latency-avg</td>
<td>The average time taken for a commit request</td>
<td>kafka.consumer:type=consumer-coordinator-metrics,client-id=([-.\w]+)</td>
</tr>
<tr>
<td>commit-latency-max</td>
<td>The max time taken for a commit request</td>
<td>kafka.consumer:type=consumer-coordinator-metrics,client-id=([-.\w]+)</td>
</tr>
<tr>
<td>commit-rate</td>
<td>The number of commit calls per second</td>
<td>kafka.consumer:type=consumer-coordinator-metrics,client-id=([-.\w]+)</td>
</tr>
<tr>
<td>commit-total</td>
<td>The total number of commit calls</td>
<td>kafka.consumer:type=consumer-coordinator-metrics,client-id=([-.\w]+)</td>
</tr>
<tr>
<td>assigned-partitions</td>
<td>The number of partitions currently assigned to this consumer</td>
<td>kafka.consumer:type=consumer-coordinator-metrics,client-id=([-.\w]+)</td>
</tr>
<tr>
<td>heartbeat-response-time-max</td>
<td>The max time taken to receive a response to a heartbeat request</td>
<td>kafka.consumer:type=consumer-coordinator-metrics,client-id=([-.\w]+)</td>
</tr>
<tr>
<td>heartbeat-rate</td>
<td>The average number of heartbeats per second</td>
<td>kafka.consumer:type=consumer-coordinator-metrics,client-id=([-.\w]+)</td>
</tr>
<tr>
<td>heartbeat-total</td>
<td>The total number of heartbeats</td>
<td>kafka.consumer:type=consumer-coordinator-metrics,client-id=([-.\w]+)</td>
</tr>
<tr>
<td>join-time-avg</td>
<td>The average time taken for a group rejoin</td>
<td>kafka.consumer:type=consumer-coordinator-metrics,client-id=([-.\w]+)</td>
</tr>
<tr>
<td>join-time-max</td>
<td>The max time taken for a group rejoin</td>
<td>kafka.consumer:type=consumer-coordinator-metrics,client-id=([-.\w]+)</td>
</tr>
<tr>
<td>join-rate</td>
<td>The number of group joins per second</td>
<td>kafka.consumer:type=consumer-coordinator-metrics,client-id=([-.\w]+)</td>
</tr>
<tr>
<td>join-total</td>
<td>The total number of group joins</td>
<td>kafka.consumer:type=consumer-coordinator-metrics,client-id=([-.\w]+)</td>
</tr>
<tr>
<td>sync-time-avg</td>
<td>The average time taken for a group sync</td>
<td>kafka.consumer:type=consumer-coordinator-metrics,client-id=([-.\w]+)</td>
</tr>
<tr>
<td>sync-time-max</td>
<td>The max time taken for a group sync</td>
<td>kafka.consumer:type=consumer-coordinator-metrics,client-id=([-.\w]+)</td>
</tr>
<tr>
<td>sync-rate</td>
<td>The number of group syncs per second</td>
<td>kafka.consumer:type=consumer-coordinator-metrics,client-id=([-.\w]+)</td>
</tr>
<tr>
<td>sync-total</td>
<td>The total number of group syncs</td>
<td>kafka.consumer:type=consumer-coordinator-metrics,client-id=([-.\w]+)</td>
</tr>
<tr>
<td>rebalance-latency-avg</td>
<td>The average time taken for a group rebalance</td>
<td>kafka.consumer:type=consumer-coordinator-metrics,client-id=([-.\w]+)</td>
</tr>
<tr>
<td>rebalance-latency-max</td>
<td>The max time taken for a group rebalance</td>
<td>kafka.consumer:type=consumer-coordinator-metrics,client-id=([-.\w]+)</td>
</tr>
<tr>
<td>rebalance-latency-total</td>
<td>The total time taken for group rebalances so far</td>
<td>kafka.consumer:type=consumer-coordinator-metrics,client-id=([-.\w]+)</td>
</tr>
<tr>
<td>rebalance-total</td>
<td>The total number of group rebalances participated</td>
<td>kafka.consumer:type=consumer-coordinator-metrics,client-id=([-.\w]+)</td>
</tr>
<tr>
<td>rebalance-rate-per-hour</td>
<td>The number of group rebalance participated per hour</td>
<td>kafka.consumer:type=consumer-coordinator-metrics,client-id=([-.\w]+)</td>
</tr>
<tr>
<td>failed-rebalance-total</td>
<td>The total number of failed group rebalances</td>
<td>kafka.consumer:type=consumer-coordinator-metrics,client-id=([-.\w]+)</td>
</tr>
<tr>
<td>failed-rebalance-rate-per-hour</td>
<td>The number of failed group rebalance event per hour</td>
<td>kafka.consumer:type=consumer-coordinator-metrics,client-id=([-.\w]+)</td>
</tr>
<tr>
<td>last-rebalance-seconds-ago</td>
<td>The number of seconds since the last rebalance event</td>
<td>kafka.consumer:type=consumer-coordinator-metrics,client-id=([-.\w]+)</td>
</tr>
<tr>
<td>last-heartbeat-seconds-ago</td>
<td>The number of seconds since the last controller heartbeat</td>
<td>kafka.consumer:type=consumer-coordinator-metrics,client-id=([-.\w]+)</td>
</tr>
<tr>
<td>partitions-revoked-latency-avg</td>
<td>The average time taken by the on-partitions-revoked rebalance listener callback</td>
<td>kafka.consumer:type=consumer-coordinator-metrics,client-id=([-.\w]+)</td>
</tr>
<tr>
<td>partitions-revoked-latency-max</td>
<td>The max time taken by the on-partitions-revoked rebalance listener callback</td>
<td>kafka.consumer:type=consumer-coordinator-metrics,client-id=([-.\w]+)</td>
</tr>
<tr>
<td>partitions-assigned-latency-avg</td>
<td>The average time taken by the on-partitions-assigned rebalance listener callback</td>
<td>kafka.consumer:type=consumer-coordinator-metrics,client-id=([-.\w]+)</td>
</tr>
<tr>
<td>partitions-assigned-latency-max</td>
<td>The max time taken by the on-partitions-assigned rebalance listener callback</td>
<td>kafka.consumer:type=consumer-coordinator-metrics,client-id=([-.\w]+)</td>
</tr>
<tr>
<td>partitions-lost-latency-avg</td>
<td>The average time taken by the on-partitions-lost rebalance listener callback</td>
<td>kafka.consumer:type=consumer-coordinator-metrics,client-id=([-.\w]+)</td>
</tr>
<tr>
<td>partitions-lost-latency-max</td>
<td>The max time taken by the on-partitions-lost rebalance listener callback</td>
<td>kafka.consumer:type=consumer-coordinator-metrics,client-id=([-.\w]+)</td>
</tr>
</tbody>
</table>
<h4 id="consumer-fetch-metrics"><a href="https://kafka.apache.org/documentation/#consumer_fetch_monitoring">Consumer Fetch Metrics</a></h4>
<blockquote>
<p><code>kafka.consumer:type=consumer-fetch-manager-metrics,client-id="{client-id}"</code></p>
</blockquote>
<table>
<thead>
<tr>
<th>ATTRIBUTE NAME</th>
<th>DESCRIPTION</th>
</tr>
</thead>
<tbody>
<tr>
<td>bytes-consumed-rate</td>
<td>The average number of bytes consumed per second</td>
</tr>
<tr>
<td>bytes-consumed-total</td>
<td>The total number of bytes consumed</td>
</tr>
<tr>
<td>fetch-latency-avg</td>
<td>The average time taken for a fetch request.</td>
</tr>
<tr>
<td>fetch-latency-max</td>
<td>The max time taken for any fetch request.</td>
</tr>
<tr>
<td>fetch-rate</td>
<td>The number of fetch requests per second.</td>
</tr>
<tr>
<td>fetch-size-avg</td>
<td>The average number of bytes fetched per request</td>
</tr>
<tr>
<td>fetch-size-max</td>
<td>The maximum number of bytes fetched per request</td>
</tr>
<tr>
<td>fetch-throttle-time-avg</td>
<td>The average throttle time in ms</td>
</tr>
<tr>
<td>fetch-throttle-time-max</td>
<td>The maximum throttle time in ms</td>
</tr>
<tr>
<td>fetch-total</td>
<td>The total number of fetch requests.</td>
</tr>
<tr>
<td>records-consumed-rate</td>
<td>The average number of records consumed per second</td>
</tr>
<tr>
<td>records-consumed-total</td>
<td>The total number of records consumed</td>
</tr>
<tr>
<td>records-lag-max</td>
<td>The maximum lag in terms of number of records for any partition in this window. NOTE: This is based on current offset and not committed offset</td>
</tr>
<tr>
<td>records-lead-min</td>
<td>The minimum lead in terms of number of records for any partition in this window</td>
</tr>
<tr>
<td>records-per-request-avg</td>
<td>The average number of records in each request</td>
</tr>
</tbody>
</table>
<blockquote>
<p><code>kafka.consumer:type=consumer-fetch-manager-metrics,client-id="{client-id}",topic="{topic}"</code></p>
</blockquote>
<table>
<thead>
<tr>
<th>ATTRIBUTE NAME</th>
<th>DESCRIPTION</th>
</tr>
</thead>
<tbody>
<tr>
<td>bytes-consumed-rate</td>
<td>The average number of bytes consumed per second for a topic</td>
</tr>
<tr>
<td>bytes-consumed-total</td>
<td>The total number of bytes consumed for a topic</td>
</tr>
<tr>
<td>fetch-size-avg</td>
<td>The average number of bytes fetched per request for a topic</td>
</tr>
<tr>
<td>fetch-size-max</td>
<td>The maximum number of bytes fetched per request for a topic</td>
</tr>
<tr>
<td>records-consumed-rate</td>
<td>The average number of records consumed per second for a topic</td>
</tr>
<tr>
<td>records-consumed-total</td>
<td>The total number of records consumed for a topic</td>
</tr>
<tr>
<td>records-per-request-avg</td>
<td>The average number of records in each request for a topic</td>
</tr>
</tbody>
</table>
<blockquote>
<p><code>kafka.consumer:type=consumer-fetch-manager-metrics,partition="{partition}",topic="{topic}",client-id="{client-id}"</code></p>
</blockquote>
<table>
<thead>
<tr>
<th>ATTRIBUTE NAME</th>
<th>DESCRIPTION</th>
</tr>
</thead>
<tbody>
<tr>
<td>preferred-read-replica</td>
<td>The current read replica for the partition, or -1 if reading from leader</td>
</tr>
<tr>
<td>records-lag</td>
<td>The latest lag of the partition</td>
</tr>
<tr>
<td>records-lag-avg</td>
<td>The average lag of the partition</td>
</tr>
<tr>
<td>records-lag-max</td>
<td>The max lag of the partition</td>
</tr>
<tr>
<td>records-lead</td>
<td>The latest lead of the partition</td>
</tr>
<tr>
<td>records-lead-avg</td>
<td>The average lead of the partition</td>
</tr>
<tr>
<td>records-lead-min</td>
<td>The min lead of the partition</td>
</tr>
</tbody>
</table>
<h3 id="connect-monitoring"><a href="https://kafka.apache.org/documentation/#connect_monitoring">Connect Monitoring</a></h3>
<p>Connect 工作进程包含所有生产者和消费者指标以及特定于 Connect 的指标。工作进程本身有许多指标，而每个连接器和任务都有其他指标。[2023-05-22 16:22:33,884] INFO Metrics 调度程序关闭（org.apache.kafka.common.metrics.Metrics:693） [2023-05-22 16:22:33,886] INFO Metrics 记者关闭（org.apache.kafka.common.metrics.Metrics:693） apache.kafka.common.metrics.Metrics:703)</p>
<blockquote>
<p><code>kafka.connect:type=connect-worker-metrics</code></p>
</blockquote>
<table>
<thead>
<tr>
<th>ATTRIBUTE NAME</th>
<th>DESCRIPTION</th>
</tr>
</thead>
<tbody>
<tr>
<td>connector-count</td>
<td>The number of connectors run in this worker.</td>
</tr>
<tr>
<td>connector-startup-attempts-total</td>
<td>The total number of connector startups that this worker has attempted.</td>
</tr>
<tr>
<td>connector-startup-failure-percentage</td>
<td>The average percentage of this worker's connectors starts that failed.</td>
</tr>
<tr>
<td>connector-startup-failure-total</td>
<td>The total number of connector starts that failed.</td>
</tr>
<tr>
<td>connector-startup-success-percentage</td>
<td>The average percentage of this worker's connectors starts that succeeded.</td>
</tr>
<tr>
<td>connector-startup-success-total</td>
<td>The total number of connector starts that succeeded.</td>
</tr>
<tr>
<td>task-count</td>
<td>The number of tasks run in this worker.</td>
</tr>
<tr>
<td>task-startup-attempts-total</td>
<td>The total number of task startups that this worker has attempted.</td>
</tr>
<tr>
<td>task-startup-failure-percentage</td>
<td>The average percentage of this worker's tasks starts that failed.</td>
</tr>
<tr>
<td>task-startup-failure-total</td>
<td>The total number of task starts that failed.</td>
</tr>
<tr>
<td>task-startup-success-percentage</td>
<td>The average percentage of this worker's tasks starts that succeeded.</td>
</tr>
<tr>
<td>task-startup-success-total</td>
<td>The total number of task starts that succeeded.</td>
</tr>
</tbody>
</table>
<blockquote>
<p><code>kafka.connect:type=connect-worker-metrics,connector="{connector}"</code></p>
</blockquote>
<table>
<thead>
<tr>
<th>ATTRIBUTE NAME</th>
<th>DESCRIPTION</th>
</tr>
</thead>
<tbody>
<tr>
<td>connector-destroyed-task-count</td>
<td>The number of destroyed tasks of the connector on the worker.</td>
</tr>
<tr>
<td>connector-failed-task-count</td>
<td>The number of failed tasks of the connector on the worker.</td>
</tr>
<tr>
<td>connector-paused-task-count</td>
<td>The number of paused tasks of the connector on the worker.</td>
</tr>
<tr>
<td>connector-restarting-task-count</td>
<td>The number of restarting tasks of the connector on the worker.</td>
</tr>
<tr>
<td>connector-running-task-count</td>
<td>The number of running tasks of the connector on the worker.</td>
</tr>
<tr>
<td>connector-total-task-count</td>
<td>The number of tasks of the connector on the worker.</td>
</tr>
<tr>
<td>connector-unassigned-task-count</td>
<td>The number of unassigned tasks of the connector on the worker.</td>
</tr>
</tbody>
</table>
<blockquote>
<p><code>kafka.connect:type=connect-worker-rebalance-metrics</code></p>
</blockquote>
<table>
<thead>
<tr>
<th>ATTRIBUTE NAME</th>
<th>DESCRIPTION</th>
</tr>
</thead>
<tbody>
<tr>
<td>completed-rebalances-total</td>
<td>The total number of rebalances completed by this worker.</td>
</tr>
<tr>
<td>connect-protocol</td>
<td>The Connect protocol used by this cluster</td>
</tr>
<tr>
<td>epoch</td>
<td>The epoch or generation number of this worker.</td>
</tr>
<tr>
<td>leader-name</td>
<td>The name of the group leader.</td>
</tr>
<tr>
<td>rebalance-avg-time-ms</td>
<td>The average time in milliseconds spent by this worker to rebalance.</td>
</tr>
<tr>
<td>rebalance-max-time-ms</td>
<td>The maximum time in milliseconds spent by this worker to rebalance.</td>
</tr>
<tr>
<td>rebalancing</td>
<td>Whether this worker is currently rebalancing.</td>
</tr>
<tr>
<td>time-since-last-rebalance-ms</td>
<td>The time in milliseconds since this worker completed the most recent rebalance.</td>
</tr>
</tbody>
</table>
<blockquote>
<p><code>kafka.connect:type=connector-metrics,connector="{connector}"</code></p>
</blockquote>
<table>
<thead>
<tr>
<th>ATTRIBUTE NAME</th>
<th>DESCRIPTION</th>
</tr>
</thead>
<tbody>
<tr>
<td>connector-class</td>
<td>The name of the connector class.</td>
</tr>
<tr>
<td>connector-type</td>
<td>The type of the connector. One of 'source' or 'sink'.</td>
</tr>
<tr>
<td>connector-version</td>
<td>The version of the connector class, as reported by the connector.</td>
</tr>
<tr>
<td>status</td>
<td>The status of the connector. One of 'unassigned', 'running', 'paused', 'stopped', 'failed', or 'restarting'.</td>
</tr>
</tbody>
</table>
<blockquote>
<p><code>kafka.connect:type=connector-task-metrics,connector="{connector}",task="{task}"</code></p>
</blockquote>
<table>
<thead>
<tr>
<th>ATTRIBUTE NAME</th>
<th>DESCRIPTION</th>
</tr>
</thead>
<tbody>
<tr>
<td>batch-size-avg</td>
<td>The average number of records in the batches the task has processed so far.</td>
</tr>
<tr>
<td>batch-size-max</td>
<td>The number of records in the largest batch the task has processed so far.</td>
</tr>
<tr>
<td>offset-commit-avg-time-ms</td>
<td>The average time in milliseconds taken by this task to commit offsets.</td>
</tr>
<tr>
<td>offset-commit-failure-percentage</td>
<td>The average percentage of this task's offset commit attempts that failed.</td>
</tr>
<tr>
<td>offset-commit-max-time-ms</td>
<td>The maximum time in milliseconds taken by this task to commit offsets.</td>
</tr>
<tr>
<td>offset-commit-success-percentage</td>
<td>The average percentage of this task's offset commit attempts that succeeded.</td>
</tr>
<tr>
<td>pause-ratio</td>
<td>The fraction of time this task has spent in the pause state.</td>
</tr>
<tr>
<td>running-ratio</td>
<td>The fraction of time this task has spent in the running state.</td>
</tr>
<tr>
<td>status</td>
<td>The status of the connector task. One of 'unassigned', 'running', 'paused', 'failed', or 'restarting'.</td>
</tr>
</tbody>
</table>
<blockquote>
<p><code>kafka.connect:type=sink-task-metrics,connector="{connector}",task="{task}"</code></p>
</blockquote>
<table>
<thead>
<tr>
<th>ATTRIBUTE NAME</th>
<th>DESCRIPTION</th>
</tr>
</thead>
<tbody>
<tr>
<td>offset-commit-completion-rate</td>
<td>The average per-second number of offset commit completions that were completed successfully.</td>
</tr>
<tr>
<td>offset-commit-completion-total</td>
<td>The total number of offset commit completions that were completed successfully.</td>
</tr>
<tr>
<td>offset-commit-seq-no</td>
<td>The current sequence number for offset commits.</td>
</tr>
<tr>
<td>offset-commit-skip-rate</td>
<td>The average per-second number of offset commit completions that were received too late and skipped/ignored.</td>
</tr>
<tr>
<td>offset-commit-skip-total</td>
<td>The total number of offset commit completions that were received too late and skipped/ignored.</td>
</tr>
<tr>
<td>partition-count</td>
<td>The number of topic partitions assigned to this task belonging to the named sink connector in this worker.</td>
</tr>
<tr>
<td>put-batch-avg-time-ms</td>
<td>The average time taken by this task to put a batch of sinks records.</td>
</tr>
<tr>
<td>put-batch-max-time-ms</td>
<td>The maximum time taken by this task to put a batch of sinks records.</td>
</tr>
<tr>
<td>sink-record-active-count</td>
<td>The number of records that have been read from Kafka but not yet completely committed/flushed/acknowledged by the sink task.</td>
</tr>
<tr>
<td>sink-record-active-count-avg</td>
<td>The average number of records that have been read from Kafka but not yet completely committed/flushed/acknowledged by the sink task.</td>
</tr>
<tr>
<td>sink-record-active-count-max</td>
<td>The maximum number of records that have been read from Kafka but not yet completely committed/flushed/acknowledged by the sink task.</td>
</tr>
<tr>
<td>sink-record-lag-max</td>
<td>The maximum lag in terms of number of records that the sink task is behind the consumer's position for any topic partitions.</td>
</tr>
<tr>
<td>sink-record-read-rate</td>
<td>The average per-second number of records read from Kafka for this task belonging to the named sink connector in this worker. This is before transformations are applied.</td>
</tr>
<tr>
<td>sink-record-read-total</td>
<td>The total number of records read from Kafka by this task belonging to the named sink connector in this worker, since the task was last restarted.</td>
</tr>
<tr>
<td>sink-record-send-rate</td>
<td>The average per-second number of records output from the transformations and sent/put to this task belonging to the named sink connector in this worker. This is after transformations are applied and excludes any records filtered out by the transformations.</td>
</tr>
<tr>
<td>sink-record-send-total</td>
<td>The total number of records output from the transformations and sent/put to this task belonging to the named sink connector in this worker, since the task was last restarted.</td>
</tr>
</tbody>
</table>
<blockquote>
<p><code>kafka.connect:type=source-task-metrics,connector="{connector}",task="{task}"</code></p>
</blockquote>
<table>
<thead>
<tr>
<th>ATTRIBUTE NAME</th>
<th>DESCRIPTION</th>
</tr>
</thead>
<tbody>
<tr>
<td>poll-batch-avg-time-ms</td>
<td>The average time in milliseconds taken by this task to poll for a batch of source records.</td>
</tr>
<tr>
<td>poll-batch-max-time-ms</td>
<td>The maximum time in milliseconds taken by this task to poll for a batch of source records.</td>
</tr>
<tr>
<td>source-record-active-count</td>
<td>The number of records that have been produced by this task but not yet completely written to Kafka.</td>
</tr>
<tr>
<td>source-record-active-count-avg</td>
<td>The average number of records that have been produced by this task but not yet completely written to Kafka.</td>
</tr>
<tr>
<td>source-record-active-count-max</td>
<td>The maximum number of records that have been produced by this task but not yet completely written to Kafka.</td>
</tr>
<tr>
<td>source-record-poll-rate</td>
<td>The average per-second number of records produced/polled (before transformation) by this task belonging to the named source connector in this worker.</td>
</tr>
<tr>
<td>source-record-poll-total</td>
<td>The total number of records produced/polled (before transformation) by this task belonging to the named source connector in this worker.</td>
</tr>
<tr>
<td>source-record-write-rate</td>
<td>The average per-second number of records written to Kafka for this task belonging to the named source connector in this worker, since the task was last restarted. This is after transformations are applied, and excludes any records filtered out by the transformations.</td>
</tr>
<tr>
<td>source-record-write-total</td>
<td>The number of records output written to Kafka for this task belonging to the named source connector in this worker, since the task was last restarted. This is after transformations are applied, and excludes any records filtered out by the transformations.</td>
</tr>
<tr>
<td>transaction-size-avg</td>
<td>The average number of records in the transactions the task has committed so far.</td>
</tr>
<tr>
<td>transaction-size-max</td>
<td>The number of records in the largest transaction the task has committed so far.</td>
</tr>
<tr>
<td>transaction-size-min</td>
<td>The number of records in the smallest transaction the task has committed so far.</td>
</tr>
</tbody>
</table>
<blockquote>
<p><code>kafka.connect:type=task-error-metrics,connector="{connector}",task="{task}"</code></p>
</blockquote>
<table>
<thead>
<tr>
<th>ATTRIBUTE NAME</th>
<th>DESCRIPTION</th>
</tr>
</thead>
<tbody>
<tr>
<td>deadletterqueue-produce-failures</td>
<td>The number of failed writes to the dead letter queue.</td>
</tr>
<tr>
<td>deadletterqueue-produce-requests</td>
<td>The number of attempted writes to the dead letter queue.</td>
</tr>
<tr>
<td>last-error-timestamp</td>
<td>The epoch timestamp when this task last encountered an error.</td>
</tr>
<tr>
<td>total-errors-logged</td>
<td>The number of errors that were logged.</td>
</tr>
<tr>
<td>total-record-errors</td>
<td>The number of record processing errors in this task.</td>
</tr>
<tr>
<td>total-record-failures</td>
<td>The number of record processing failures in this task.</td>
</tr>
<tr>
<td>total-records-skipped</td>
<td>The number of records skipped due to errors.</td>
</tr>
<tr>
<td>total-retries</td>
<td>The number of operations retried.</td>
</tr>
</tbody>
</table>
<h3 id="streams-monitoring"><a href="https://kafka.apache.org/documentation/#kafka_streams_monitoring">Streams Monitoring</a></h3>
<p>Kafka Streams 实例包含所有生产者和消费者指标以及特定于 Streams 的其他指标。这些指标具有三个记录级别：<code>info</code>、<code>debug</code>和<code>trace</code>。</p>
<p>请注意，指标有 4 层层次结构。在顶层，每个启动的 Kafka Streams 客户端都有客户端级指标。每个客户端都有流线程，有自己的指标。每个流线程都有任务，有自己的指标。每个任务都有多个处理器节点，并具有自己的指标。每个任务还​​有许多状态存储和记录缓存，它们都有自己的指标。</p>
<p>使用以下配置选项指定您要收集的指标：</p>
<pre><code>metrics.recording.level=&quot;info&quot;
</code></pre>
<h4 id="client-metrics"><a href="https://kafka.apache.org/documentation/#kafka_streams_client_monitoring">Client Metrics</a></h4>
<p>以下所有指标的记录级别均为<code>info</code>：</p>
<table>
<thead>
<tr>
<th>METRIC/ATTRIBUTE NAME</th>
<th>DESCRIPTION</th>
<th>MBEAN NAME</th>
</tr>
</thead>
<tbody>
<tr>
<td>version</td>
<td>The version of the Kafka Streams client.</td>
<td>kafka.streams:type=stream-metrics,client-id=([-.\w]+)</td>
</tr>
<tr>
<td>commit-id</td>
<td>The version control commit ID of the Kafka Streams client.</td>
<td>kafka.streams:type=stream-metrics,client-id=([-.\w]+)</td>
</tr>
<tr>
<td>application-id</td>
<td>The application ID of the Kafka Streams client.</td>
<td>kafka.streams:type=stream-metrics,client-id=([-.\w]+)</td>
</tr>
<tr>
<td>topology-description</td>
<td>The description of the topology executed in the Kafka Streams client.</td>
<td>kafka.streams:type=stream-metrics,client-id=([-.\w]+)</td>
</tr>
<tr>
<td>state</td>
<td>The state of the Kafka Streams client.</td>
<td>kafka.streams:type=stream-metrics,client-id=([-.\w]+)</td>
</tr>
<tr>
<td>failed-stream-threads</td>
<td>The number of failed stream threads since the start of the Kafka Streams client.</td>
<td>kafka.streams:type=stream-metrics,client-id=([-.\w]+)</td>
</tr>
</tbody>
</table>
<h4 id="thread-metrics"><a href="https://kafka.apache.org/documentation/#kafka_streams_thread_monitoring">Thread Metrics</a></h4>
<p>以下所有指标的记录级别均为<code>info</code>：</p>
<table>
<thead>
<tr>
<th>METRIC/ATTRIBUTE NAME</th>
<th>DESCRIPTION</th>
<th>MBEAN NAME</th>
</tr>
</thead>
<tbody>
<tr>
<td>commit-latency-avg</td>
<td>The average execution time in ms, for committing, across all running tasks of this thread.</td>
<td>kafka.streams:type=stream-thread-metrics,thread-id=([-.\w]+)</td>
</tr>
<tr>
<td>commit-latency-max</td>
<td>The maximum execution time in ms, for committing, across all running tasks of this thread.</td>
<td>kafka.streams:type=stream-thread-metrics,thread-id=([-.\w]+)</td>
</tr>
<tr>
<td>poll-latency-avg</td>
<td>The average execution time in ms, for consumer polling.</td>
<td>kafka.streams:type=stream-thread-metrics,thread-id=([-.\w]+)</td>
</tr>
<tr>
<td>poll-latency-max</td>
<td>The maximum execution time in ms, for consumer polling.</td>
<td>kafka.streams:type=stream-thread-metrics,thread-id=([-.\w]+)</td>
</tr>
<tr>
<td>process-latency-avg</td>
<td>The average execution time in ms, for processing.</td>
<td>kafka.streams:type=stream-thread-metrics,thread-id=([-.\w]+)</td>
</tr>
<tr>
<td>process-latency-max</td>
<td>The maximum execution time in ms, for processing.</td>
<td>kafka.streams:type=stream-thread-metrics,thread-id=([-.\w]+)</td>
</tr>
<tr>
<td>punctuate-latency-avg</td>
<td>The average execution time in ms, for punctuating.</td>
<td>kafka.streams:type=stream-thread-metrics,thread-id=([-.\w]+)</td>
</tr>
<tr>
<td>punctuate-latency-max</td>
<td>The maximum execution time in ms, for punctuating.</td>
<td>kafka.streams:type=stream-thread-metrics,thread-id=([-.\w]+)</td>
</tr>
<tr>
<td>commit-rate</td>
<td>The average number of commits per second.</td>
<td>kafka.streams:type=stream-thread-metrics,thread-id=([-.\w]+)</td>
</tr>
<tr>
<td>commit-total</td>
<td>The total number of commit calls.</td>
<td>kafka.streams:type=stream-thread-metrics,thread-id=([-.\w]+)</td>
</tr>
<tr>
<td>poll-rate</td>
<td>The average number of consumer poll calls per second.</td>
<td>kafka.streams:type=stream-thread-metrics,thread-id=([-.\w]+)</td>
</tr>
<tr>
<td>poll-total</td>
<td>The total number of consumer poll calls.</td>
<td>kafka.streams:type=stream-thread-metrics,thread-id=([-.\w]+)</td>
</tr>
<tr>
<td>process-rate</td>
<td>The average number of processed records per second.</td>
<td>kafka.streams:type=stream-thread-metrics,thread-id=([-.\w]+)</td>
</tr>
<tr>
<td>process-total</td>
<td>The total number of processed records.</td>
<td>kafka.streams:type=stream-thread-metrics,thread-id=([-.\w]+)</td>
</tr>
<tr>
<td>punctuate-rate</td>
<td>The average number of punctuate calls per second.</td>
<td>kafka.streams:type=stream-thread-metrics,thread-id=([-.\w]+)</td>
</tr>
<tr>
<td>punctuate-total</td>
<td>The total number of punctuate calls.</td>
<td>kafka.streams:type=stream-thread-metrics,thread-id=([-.\w]+)</td>
</tr>
<tr>
<td>task-created-rate</td>
<td>The average number of tasks created per second.</td>
<td>kafka.streams:type=stream-thread-metrics,thread-id=([-.\w]+)</td>
</tr>
<tr>
<td>task-created-total</td>
<td>The total number of tasks created.</td>
<td>kafka.streams:type=stream-thread-metrics,thread-id=([-.\w]+)</td>
</tr>
<tr>
<td>task-closed-rate</td>
<td>The average number of tasks closed per second.</td>
<td>kafka.streams:type=stream-thread-metrics,thread-id=([-.\w]+)</td>
</tr>
<tr>
<td>task-closed-total</td>
<td>The total number of tasks closed.</td>
<td>kafka.streams:type=stream-thread-metrics,thread-id=([-.\w]+)</td>
</tr>
<tr>
<td>blocked-time-ns-total</td>
<td>The total time the thread spent blocked on kafka.</td>
<td>kafka.streams:type=stream-thread-metrics,thread-id=([-.\w]+)</td>
</tr>
<tr>
<td>thread-start-time</td>
<td>The time that the thread was started.</td>
<td>kafka.streams:type=stream-thread-metrics,thread-id=([-.\w]+)</td>
</tr>
</tbody>
</table>
<h4 id="task-metrics"><a href="https://kafka.apache.org/documentation/#kafka_streams_task_monitoring">Task Metrics</a></h4>
<p>以下所有指标的记录级别均为<code>debug</code>，但 drop-records-* 和 active-process-ratio 指标的记录级别除外<code>info</code>：</p>
<table>
<thead>
<tr>
<th>METRIC/ATTRIBUTE NAME</th>
<th>DESCRIPTION</th>
<th>MBEAN NAME</th>
</tr>
</thead>
<tbody>
<tr>
<td>process-latency-avg</td>
<td>The average execution time in ns, for processing.</td>
<td>kafka.streams:type=stream-task-metrics,thread-id=([-.\w]+),task-id=([-.\w]+)</td>
</tr>
<tr>
<td>process-latency-max</td>
<td>The maximum execution time in ns, for processing.</td>
<td>kafka.streams:type=stream-task-metrics,thread-id=([-.\w]+),task-id=([-.\w]+)</td>
</tr>
<tr>
<td>process-rate</td>
<td>The average number of processed records per second across all source processor nodes of this task.</td>
<td>kafka.streams:type=stream-task-metrics,thread-id=([-.\w]+),task-id=([-.\w]+)</td>
</tr>
<tr>
<td>process-total</td>
<td>The total number of processed records across all source processor nodes of this task.</td>
<td>kafka.streams:type=stream-task-metrics,thread-id=([-.\w]+),task-id=([-.\w]+)</td>
</tr>
<tr>
<td>commit-latency-avg</td>
<td>The average execution time in ns, for committing.</td>
<td>kafka.streams:type=stream-task-metrics,thread-id=([-.\w]+),task-id=([-.\w]+)</td>
</tr>
<tr>
<td>commit-latency-max</td>
<td>The maximum execution time in ns, for committing.</td>
<td>kafka.streams:type=stream-task-metrics,thread-id=([-.\w]+),task-id=([-.\w]+)</td>
</tr>
<tr>
<td>commit-rate</td>
<td>The average number of commit calls per second.</td>
<td>kafka.streams:type=stream-task-metrics,thread-id=([-.\w]+),task-id=([-.\w]+)</td>
</tr>
<tr>
<td>commit-total</td>
<td>The total number of commit calls.</td>
<td>kafka.streams:type=stream-task-metrics,thread-id=([-.\w]+),task-id=([-.\w]+)</td>
</tr>
<tr>
<td>record-lateness-avg</td>
<td>The average observed lateness of records (stream time - record timestamp).</td>
<td>kafka.streams:type=stream-task-metrics,thread-id=([-.\w]+),task-id=([-.\w]+)</td>
</tr>
<tr>
<td>record-lateness-max</td>
<td>The max observed lateness of records (stream time - record timestamp).</td>
<td>kafka.streams:type=stream-task-metrics,thread-id=([-.\w]+),task-id=([-.\w]+)</td>
</tr>
<tr>
<td>enforced-processing-rate</td>
<td>The average number of enforced processings per second.</td>
<td>kafka.streams:type=stream-task-metrics,thread-id=([-.\w]+),task-id=([-.\w]+)</td>
</tr>
<tr>
<td>enforced-processing-total</td>
<td>The total number enforced processings.</td>
<td>kafka.streams:type=stream-task-metrics,thread-id=([-.\w]+),task-id=([-.\w]+)</td>
</tr>
<tr>
<td>dropped-records-rate</td>
<td>The average number of records dropped within this task.</td>
<td>kafka.streams:type=stream-task-metrics,thread-id=([-.\w]+),task-id=([-.\w]+)</td>
</tr>
<tr>
<td>dropped-records-total</td>
<td>The total number of records dropped within this task.</td>
<td>kafka.streams:type=stream-task-metrics,thread-id=([-.\w]+),task-id=([-.\w]+)</td>
</tr>
<tr>
<td>active-process-ratio</td>
<td>The fraction of time the stream thread spent on processing this task among all assigned active tasks.</td>
<td>kafka.streams:type=stream-task-metrics,thread-id=([-.\w]+),task-id=([-.\w]+)</td>
</tr>
</tbody>
</table>
<h4 id="processor-node-metrics"><a href="https://kafka.apache.org/documentation/#kafka_streams_node_monitoring">Processor Node Metrics</a></h4>
<p>以下指标仅在某些类型的节点上可用，即 process-* 指标仅适用于源处理器节点，suppression-emit-* 指标仅适用于抑制操作节点，而 record-e2e-latency- * 指标仅适用于源处理器节点和终端节点（没有后继节点的节点）。所有指标的记录级别均为<code>debug</code>，但 record-e2e-latency-* 指标的记录级别除外<code>info</code>：</p>
<table>
<thead>
<tr>
<th>METRIC/ATTRIBUTE NAME</th>
<th>DESCRIPTION</th>
<th>MBEAN NAME</th>
</tr>
</thead>
<tbody>
<tr>
<td>bytes-consumed-total</td>
<td>The total number of bytes consumed by a source processor node.</td>
<td>kafka.streams:type=stream-topic-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),processor-node-id=([-.\w]+),topic=([-.\w]+)</td>
</tr>
<tr>
<td>bytes-produced-total</td>
<td>The total number of bytes produced by a sink processor node.</td>
<td>kafka.streams:type=stream-topic-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),processor-node-id=([-.\w]+),topic=([-.\w]+)</td>
</tr>
<tr>
<td>process-rate</td>
<td>The average number of records processed by a source processor node per second.</td>
<td>kafka.streams:type=stream-processor-node-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),processor-node-id=([-.\w]+)</td>
</tr>
<tr>
<td>process-total</td>
<td>The total number of records processed by a source processor node per second.</td>
<td>kafka.streams:type=stream-processor-node-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),processor-node-id=([-.\w]+)</td>
</tr>
<tr>
<td>suppression-emit-rate</td>
<td>The rate at which records that have been emitted downstream from suppression operation nodes.</td>
<td>kafka.streams:type=stream-processor-node-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),processor-node-id=([-.\w]+)</td>
</tr>
<tr>
<td>suppression-emit-total</td>
<td>The total number of records that have been emitted downstream from suppression operation nodes.</td>
<td>kafka.streams:type=stream-processor-node-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),processor-node-id=([-.\w]+)</td>
</tr>
<tr>
<td>record-e2e-latency-avg</td>
<td>The average end-to-end latency of a record, measured by comparing the record timestamp with the system time when it has been fully processed by the node.</td>
<td>kafka.streams:type=stream-processor-node-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),processor-node-id=([-.\w]+)</td>
</tr>
<tr>
<td>record-e2e-latency-max</td>
<td>The maximum end-to-end latency of a record, measured by comparing the record timestamp with the system time when it has been fully processed by the node.</td>
<td>kafka.streams:type=stream-processor-node-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),processor-node-id=([-.\w]+)</td>
</tr>
<tr>
<td>record-e2e-latency-min</td>
<td>The minimum end-to-end latency of a record, measured by comparing the record timestamp with the system time when it has been fully processed by the node.</td>
<td>kafka.streams:type=stream-processor-node-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),processor-node-id=([-.\w]+)</td>
</tr>
<tr>
<td>records-consumed-total</td>
<td>The total number of records consumed by a source processor node.</td>
<td>kafka.streams:type=stream-topic-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),processor-node-id=([-.\w]+),topic=([-.\w]+)</td>
</tr>
<tr>
<td>records-produced-total</td>
<td>The total number of records produced by a sink processor node.</td>
<td>kafka.streams:type=stream-topic-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),processor-node-id=([-.\w]+),topic=([-.\w]+)</td>
</tr>
</tbody>
</table>
<h4 id="state-store-metrics"><a href="https://kafka.apache.org/documentation/#kafka_streams_store_monitoring">State Store Metrics</a></h4>
<p>以下所有指标的记录级别均为<code>debug</code>，但 record-e2e-latency-* 指标的记录级别除外<code>trace</code>。请注意，该<code>store-scope</code>值是在<code>StoreSupplier#metricsScope()</code>用户自定义状态存储中指定的；对于内置状态存储，目前我们有：</p>
<ul>
<li><code>in-memory-state</code></li>
<li><code>in-memory-lru-state</code></li>
<li><code>in-memory-window-state</code></li>
<li><code>in-memory-suppression</code>（用于抑制缓冲液）</li>
<li><code>rocksdb-state</code>（对于 RocksDB 支持的键值存储）</li>
<li><code>rocksdb-window-state</code>（对于 RocksDB 支持的橱窗商店）</li>
<li><code>rocksdb-session-state</code>（对于 RocksDB 支持的会话存储）</li>
</ul>
<p>指标suppression-buffer-size-avg、suppression-buffer-size-max、suppression-buffer-count-avg和suppression-buffer-count-max仅适用于抑制缓冲区。所有其他指标均不可用于抑制缓冲区。</p>
<table>
<thead>
<tr>
<th>METRIC/ATTRIBUTE NAME</th>
<th>DESCRIPTION</th>
<th>MBEAN NAME</th>
</tr>
</thead>
<tbody>
<tr>
<td>put-latency-avg</td>
<td>The average put execution time in ns.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
<tr>
<td>put-latency-max</td>
<td>The maximum put execution time in ns.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
<tr>
<td>put-if-absent-latency-avg</td>
<td>The average put-if-absent execution time in ns.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
<tr>
<td>put-if-absent-latency-max</td>
<td>The maximum put-if-absent execution time in ns.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
<tr>
<td>get-latency-avg</td>
<td>The average get execution time in ns.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
<tr>
<td>get-latency-max</td>
<td>The maximum get execution time in ns.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
<tr>
<td>delete-latency-avg</td>
<td>The average delete execution time in ns.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
<tr>
<td>delete-latency-max</td>
<td>The maximum delete execution time in ns.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
<tr>
<td>put-all-latency-avg</td>
<td>The average put-all execution time in ns.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
<tr>
<td>put-all-latency-max</td>
<td>The maximum put-all execution time in ns.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
<tr>
<td>all-latency-avg</td>
<td>The average all operation execution time in ns.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
<tr>
<td>all-latency-max</td>
<td>The maximum all operation execution time in ns.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
<tr>
<td>range-latency-avg</td>
<td>The average range execution time in ns.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
<tr>
<td>range-latency-max</td>
<td>The maximum range execution time in ns.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
<tr>
<td>flush-latency-avg</td>
<td>The average flush execution time in ns.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
<tr>
<td>flush-latency-max</td>
<td>The maximum flush execution time in ns.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
<tr>
<td>restore-latency-avg</td>
<td>The average restore execution time in ns.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
<tr>
<td>restore-latency-max</td>
<td>The maximum restore execution time in ns.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
<tr>
<td>put-rate</td>
<td>The average put rate for this store.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
<tr>
<td>put-if-absent-rate</td>
<td>The average put-if-absent rate for this store.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
<tr>
<td>get-rate</td>
<td>The average get rate for this store.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
<tr>
<td>delete-rate</td>
<td>The average delete rate for this store.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
<tr>
<td>put-all-rate</td>
<td>The average put-all rate for this store.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
<tr>
<td>all-rate</td>
<td>The average all operation rate for this store.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
<tr>
<td>range-rate</td>
<td>The average range rate for this store.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
<tr>
<td>flush-rate</td>
<td>The average flush rate for this store.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
<tr>
<td>restore-rate</td>
<td>The average restore rate for this store.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
<tr>
<td>suppression-buffer-size-avg</td>
<td>The average total size, in bytes, of the buffered data over the sampling window.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),in-memory-suppression-id=([-.\w]+)</td>
</tr>
<tr>
<td>suppression-buffer-size-max</td>
<td>The maximum total size, in bytes, of the buffered data over the sampling window.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),in-memory-suppression-id=([-.\w]+)</td>
</tr>
<tr>
<td>suppression-buffer-count-avg</td>
<td>The average number of records buffered over the sampling window.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),in-memory-suppression-id=([-.\w]+)</td>
</tr>
<tr>
<td>suppression-buffer-count-max</td>
<td>The maximum number of records buffered over the sampling window.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),in-memory-suppression-id=([-.\w]+)</td>
</tr>
<tr>
<td>record-e2e-latency-avg</td>
<td>The average end-to-end latency of a record, measured by comparing the record timestamp with the system time when it has been fully processed by the node.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
<tr>
<td>record-e2e-latency-max</td>
<td>The maximum end-to-end latency of a record, measured by comparing the record timestamp with the system time when it has been fully processed by the node.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
<tr>
<td>record-e2e-latency-min</td>
<td>The minimum end-to-end latency of a record, measured by comparing the record timestamp with the system time when it has been fully processed by the node.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
</tbody>
</table>
<h4 id="rocksdb-metrics"><a href="https://kafka.apache.org/documentation/#kafka_streams_rocksdb_monitoring">RocksDB Metrics</a></h4>
<p>RocksDB 指标分为基于统计的指标和基于属性的指标。前者是从 RocksDB 状态存储收集的统计信息中记录的，而后者是从 RocksDB 公开的属性中记录的。RocksDB 收集的统计数据提供了一段时间内的累积测量值，例如写入状态存储的字节数。RocksDB 公开的属性提供当前测量值，例如当前使用的内存量。请注意，<code>store-scope</code>内置 RocksDB 状态存储当前如下：</p>
<ul>
<li><code>rocksdb-state</code>（对于 RocksDB 支持的键值存储）</li>
<li><code>rocksdb-window-state</code>（对于 RocksDB 支持的橱窗商店）</li>
<li><code>rocksdb-session-state</code>（对于 RocksDB 支持的会话存储）</li>
</ul>
<p><strong>RocksDB 基于统计的指标：</strong> 以下所有基于统计的指标的记录级别都是 ，<code>debug</code>因为在 RocksDB 中收集统计数据<a href="https://github.com/facebook/rocksdb/wiki/Statistics#stats-level-and-performance-costs">可能会对性能产生影响</a>。每分钟从 RocksDB 状态存储收集基于统计数据的指标。如果状态存储由多个 RocksDB 实例组成（如 WindowStores 和 SessionStores 的情况），则每个指标都会报告状态存储的 RocksDB 实例的聚合。</p>
<table>
<thead>
<tr>
<th>METRIC/ATTRIBUTE NAME</th>
<th>DESCRIPTION</th>
<th>MBEAN NAME</th>
</tr>
</thead>
<tbody>
<tr>
<td>bytes-written-rate</td>
<td>The average number of bytes written per second to the RocksDB state store.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
<tr>
<td>bytes-written-total</td>
<td>The total number of bytes written to the RocksDB state store.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
<tr>
<td>bytes-read-rate</td>
<td>The average number of bytes read per second from the RocksDB state store.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
<tr>
<td>bytes-read-total</td>
<td>The total number of bytes read from the RocksDB state store.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
<tr>
<td>memtable-bytes-flushed-rate</td>
<td>The average number of bytes flushed per second from the memtable to disk.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
<tr>
<td>memtable-bytes-flushed-total</td>
<td>The total number of bytes flushed from the memtable to disk.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
<tr>
<td>memtable-hit-ratio</td>
<td>The ratio of memtable hits relative to all lookups to the memtable.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
<tr>
<td>memtable-flush-time-avg</td>
<td>The average duration of memtable flushes to disc in ms.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
<tr>
<td>memtable-flush-time-min</td>
<td>The minimum duration of memtable flushes to disc in ms.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
<tr>
<td>memtable-flush-time-max</td>
<td>The maximum duration of memtable flushes to disc in ms.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
<tr>
<td>block-cache-data-hit-ratio</td>
<td>The ratio of block cache hits for data blocks relative to all lookups for data blocks to the block cache.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
<tr>
<td>block-cache-index-hit-ratio</td>
<td>The ratio of block cache hits for index blocks relative to all lookups for index blocks to the block cache.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
<tr>
<td>block-cache-filter-hit-ratio</td>
<td>The ratio of block cache hits for filter blocks relative to all lookups for filter blocks to the block cache.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
<tr>
<td>write-stall-duration-avg</td>
<td>The average duration of write stalls in ms.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
<tr>
<td>write-stall-duration-total</td>
<td>The total duration of write stalls in ms.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
<tr>
<td>bytes-read-compaction-rate</td>
<td>The average number of bytes read per second during compaction.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
<tr>
<td>bytes-written-compaction-rate</td>
<td>The average number of bytes written per second during compaction.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
<tr>
<td>compaction-time-avg</td>
<td>The average duration of disc compactions in ms.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
<tr>
<td>compaction-time-min</td>
<td>The minimum duration of disc compactions in ms.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
<tr>
<td>compaction-time-max</td>
<td>The maximum duration of disc compactions in ms.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
<tr>
<td>number-open-files</td>
<td>The number of current open files.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
<tr>
<td>number-file-errors-total</td>
<td>The total number of file errors occurred.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
</tbody>
</table>
<p><strong>RocksDB 基于属性的指标：</strong> 以下所有基于属性的指标的记录级别均为 ，<code>info</code>并在访问指标时进行记录。如果状态存储由多个 RocksDB 实例组成，如 WindowStores 和 SessionStores 的情况，则每个指标报告状态存储的所有 RocksDB 实例的总和，块缓存指标除外 <code>block-cache-*</code>。如果每个实例都使用自己的块缓存，则块缓存指标报告所有 RocksDB 实例的总和；如果在所有实例之间共享单个块缓存，则它们仅报告一个实例的记录值。</p>
<table>
<thead>
<tr>
<th>METRIC/ATTRIBUTE NAME</th>
<th>DESCRIPTION</th>
<th>MBEAN NAME</th>
</tr>
</thead>
<tbody>
<tr>
<td>num-immutable-mem-table</td>
<td>The number of immutable memtables that have not yet been flushed.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
<tr>
<td>cur-size-active-mem-table</td>
<td>The approximate size of the active memtable in bytes.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
<tr>
<td>cur-size-all-mem-tables</td>
<td>The approximate size of active and unflushed immutable memtables in bytes.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
<tr>
<td>size-all-mem-tables</td>
<td>The approximate size of active, unflushed immutable, and pinned immutable memtables in bytes.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
<tr>
<td>num-entries-active-mem-table</td>
<td>The number of entries in the active memtable.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
<tr>
<td>num-entries-imm-mem-tables</td>
<td>The number of entries in the unflushed immutable memtables.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
<tr>
<td>num-deletes-active-mem-table</td>
<td>The number of delete entries in the active memtable.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
<tr>
<td>num-deletes-imm-mem-tables</td>
<td>The number of delete entries in the unflushed immutable memtables.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
<tr>
<td>mem-table-flush-pending</td>
<td>This metric reports 1 if a memtable flush is pending, otherwise it reports 0.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
<tr>
<td>num-running-flushes</td>
<td>The number of currently running flushes.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
<tr>
<td>compaction-pending</td>
<td>This metric reports 1 if at least one compaction is pending, otherwise it reports 0.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
<tr>
<td>num-running-compactions</td>
<td>The number of currently running compactions.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
<tr>
<td>estimate-pending-compaction-bytes</td>
<td>The estimated total number of bytes a compaction needs to rewrite on disk to get all levels down to under target size (only valid for level compaction).</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
<tr>
<td>total-sst-files-size</td>
<td>The total size in bytes of all SST files.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
<tr>
<td>live-sst-files-size</td>
<td>The total size in bytes of all SST files that belong to the latest LSM tree.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
<tr>
<td>num-live-versions</td>
<td>Number of live versions of the LSM tree.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
<tr>
<td>block-cache-capacity</td>
<td>The capacity of the block cache in bytes.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
<tr>
<td>block-cache-usage</td>
<td>The memory size of the entries residing in block cache in bytes.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
<tr>
<td>block-cache-pinned-usage</td>
<td>The memory size for the entries being pinned in the block cache in bytes.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
<tr>
<td>estimate-num-keys</td>
<td>The estimated number of keys in the active and unflushed immutable memtables and storage.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
<tr>
<td>estimate-table-readers-mem</td>
<td>The estimated memory in bytes used for reading SST tables, excluding memory used in block cache.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
<tr>
<td>background-errors</td>
<td>The total number of background errors.</td>
<td>kafka.streams:type=stream-state-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td>
</tr>
</tbody>
</table>
<h4 id="record-cache-metrics"><a href="https://kafka.apache.org/documentation/#kafka_streams_cache_monitoring">Record Cache Metrics</a></h4>
<p>以下所有指标的记录级别均为<code>debug</code>：</p>
<table>
<thead>
<tr>
<th>METRIC/ATTRIBUTE NAME</th>
<th>DESCRIPTION</th>
<th>MBEAN NAME</th>
</tr>
</thead>
<tbody>
<tr>
<td>hit-ratio-avg</td>
<td>The average cache hit ratio defined as the ratio of cache read hits over the total cache read requests.</td>
<td>kafka.streams:type=stream-record-cache-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),record-cache-id=([-.\w]+)</td>
</tr>
<tr>
<td>hit-ratio-min</td>
<td>The minimum cache hit ratio.</td>
<td>kafka.streams:type=stream-record-cache-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),record-cache-id=([-.\w]+)</td>
</tr>
<tr>
<td>hit-ratio-max</td>
<td>The maximum cache hit ratio.</td>
<td>kafka.streams:type=stream-record-cache-metrics,thread-id=([-.\w]+),task-id=([-.\w]+),record-cache-id=([-.\w]+)</td>
</tr>
</tbody>
</table>
<h3 id="others"><a href="https://kafka.apache.org/documentation/#others_monitoring">Others</a></h3>
<p>我们建议监控 GC 时间和其他统计信息以及各种服务器统计信息，例如 CPU 利用率、I/O 服务时间等。在客户端，我们建议监控消息/字节率（全局和每个主题）、请求率/大小/时间，在消费者方面，所有分区之间消息的最大延迟和最小获取请求率。为了让消费者跟上，最大延迟需要小于阈值，最小获取率需要大于 0。</p>
<h2 id="69"><a href="https://kafka.apache.org/documentation/#zk">6.9 动物园管理员</a></h2>
<h3 id="_48"><a href="https://kafka.apache.org/documentation/#zkversion">稳定版</a></h3>
<p>当前的稳定分支是 3.5。Kafka 会定期更新以包含 3.5 系列中的最新版本。</p>
<h3 id="zookeeper"><a href="https://kafka.apache.org/documentation/#zk_depr">ZooKeeper 弃用</a></h3>
<p>随着 Apache Kafka 3.5 的发布，Zookeeper 现已被标记为已弃用。计划在 Apache Kafka 的下一个主要版本（版本 4.0）中删除 ZooKeeper，该版本计划最早于 2024 年 4 月进行。在弃用阶段，仍然支持 ZooKeeper 进行 Kafka 集群的元数据管理，但不建议这样做用于新的部署。KRaft 中仍有一小部分功能有待实现，请参阅<a href="https://kafka.apache.org/documentation/#kraft_missing">当前缺失的功能</a>以获取更多信息。</p>
<h4 id="_49"><a href="https://kafka.apache.org/documentation/#zk_drep_migration">移民</a></h4>
<p>将现有的基于 ZooKeeper 的 Kafka 集群迁移到 KRaft 目前处于预览阶段，我们预计它可以在 3.6 版本中投入生产使用。建议用户开始计划迁移到 KRaft，并开始测试以提供反馈。有关如何执行从<a href="https://kafka.apache.org/documentation/#kraft_zk_migration">ZooKeeper 到 KRaft</a>实时迁移以及当前限制的详细信息，请参阅ZooKeeper 到 KRaft 迁移。</p>
<h4 id="3x-zookeeper"><a href="https://kafka.apache.org/documentation/#zk_depr_3xsupport">3.x 和 ZooKeeper 支持</a></h4>
<p>支持 ZooKeeper 模式的最终 3.x 小版本将在发布后 12 个月内获得关键错误修复和安全修复。</p>
<h4 id="zookeeper-kraft"><a href="https://kafka.apache.org/documentation/#zk_depr_timeline">ZooKeeper 和 KRaft 时间轴</a></h4>
<p>有关 ZooKeeper 删除和计划的 KRaft 功能发布的暂定时间表的详细信息和更新，请参阅<a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-833%3A+Mark+KRaft+as+Production+Ready">KIP-833</a>。</p>
<h3 id="zookeeper_1"><a href="https://kafka.apache.org/documentation/#zkops">运行 ZooKeeper</a></h3>
<p>在操作上，我们为健康的 ZooKeeper 安装执行以下操作：</p>
<ul>
<li>物理/硬件/网络布局中的冗余：尽量不要将它们全部放在同一个机架中，体面（但不要疯狂）的硬件，尽量保留冗余电源和网络路径等。典型的 ZooKeeper 整体有 5 个或7 台服务器，分别允许 2 台和 3 台服务器宕机。如果您的部署规模较小，那么使用 3 台服务器是可以接受的，但请记住，在这种情况下您只能容忍 1 台服务器停机。</li>
<li>I/O 隔离：如果您执行大量写入类型流量，您几乎肯定希望事务日志位于专用磁盘组上。对事务日志的写入是同步的（但为了性能而进行批处理），因此并发写入会显着影响性能。ZooKeeper 快照就是这样一种并发写入源，理想情况下应该写入与事务日志分开的磁盘组上。快照异步写入磁盘，因此通常可以与操作系统和消息日志文件共享。您可以通过 dataLogDir 参数将服务器配置为使用单独的磁盘组。</li>
<li>应用程序隔离：除非您真正了解要安装在同一机器上的其他应用程序的应用程序模式，否则最好单独运行 ZooKeeper（尽管这可能是与硬件功能的平衡行为）。</li>
<li>谨慎使用虚拟化：它可以工作，具体取决于您的集群布局、读/写模式和 SLA，但是虚拟化层引入的微小开销可能会累积起来并导致 ZooKeeper 失效，因为它可能对时间非常敏感</li>
<li>ZooKeeper 配置：它是 java，请确保给它“足够”的堆空间（我们通常使用 3-5G 运行它们，但这主要是由于我们这里的数据集大小）。不幸的是，我们没有一个好的公式，但请记住，允许更多的 ZooKeeper 状态意味着快照可能会变得很大，而大快照会影响恢复时间。事实上，如果快照变得太大（几 GB），那么您可能需要增加 initLimit 参数，以便为服务器提供足够的时间来恢复并加入集合。</li>
<li>监控：JMX 和 4 字母单词 (4lw) 命令都非常有用，它们在某些情况下确实重叠（在这些情况下，我们更喜欢 4 字母命令，它们看起来更可预测，或者至少，它们与LI 监控基础设施）</li>
<li>不要过度构建集群：大型集群，尤其是在写入大量使用模式中，意味着大量的集群内通信（写入和后续集群成员更新的仲裁），但不要构建不足（并有淹没集群的风险）。拥有更多服务器会增加您的读取能力。</li>
</ul>
<p>总体而言，我们尝试使 ZooKeeper 系统尽可能小，以处理负载（加上标准增长容量规划）并尽可能简单。与官方版本相比，我们尽量不对配置或应用程序布局做任何花哨的事情，并尽可能保持其独立性。由于这些原因，我们倾向于跳过操作系统打包版本，因为它倾向于尝试将内容放入操作系统标准层次结构中，这可能会“混乱”，因为缺乏更好的措辞方式。</p>
<h2 id="610-kraft"><a href="https://kafka.apache.org/documentation/#kraft">6.10 KRaft</a></h2>
<h3 id="_50"><a href="https://kafka.apache.org/documentation/#kraft_config">配置</a></h3>
<h4 id="_51"><a href="https://kafka.apache.org/documentation/#kraft_role">流程角色</a></h4>
<p>在 KRaft 模式下，每个 Kafka 服务器都可以使用该属性配置为控制器、代理或两者<code>process.roles</code>。该属性可以具有以下值：</p>
<ul>
<li>如果<code>process.roles</code>设置为<code>broker</code>，则服务器充当代理。</li>
<li>如果<code>process.roles</code>设置为<code>controller</code>，则服务器充当控制器。</li>
<li>如果<code>process.roles</code>设置为<code>broker,controller</code>，则服务器既充当代理又充当控制器。</li>
<li>如果<code>process.roles</code>根本没有设置，则假定处于 ZooKeeper 模式。</li>
</ul>
<p>既充当代理又充当控制器的 Kafka 服务器被称为“组合”服务器。对于开发环境等小型用例，组合服务器更易于操作。主要缺点是控制器与系统其他部分的隔离程度较低。例如，无法在组合模式下与代理分开滚动或扩展控制器。在关键部署环境中不建议使用组合模式。</p>
<h4 id="_52"><a href="https://kafka.apache.org/documentation/#kraft_voter">控制器</a></h4>
<p>在KRaft模式下，选择特定的Kafka服务器作为控制器（与基于ZooKeeper的模式不同，在该模式中任何服务器都可以成为控制器）。被选为控制器的服务器将参与元数据仲裁。每个控制器都是当前活动控制器的活动控制器或热备用控制器。</p>
<p>Kafka 管理员通常会选择 3 或 5 个服务器来担任此角色，具体取决于成本和系统在不影响可用性的情况下应承受的并发故障数量等因素。大多数控制器必须处于活动状态才能保持可用性。3个控制器时，集群可以容忍1个控制器故障；如果有 5 个控制器，集群可以容忍 2 个控制器故障。</p>
<p>Kafka 集群中的所有服务器都使用该<code>controller.quorum.voters</code>属性发现仲裁投票者。这标识了应使用的仲裁控制器服务器。必须枚举所有控制器。每个控制器均通过其<code>id</code>、<code>host</code>和<code>port</code>信息进行标识。例如：</p>
<pre><code class="language-bash">controller.quorum.voters=id1@host1:port1,id2@host2:port2,id3@host3:port3
</code></pre>
<p>如果一个Kafka集群有3个控制器，分别名为controller1、controller2和controller3，那么controller1可能有以下配置：</p>
<pre><code class="language-bash">
process.roles=controller
node.id=1
listeners=CONTROLLER://controller1.example.com:9093
controller.quorum.voters=1@controller1.example.com:9093,2@controller2.example.com:9093,3@controller3.example.com:9093
</code></pre>
<p>每个broker和控制器都必须设置该<code>controller.quorum.voters</code>属性。属性中提供的节点 ID<code>controller.quorum.voters</code>必须与控制器服务器上的相应 ID 匹配。例如，在controller1上，node.id必须设置为1，依此类推。每个节点 ID 在特定集群中的所有服务器中必须是唯一的。任何两个服务器都不能具有相同的节点 ID，无论其<code>process.roles</code>值如何。</p>
<h3 id="_53"><a href="https://kafka.apache.org/documentation/#kraft_storage">存储工具</a></h3>
<p>该<code>kafka-storage.sh random-uuid</code>命令可用于为新集群生成集群 ID。使用该命令格式化集群中的每个服务器时，必须使用此集群 ID <code>kafka-storage.sh format</code>。</p>
<p>这与Kafka过去的运作方式不同。此前，Kafka会自动格式化空白存储目录，并自动生成新的集群ID。进行更改的原因之一是自动格式化有时会掩盖错误情况。这对于控制器和代理服务器维护的元数据日志尤其重要。如果大多数控制器能够以空日志目录启动，则可能会在丢失已提交数据的情况下选举领导者。</p>
<h3 id="_54"><a href="https://kafka.apache.org/documentation/#kraft_debug">调试</a></h3>
<h4 id="_55"><a href="https://kafka.apache.org/documentation/#kraft_metadata_tool">元数据仲裁工具</a></h4>
<p>该<code>kafka-metadata-quorum</code>工具可用于描述集群元数据分区的运行时状态。例如，以下命令显示元数据仲裁的摘要：</p>
<pre><code class="language-bash">  &gt; bin/kafka-metadata-quorum.sh --bootstrap-server  broker_host:port describe --status
ClusterId:              fMCL8kv1SWm87L_Md-I2hg
LeaderId:               3002
LeaderEpoch:            2
HighWatermark:          10
MaxFollowerLag:         0
MaxFollowerLagTimeMs:   -1
CurrentVoters:          [3000,3001,3002]
CurrentObservers:       [0,1,2]
</code></pre>
<h4 id="_56"><a href="https://kafka.apache.org/documentation/#kraft_dump_log">转储日志工具</a></h4>
<p>该<code>kafka-dump-log</code>工具可用于调试集群元数据目录的日志段和快照。该工具将扫描提供的文件并解码元数据记录。例如，此命令解码并打印第一个日志段中的记录：</p>
<pre><code class="language-bash">  &gt; bin/kafka-dump-log.sh --cluster-metadata-decoder --files metadata_log_dir/__cluster_metadata-0/00000000000000000000.log
</code></pre>
<p>此命令解码并打印集群元数据快照中的记录：</p>
<pre><code class="language-bash">  &gt; bin/kafka-dump-log.sh --cluster-metadata-decoder --files metadata_log_dir/__cluster_metadata-0/00000000000000000100-0000000001.checkpoint
</code></pre>
<h4 id="_57"><a href="https://kafka.apache.org/documentation/#kraft_shell_tool">元数据外壳</a></h4>
<p>该<code>kafka-metadata-shell</code>工具可用于交互式检查集群元数据分区的状态：</p>
<pre><code class="language-bash">
  &gt; bin/kafka-metadata-shell.sh  --snapshot metadata_log_dir/__cluster_metadata-0/00000000000000000000.log
&gt;&gt; ls /
brokers  local  metadataQuorum  topicIds  topics
&gt;&gt; ls /topics
foo
&gt;&gt; cat /topics/foo/0/data
{
  &quot;partitionId&quot; : 0,
  &quot;topicId&quot; : &quot;5zoAlv-xEh9xRANKXt1Lbg&quot;,
  &quot;replicas&quot; : [ 1 ],
  &quot;isr&quot; : [ 1 ],
  &quot;removingReplicas&quot; : null,
  &quot;addingReplicas&quot; : null,
  &quot;leader&quot; : 1,
  &quot;leaderEpoch&quot; : 0,
  &quot;partitionEpoch&quot; : 0
}
&gt;&gt; exit

</code></pre>
<h3 id="_58"><a href="https://kafka.apache.org/documentation/#kraft_deployment">部署注意事项</a></h3>
<ul>
<li>Kafka 服务器<code>process.role</code>应设置为其中之一<code>broker</code>或<code>controller</code>但不能同时设置两者。组合模式可以在开发环境中使用，但在关键部署环境中应避免使用。</li>
<li>为了实现冗余，Kafka 集群应使用 3 个控制器。在关键环境中不建议使用超过 3 个控制器。在极少数情况下，出现部分网络故障时，集群元数据仲裁可能会变得不可用。此限制将在 Kafka 的未来版本中得到解决。</li>
<li>Kafka 控制器将集群的所有元数据存储在内存和磁盘上。我们认为，对于典型的 Kafka 集群，5GB 主内存和元数据日志管理器上的 5GB 磁盘空间就足够了。</li>
</ul>
<h3 id="_59"><a href="https://kafka.apache.org/documentation/#kraft_missing">缺少的功能</a></h3>
<p>KRaft 模式中未完全实现以下功能：</p>
<ul>
<li>支持具有多个存储目录的JBOD配置</li>
<li>修改独立 KRaft 控制器上的某些动态配置</li>
<li>委托代币</li>
</ul>
<h3 id="zookeeper-kraft_1"><a href="https://kafka.apache.org/documentation/#kraft_zk_migration">ZooKeeper 到 KRaft 迁移</a></h3>
<p><strong>ZooKeeper 到 KRaft 的迁移被视为早期访问功能，不建议用于生产集群。</strong></p>
<p>ZK 到 KRaft 的迁移尚不支持以下功能：</p>
<ul>
<li>迁移期间或迁移之后降级到 ZooKeeper 模式</li>
<li><a href="https://kafka.apache.org/documentation/#kraft_missing">KRaft 尚不支持的</a>其他功能<a href="https://kafka.apache.org/documentation/#kraft_missing"></a></li>
</ul>
<p>请使用 <a href="https://issues.apache.org/jira/projects/KAFKA">项目 JIRA</a>和“kraft”组件报告 ZooKeeper 到 KRaft 迁移的问题。</p>
<h2 id="_60">术语</h2>
<p>我们这里使用术语“迁移”来指将Kafka集群的元数据系统从ZooKeeper更改为KRaft并迁移现有元数据的过程。“升级”是指安装更新版本的 Kafka。不建议在执行元数据迁移的同时升级软件。</p>
<p>我们还使用术语“ZK 模式”来指代使用 ZooKeeper 作为元数据系统的 Kafka 代理。“KRaft 模式”是指使用 KRaft 控制器仲裁作为其元数据系统的 Kafka 代理。</p>
<h2 id="_61">准备迁移</h2>
<p>在开始迁移之前，Kafka 代理必须升级到软件版本 3.5.0，并将“inter.broker.protocol.version”配置设置为“3.5”。有关升级说明，请参阅<a href="https://kafka.apache.org/documentation/#upgrade_3_5_0">升级到 3.5.0</a>。</p>
<p>建议在迁移处于活动状态时为迁移组件启用 TRACE 级别日志记录。这可以通过将以下 log4j 配置添加到每个 KRaft 控制器的“log4j.properties”文件来完成。</p>
<pre><code class="language-test">log4j.logger.org.apache.kafka.metadata.migration=TRACE
</code></pre>
<p>在迁移期间在 KRaft 控制器和 ZK 代理上启用 DEBUG 日志记录通常很有用。</p>
<h2 id="kraft_2">配置 KRaft 控制器仲裁</h2>
<p>在开始迁移之前需要做两件事。首先，必须配置代理以支持迁移，其次，必须部署 KRaft 控制器仲裁。KRaft 控制器应配置与现有 Kafka 集群相同的集群 ID。这可以通过检查代理数据目录中的“meta.properties”文件之一或运行以下命令来找到。</p>
<pre><code class="language-bash">./bin/zookeeper-shell.sh localhost:2181 get /cluster/id
</code></pre>
<p>KRaft 控制器仲裁还应配置最新<code>metadata.version</code>的“3.4”。有关 KRaft 部署的更多说明，请参阅<a href="https://kafka.apache.org/documentation/#kraft_config">上述文档</a>。</p>
<p>除了标准的 KRaft 配置之外，KRaft 控制器还需要启用迁移支持并提供 ZooKeeper 连接配置。</p>
<p>以下是准备迁移的 KRaft 控制器的示例配置：</p>
<pre><code class="language-py"># Sample KRaft cluster controller.properties listening on 9093
process.roles=controller
node.id=3000
controller.quorum.voters=3000@localhost:9093
controller.listener.names=CONTROLLER
listeners=CONTROLLER://:9093

# Enable the migration
zookeeper.metadata.migration.enable=true

# ZooKeeper client configuration
zookeeper.connect=localhost:2181

# Other configs ...
</code></pre>
<p><em>注意：KRaft 集群<code>node.id</code>值必须与任何现有的 ZK 代理不同<code>broker.id</code>。在 KRaft 模式中，代理和控制器共享相同的 Node ID 命名空间。</em></p>
<h2 id="_62">在代理上启用迁移</h2>
<p>一旦 KRaft 控制器仲裁启动，代理将需要重新配置并重新启动。代理可以以滚动方式重新启动，以避免影响集群可用性。每个代理都需要以下配置才能与 KRaft 控制器通信并启用迁移。</p>
<ul>
<li><a href="https://kafka.apache.org/documentation/#brokerconfigs_controller.quorum.voters">controller.quorum.voters</a></li>
<li><a href="https://kafka.apache.org/documentation/#brokerconfigs_controller.listener.names">controller.listener.names</a></li>
<li>还应将controller.listener.name添加到<a href="https://kafka.apache.org/documentation/#brokerconfigs_listener.security.protocol.map">listener.security.property.map</a></li>
<li><a href="https://kafka.apache.org/documentation/#brokerconfigs_zookeeper.metadata.migration.enable">zookeeper.metadata.migration.enable</a></li>
</ul>
<p>以下是准备迁移的代理的示例配置：</p>
<pre><code class="language-py"># Sample ZK broker server.properties listening on 9092
broker.id=0
listeners=PLAINTEXT://:9092
advertised.listeners=PLAINTEXT://localhost:9092
listener.security.protocol.map=PLAINTEXT:PLAINTEXT,CONTROLLER:PLAINTEXT

# Set the IBP
inter.broker.protocol.version=3.5

# Enable the migration
zookeeper.metadata.migration.enable=true

# ZooKeeper client configuration
zookeeper.connect=localhost:2181

# KRaft controller quorum configuration
controller.quorum.voters=3000@localhost:9093
controller.listener.names=CONTROLLER
</code></pre>
<p><em>注意：使用必要的配置重新启动最终的 ZK 代理后，迁移将自动开始。</em> 迁移完成后，在主控上可以观察到一条INFO级别的日志：</p>
<pre><code class="language-text">Completed migration of metadata from Zookeeper to KRaft
</code></pre>
<h2 id="kraft_3">将代理迁移到 KRaft</h2>
<p>一旦 KRaft 控制器完成元数据迁移，代理仍将以 ZK 模式运行。当 KRaft 控制器处于迁移模式时，它将继续向 ZK 模式代理发送控制器 RPC。这包括 UpdateMetadata 和 LeaderAndIsr 等 RPC。</p>
<p>要将代理迁移到 KRaft，只需将它们重新配置为 KRaft 代理并重新启动即可。以上面的代理配置为例，我们将替换<code>broker.id</code>为<code>node.id</code>并添加 <code>process.roles=broker</code>。代理在重新启动时保持相同的代理/节点 ID 非常重要。此时应该删除 Zookeeper 配置。</p>
<pre><code class="language-py"># Sample KRaft broker server.properties listening on 9092
process.roles=broker
node.id=0
listeners=PLAINTEXT://:9092
advertised.listeners=PLAINTEXT://localhost:9092
listener.security.protocol.map=PLAINTEXT:PLAINTEXT,CONTROLLER:PLAINTEXT

# Don't set the IBP, KRaft uses &quot;metadata.version&quot; feature flag
# inter.broker.protocol.version=3.5

# Remove the migration enabled flag
# zookeeper.metadata.migration.enable=true

# Remove ZooKeeper client configuration
# zookeeper.connect=localhost:2181

# Keep the KRaft controller quorum configuration
controller.quorum.voters=3000@localhost:9093
controller.listener.names=CONTROLLER
</code></pre>
<p>每个代理都会使用 KRaft 配置重新启动，直到整个集群在 KRaft 模式下运行。</p>
<h2 id="_63">完成迁移</h2>
<p>在 KRaft 模式下重新启动所有代理后，完成迁移的最后一步是使 KRaft 控制器退出迁移模式。这是通过从每个配置中删除“zookeeper.metadata.migration.enable”属性并一次重新启动它们来完成的。</p>
<pre><code class="language-py"># Sample KRaft cluster controller.properties listening on 9093
process.roles=controller
node.id=3000
controller.quorum.voters=3000@localhost:9093
controller.listener.names=CONTROLLER
listeners=CONTROLLER://:9093

# Disable the migration
# zookeeper.metadata.migration.enable=true

# Remove ZooKeeper client configuration
# zookeeper.connect=localhost:2181

# Other configs ...
</code></pre>
<h2 id="611"><a href="https://kafka.apache.org/documentation/#kraft">6.11分层存储</a></h2>
<h3 id="_64"><a href="https://kafka.apache.org/documentation/#tiered_storage_overview">分层存储概述</a></h3>
<p>Kafka数据主要以流媒体方式使用尾部读取来消费。Tail读取利用操作系统的页面缓存来服务数据，而不是磁盘读取。旧数据通常从磁盘读取以进行回填或故障恢复，并且不常见。</p>
<p>在分层存储方法中，Kafka集群配置了两层存储——本地存储和远程。本地层与当前使用Kafka Broker 上的本地磁盘存储日志段的Kafka相同。新的远程层使用外部存储系统，如HDFS或S3，来存储已完成的日志段。有关更多信息，请查看<a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-405%3A+Kafka+Tiered+Storage">KIP-405</a>。</p>
<p><strong>注意：分层存储被视为早期访问功能，不建议在生产环境中使用</strong></p>
<h3 id="_65"><a href="https://kafka.apache.org/documentation/#tiered_storage_config">配置</a></h3>
<h4 id="broker_1"><a href="https://kafka.apache.org/documentation/#tiered_storage_config_broker">Broker 配置</a></h4>
<p>默认情况下，Kafka服务器不会启用分层存储功能。<code>remote.log.storage.system.enable</code>是控制是否在 Broker 中启用分层存储功能的属性。将其设置为“true”启用此功能。</p>
<p><code>RemoteStorageManager</code>是一个提供远程日志段和索引生命周期的接口。Kafka服务器不提供RemoteStorageManager的开箱即用实现。配置<code>remote.log.storage.manager.class.name</code>和<code>remote.log.storage.manager.class.path</code>，以指定RemoteStorageManager的实现。</p>
<p><code>RemoteLogMetadataManager</code>是一个接口，用于提供具有强烈一致的语义的远程日志段元数据生命周期。默认情况下，Kafka提供了一个以存储为内部主题的实现。可以通过配置<code>remote.log.metadata.manager.class.name</code>和<code>remote.log.metadata.manager.class.path</code>来更改此实现。当采用默认的基于kafka内部主题的实现时，<code>remote.log.metadata.manager.listener.name</code>是一个强制性属性，用于指定默认的RemoteLogMetadataManager实现创建的客户端。</p>
<h4 id="_66"><a href="https://kafka.apache.org/documentation/#tiered_storage_config_topic">主题配置</a></h4>
<p>在正确配置分层存储功能的代理端配置后，仍然需要设置主题级别的配置。<code>remote.storage.enable</code>是确定主题是否想要使用分层存储的开关。默认情况下，它被设置为false。启用<code>remote.storage.enable</code>属性后，接下来要考虑的是日志保留。当为主题启用分层存储时，需要设置2个额外的日志保留配置：</p>
<ul>
<li><code>local.retention.ms</code></li>
<li><code>retention.ms</code></li>
<li><code>local.retention.bytes</code></li>
<li><code>retention.bytes</code></li>
</ul>
<p>The configuration prefixed with <code>local</code> are to specify the time/size the "local" log file can accept before moving to remote storage, and then get deleted. If unset, The value in <code>retention.ms</code> and <code>retention.bytes</code> will be used.</p>
<h3 id="_67"><a href="https://kafka.apache.org/documentation/#tiered_storage_config_ex">快速入门示例</a></h3>
<p>Apache Kafka不提供开箱即用的RemoteStorageManager实现。要预览分层存储功能，可以使用为集成测试而实施的<a href="https://github.com/apache/kafka/blob/trunk/storage/src/test/java/org/apache/kafka/server/log/remote/storage/LocalTieredStorage.java">LocalTieredStorage</a>，这将在本地存储中创建一个临时目录来模拟远程存储。</p>
<p>要采用“LocalTieredStorage”，需要在本地构建测试库</p>
<pre><code class="language-bash"># please checkout to the specific version tag you're using before building it
# ex: `git checkout 3.6.0`
./gradlew clean :storage:testJar
</code></pre>
<p>构建成功后，在 <code>storage/build/libs</code> 下应该有一个 <code>kafka-storage-x.x.x-test.jar</code> 文件。接下来，在代理端设置配置以启用分层存储功能。</p>
<pre><code class="language-bash"># Sample Zookeeper/Kraft broker server.properties listening on PLAINTEXT://:9092
remote.log.storage.system.enable=true

# Setting the listener for the clients in RemoteLogMetadataManager to talk to the brokers.
remote.log.metadata.manager.listener.name=PLAINTEXT

# Please provide the implementation info for remoteStorageManager.
# This is the mandatory configuration for tiered storage.
# Here, we use the `LocalTieredStorage` built above.
remote.log.storage.manager.class.name=org.apache.kafka.server.log.remote.storage.LocalTieredStorage
remote.log.storage.manager.class.path=/PATH/TO/kafka-storage-x.x.x-test.jar

# These 2 prefix are default values, but customizable
remote.log.storage.manager.impl.prefix=rsm.config.
remote.log.metadata.manager.impl.prefix=rlmm.config.

# Configure the directory used for `LocalTieredStorage`
# Note, please make sure the brokers need to have access to this directory
rsm.config.dir=/tmp/kafka-remote-storage

# This needs to be changed if number of brokers in the cluster is more than 1
rlmm.config.remote.log.metadata.topic.replication.factor=1

# Try to speed up the log retention check interval for testing
log.retention.check.interval.ms=1000
</code></pre>
<p>按照<a href="https://kafka.apache.org/documentation/#quickstart_startserver">快速入门指南</a>启动卡夫卡环境。然后，创建一个具有配置启用分层存储的主题：</p>
<pre><code class="language-bash"># remote.storage.enable=true -&gt; enables tiered storage on the topic
# local.retention.ms=1000 -&gt; The number of milliseconds to keep the local log segment before it gets deleted.
  Note that a local log segment is eligible for deletion only after it gets uploaded to remote.
# retention.ms=3600000 -&gt; when segments exceed this time, the segments in remote storage will be deleted
# segment.bytes=1048576 -&gt; for test only, to speed up the log segment rolling interval
# file.delete.delay.ms=10000 -&gt; for test only, to speed up the local-log segment file delete delay

bin/kafka-topics.sh --create --topic tieredTopic --bootstrap-server localhost:9092 \
--config remote.storage.enable=true --config local.retention.ms=1000 --config retention.ms=3600000 \
--config segment.bytes=1048576 --config file.delete.delay.ms=1000
</code></pre>
<p>尝试向“分层主题”主题发送消息以滚动日志段：</p>
<pre><code class="language-bash">bin/kafka-producer-perf-test.sh --topic tieredTopic --num-records 1200 --record-size 1024 --throughput -1 --producer-props bootstrap.servers=localhost:9092
</code></pre>
<p>然后，滚动活动段后，旧段应移动到远程存储并删除。这可以通过检查上面配置的远程日志目录来验证。例如：</p>
<pre><code class="language-bash"> &gt; ls /tmp/kafka-remote-storage/kafka-tiered-storage/tieredTopic-0-jF8s79t9SrG_PNqlwv7bAA
00000000000000000000-knnxbs3FSRyKdPcSAOQC-w.index
00000000000000000000-knnxbs3FSRyKdPcSAOQC-w.snapshot
00000000000000000000-knnxbs3FSRyKdPcSAOQC-w.leader_epoch_checkpoint
00000000000000000000-knnxbs3FSRyKdPcSAOQC-w.timeindex
00000000000000000000-knnxbs3FSRyKdPcSAOQC-w.log
</code></pre>
<p>最后，我们可以尝试从头开始消耗一些数据并打印偏移数，以确保它能成功从远程存储中获取偏移0。</p>
<pre><code class="language-bash">bin/kafka-console-consumer.sh --topic tieredTopic --from-beginning --max-messages 1 --bootstrap-server localhost:9092 --property print.offset=true
</code></pre>
<p>请注意，如果您想在集群级别禁用分层存储，您应该明确删除启用分层存储的主题。尝试禁用集群级别的分层存储而不删除使用分层存储的主题，将导致启动期间的异常。</p>
<pre><code class="language-bash">bin/kafka-topics.sh --delete --topic tieredTopic --bootstrap-server localhost:9092
</code></pre>
<p>删除主题后，您可以在代理配置中安全地设置 <code>remote.log.storage.system.enable=false</code>。</p>
<h3 id="_68"><a href="https://kafka.apache.org/documentation/#tiered_storage_limitation">限制</a></h3>
<p>虽然分层存储的早期访问版本提供了尝试此新功能的机会，但重要的是要意识到以下限制：</p>
<ul>
<li>不支持具有多个日志目录的集群（即JBOD功能）</li>
<li>不支持压缩主题</li>
<li>无法禁用主题级别的分层存储</li>
<li>在代理级别禁用分层存储之前，需要删除启用分层存储的主题</li>
<li>与分层存储功能相关的管理操作仅在3.0版本起的客户端上受支持</li>
</ul>
<p>有关更多信息，请查看<a href="https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Tiered+Storage+Early+Access+Release+Notes">分层存储早期访问发布说明</a>。</p></div>
            </div>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script src="../js/bootstrap.bundle.min.js"></script>
        <script>
            var base_url = "..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../js/base.js"></script>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <script src="../search/main.js"></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
            </div>
            <div class="modal-body">
                <p>From here you can search these documents. Enter your search terms below.</p>
                <form>
                    <div class="form-group">
                        <input type="search" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results" data-no-results-text="No results found"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
